{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3D ConvNeXt-like Components ---\n",
    "\n",
    "class LayerNorm3D(nn.Module):\n",
    "    \"\"\" LayerNorm that supports 3D input (B, C, D, H, W). \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        # PyTorch's LayerNorm applies over the last dimensions.\n",
    "        # For (B, C, D, H, W), we want it over (D, H, W) or (C, D, H, W).\n",
    "        # ConvNeXt uses it over channels after depthwise conv, then over features.\n",
    "        # Let's apply it over (D, H, W) which is typical for spatial norms.\n",
    "        if isinstance(normalized_shape, int):\n",
    "             # Assume channel dimension if int\n",
    "             self.normalized_shape = (normalized_shape,)\n",
    "        else:\n",
    "             self.normalized_shape = tuple(normalized_shape) # Should be (C, D, H, W) or (D, H, W)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is (B, C, D, H, W)\n",
    "        # Apply norm over the last self.normalized_shape dimensions\n",
    "        # This requires permuting for LayerNorm and then permuting back\n",
    "        # A simpler way is to compute mean/var manually or use F.layer_norm\n",
    "        \n",
    "        # F.layer_norm expects (..., normalized_shape)\n",
    "        # We have (B, C, D, H, W) and want to normalize (D, H, W) or (C, D, H, W)\n",
    "        \n",
    "        # Normalize over spatial dimensions (D, H, W) - similar to InstanceNorm but batch-wise mean/var\n",
    "        # Alternative: Normalize over (C, D, H, W) - less common but matches some interpretations\n",
    "        \n",
    "        # Let's try normalizing over (D, H, W)\n",
    "        mean = x.mean(dim=(-3, -2, -1), keepdim=True)\n",
    "        var = x.var(dim=(-3, -2, -1), keepdim=True, unbiased=False)\n",
    "        x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # Reshape weight and bias to match x dims for broadcasting\n",
    "        # weight and bias are (C,) if normalized_shape is C\n",
    "        # need to reshape to (1, C, 1, 1, 1)\n",
    "        if len(self.normalized_shape) == 1 and self.normalized_shape[0] == x.size(1):\n",
    "             weight = self.weight.view(1, -1, 1, 1, 1)\n",
    "             bias = self.bias.view(1, -1, 1, 1, 1)\n",
    "        else:\n",
    "             # This case is complex if normalized_shape is (D, H, W) - need to match\n",
    "             # For this example, let's assume channel-wise normalization after depthwise\n",
    "             # This matches ConvNeXt paper's application slightly better (applied to channels)\n",
    "             # Let's redefine to normalize over channels: (B, C, D, H, W) -> norm over C\n",
    "             mean = x.mean(dim=1, keepdim=True)\n",
    "             var = x.var(dim=1, keepdim=True, unbiased=False)\n",
    "             x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "             weight = self.weight.view(1, -1, 1, 1, 1) # Assuming normalized_shape is C\n",
    "             bias = self.bias.view(1, -1, 1, 1, 1)\n",
    "\n",
    "\n",
    "        return x * weight + bias\n",
    "\n",
    "class ConvNeXtBlock3D(nn.Module):\n",
    "    \"\"\" ConvNeXt Block adapted for 3D input. \"\"\"\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        # Larger depthwise kernel (e.g., 7x7x7)\n",
    "        self.dwconv = nn.Conv3d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
    "        # LayerNorm applied over channels\n",
    "        self.norm = LayerNorm3D(dim)\n",
    "        # 1x1x1 convolution to expand channels (factor 4)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
    "        # GELU activation\n",
    "        self.act = nn.GELU()\n",
    "        # 1x1x1 convolution to contract channels\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        \n",
    "        # Layer scale\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
    "                                   requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        \n",
    "        # Drop path (stochastic depth) - not implemented in this basic example,\n",
    "        # but can be added for regularization.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B, C, D, H, W)\n",
    "        input = x\n",
    "        \n",
    "        # Depthwise convolution\n",
    "        x = self.dwconv(x)\n",
    "        \n",
    "        # Permute for LayerNorm and Linear layers: (B, C, D, H, W) -> (B, D, H, W, C)\n",
    "        # Apply norm over channels: (B, C, D, H, W) -> norm over C\n",
    "        # Let's stick to channel-wise norm after DW conv as per ConvNeXt\n",
    "        x = self.norm(x) # Norm is over channels (C)\n",
    "\n",
    "        # Permute back for Linear: (B, C, D, H, W) -> (B, D, H, W, C)\n",
    "        x = x.permute(0, 2, 3, 4, 1) \n",
    "        \n",
    "        # Pointwise convolutions (implemented as Linear layers)\n",
    "        x = self.pwconv1(x) # (B, D, H, W, C*4)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x) # (B, D, H, W, C)\n",
    "        \n",
    "        # Apply layer scale if gamma is used\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x # (B, D, H, W, C) * (C,) broadcasts\n",
    "        \n",
    "        # Permute back to (B, C, D, H, W) for residual connection\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "        \n",
    "        # Residual connection\n",
    "        x = input + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transformer Components ---\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" Basic fixed sinusoidal positional encoding. \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # Add batch dimension\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B, SeqLen, embed_dim)\n",
    "        # Add positional encoding to the tokens\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f57ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full Model Combining Components ---\n",
    "\n",
    "class WaveformInversionModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dims=(1, 128, 512, 128), # (C, S, T, R) - Example: 1 channel, 128 sources, 512 time steps, 128 receivers\n",
    "                 output_dims=(1, 256, 256),   # (C, H, W) - Example: 1 channel velocity map, 256x256 spatial\n",
    "                 convnext_channels=[32, 64, 128, 256], # Channels for 3D CNN stages\n",
    "                 convnext_depths=[2, 2, 6, 2],      # Number of blocks in each 3D CNN stage\n",
    "                 convnext_downsample_strides=[(1, 2, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2)], # Stride for downsampling between stages\n",
    "                 transformer_layers=4, \n",
    "                 transformer_heads=8, \n",
    "                 transformer_embed_dim=256, # Transformer embedding dimension\n",
    "                 transformer_ffn_dim=1024,\n",
    "                 decoder_channels=[128, 64, 32], # Channels for 2D decoder stages\n",
    "                 decoder_upsample_strides=[(2, 2), (2, 2), (2, 2)], # Stride for upsampling (H, W)\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        in_channels, in_S, in_T, in_R = input_dims\n",
    "        out_channels, out_H, out_W = output_dims\n",
    "\n",
    "        self.convnext_channels = convnext_channels\n",
    "        self.convnext_depths = convnext_depths\n",
    "        self.convnext_downsample_strides = convnext_downsample_strides\n",
    "        self.transformer_embed_dim = transformer_embed_dim\n",
    "        self.decoder_upsample_strides = decoder_upsample_strides\n",
    "\n",
    "        # --- 3D ConvNeXt Encoder ---\n",
    "        stages = []\n",
    "        # Initial convolution\n",
    "        stages.append(nn.Conv3d(in_channels, convnext_channels[0], kernel_size=4, stride=convnext_downsample_strides[0], padding=1))\n",
    "        # Calculate dimensions after initial conv\n",
    "        current_S = math.floor((in_S - 4 + 2*1) / convnext_downsample_strides[0][0]) + 1\n",
    "        current_T = math.floor((in_T - 4 + 2*1) / convnext_downsample_strides[0][1]) + 1\n",
    "        current_R = math.floor((in_R - 4 + 2*1) / convnext_downsample_strides[0][2]) + 1\n",
    "\n",
    "        for i in range(len(convnext_channels)):\n",
    "            dim = convnext_channels[i]\n",
    "            # Add ConvNeXt blocks for the stage\n",
    "            stage_blocks = [\n",
    "                ConvNeXtBlock3D(dim) for _ in range(convnext_depths[i])\n",
    "            ]\n",
    "            stages.append(nn.Sequential(*stage_blocks))\n",
    "\n",
    "            # Add downsampling layer between stages (except after the last stage)\n",
    "            if i < len(convnext_channels) - 1:\n",
    "                next_dim = convnext_channels[i+1]\n",
    "                downsample_stride = convnext_downsample_strides[i+1]\n",
    "                stages.append(nn.Sequential(\n",
    "                    LayerNorm3D(dim), # Norm before downsampling\n",
    "                    nn.Conv3d(dim, next_dim, kernel_size=2, stride=downsample_stride) # 2x2x2 conv with stride\n",
    "                ))\n",
    "                current_S = math.floor((current_S - 2) / downsample_stride[0]) + 1\n",
    "                current_T = math.floor((current_T - 2) / downsample_stride[1]) + 1\n",
    "                current_R = math.floor((current_R - 2) / downsample_stride[2]) + 1\n",
    "        \n",
    "        self.encoder_3d = nn.Sequential(*stages)\n",
    "        \n",
    "        self.final_3d_channels = convnext_channels[-1]\n",
    "        self.final_S = current_S\n",
    "        self.final_T = current_T\n",
    "        self.final_R = current_R\n",
    "        self.sequence_length = self.final_S * self.final_T * self.final_R\n",
    "\n",
    "        print(f\"3D Encoder output shape (before flatten): (B, {self.final_3d_channels}, {self.final_S}, {self.final_T}, {self.final_R})\")\n",
    "        print(f\"Transformer sequence length: {self.sequence_length}\")\n",
    "\n",
    "        # --- Project to Transformer Embedding Dim ---\n",
    "        self.encoder_to_transformer_proj = nn.Linear(self.final_3d_channels, transformer_embed_dim)\n",
    "        self.pos_embedding = PositionalEncoding(transformer_embed_dim, max_len=self.sequence_length)\n",
    "\n",
    "        # --- Transformer Encoder ---\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=transformer_embed_dim,\n",
    "            nhead=transformer_heads,\n",
    "            dim_feedforward=transformer_ffn_dim,\n",
    "            batch_first=True # Input expects (batch, seq, dim)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=transformer_layers)\n",
    "\n",
    "        # --- Map Transformer Output to 2D Latent Grid ---\n",
    "        # We need to reshape the sequence (B, SeqLen, embed_dim) to (B, C_dec_init, H_latent, W_latent)\n",
    "        # Where H_latent * W_latent = SeqLen.\n",
    "        # Let's try to find suitable H_latent, W_latent from SeqLen = S'*T'*R'\n",
    "        # This mapping is arbitrary without specific domain knowledge or learned approach.\n",
    "        # For demonstration, let's try to make H_latent, W_latent roughly spatial dimensions.\n",
    "        # S' relates to sources, R' relates to receivers (both spatial). T' relates to time.\n",
    "        # Let's try to map S'xR' to H_latent x W_latent and somehow incorporate T'.\n",
    "        # A simpler way is to calculate factors of SeqLen and choose H_latent, W_latent.\n",
    "        # Example: SeqLen = 8192 = 64 * 128. Let H_latent = 64, W_latent = 128.\n",
    "        \n",
    "        # Need to calculate the total upsampling factor needed in 2D decoder\n",
    "        total_upsample_H = out_H / H_latent # This requires H_latent to be a divisor of out_H\n",
    "        total_upsample_W = out_W / W_latent # This requires W_latent to be a divisor of out_W\n",
    "        \n",
    "        # Let's assume we can find H_latent, W_latent such that SeqLen = H_latent * W_latent\n",
    "        # and total_upsample_H, total_upsample_W are powers of 2 or match decoder strides.\n",
    "        \n",
    "        # For now, let's make a simplifying assumption:\n",
    "        # We will infer H_latent, W_latent such that H_latent * W_latent = SeqLen\n",
    "        # and the total upsampling factors match the target output dimensions.\n",
    "        # This requires careful design of convnext_downsample_strides and decoder_upsample_strides\n",
    "        # and finding suitable H_latent, W_latent.\n",
    "        \n",
    "        # A safer approach for a demo is to calculate SeqLen, choose *any* suitable H_latent, W_latent\n",
    "        # such that H_latent * W_latent = SeqLen, and *then* define decoder strides to reach the target H, W.\n",
    "        \n",
    "        # Let's calculate the target H_latent, W_latent based on the final encoder dims:\n",
    "        # Try to roughly map S' to H_latent, R' to W_latent, and somehow collapse T'.\n",
    "        # This is tricky. Let's just find factors of SeqLen.\n",
    "        # Example: SeqLen = 8192. Factors include (64, 128). Let's try H_latent=64, W_latent=128.\n",
    "        \n",
    "        # --- Determining H_latent, W_latent ---\n",
    "        # This is a design choice. H_latent * W_latent must equal self.sequence_length.\n",
    "        # The decoder will upsample from (H_latent, W_latent) to (out_H, out_W).\n",
    "        # The product of decoder upsample strides must be out_H/H_latent and out_W/W_latent.\n",
    "        \n",
    "        # Let's make a pragmatic choice for H_latent, W_latent for the example:\n",
    "        # Find a pair of factors (h, w) for SeqLen such that h and w are reasonably close\n",
    "        # and h/out_H, w/out_W allow integer total upsampling factors.\n",
    "        \n",
    "        # Simplified approach: Find factors. Just pick one combination for the demo.\n",
    "        # Let's assume H_latent and W_latent are determined based on the structure, e.g.,\n",
    "        # maybe collapse T' dimension and roughly scale S' and R' to H_latent and W_latent.\n",
    "        # Let's assume for this example that H_latent = self.final_S * factor_S, W_latent = self.final_R * factor_R\n",
    "        # and T' dimension is somehow incorporated into channels or collapsed differently.\n",
    "        # This requires the total size self.final_S * self.final_T * self.final_R to be reshaped into H_latent * W_latent.\n",
    "        # This implies H_latent * W_latent must equal self.sequence_length.\n",
    "\n",
    "        # A common approach: Assume H_latent, W_latent are derived from the input dimensions by total stride.\n",
    "        # E.g., if total downsampling is 8x in S, 16x in T, 8x in R.\n",
    "        # S'=S/8, T'=T/16, R'=R/8. SeqLen = (S/8)*(T/16)*(R/8).\n",
    "        # If output is HxW, maybe H_latent = H / total_decoder_stride_H, W_latent = W / total_decoder_stride_W.\n",
    "        # And we need H_latent * W_latent = SeqLen.\n",
    "        \n",
    "        # Let's define H_latent and W_latent first based on required upsampling.\n",
    "        # Decoder has N stages with strides s_h_i, s_w_i. Total stride H = prod(s_h_i), Total stride W = prod(s_w_i).\n",
    "        # H_latent = out_H // Total_stride_H, W_latent = out_W // Total_stride_W.\n",
    "        # This requires SeqLen = H_latent * W_latent.\n",
    "\n",
    "        total_upsample_H = math.prod([s[0] for s in decoder_upsample_strides])\n",
    "        total_upsample_W = math.prod([s[1] for s in decoder_upsample_strides])\n",
    "        \n",
    "        self.H_latent = out_H // total_upsample_H\n",
    "        self.W_latent = out_W // total_upsample_W\n",
    "\n",
    "        print(f\"Decoder latent shape: ({self.H_latent}, {self.W_latent})\")\n",
    "        print(f\"Required sequence length for reshape: {self.H_latent * self.W_latent}\")\n",
    "        \n",
    "        # Ensure the calculated latent size matches the transformer sequence length\n",
    "        # This might require adjusting ConvNeXt strides or decoder strides\n",
    "        if self.H_latent * self.W_latent != self.sequence_length:\n",
    "             raise ValueError(f\"Mismatch between transformer sequence length ({self.sequence_length}) \"\n",
    "                             f\"and required latent decoder size ({self.H_latent * self.W_latent}). \"\n",
    "                             f\"Adjust ConvNeXt strides or decoder strides.\")\n",
    "\n",
    "        self.transformer_to_decoder_proj = nn.Linear(transformer_embed_dim, decoder_channels[0] * self.H_latent * self.W_latent)\n",
    "        # Alternatively, project seq items to channels and reshape\n",
    "        self.transformer_output_channels = decoder_channels[0]\n",
    "        self.transformer_to_decoder_reshape_proj = nn.Linear(transformer_embed_dim, self.transformer_output_channels)\n",
    "\n",
    "\n",
    "        # --- 2D Decoder ---\n",
    "        decoder_stages = []\n",
    "        in_c = self.transformer_output_channels # Starting channels for 2D decoder\n",
    "        current_H, current_W = self.H_latent, self.W_latent\n",
    "\n",
    "        for i in range(len(decoder_channels)):\n",
    "            out_c = decoder_channels[i]\n",
    "            upsample_stride = decoder_upsample_strides[i]\n",
    "            \n",
    "            # Use ConvTranspose2d for upsampling\n",
    "            decoder_stages.append(nn.ConvTranspose2d(in_c, out_c, kernel_size=upsample_stride, stride=upsample_stride))\n",
    "            \n",
    "            current_H *= upsample_stride[0]\n",
    "            current_W *= upsample_stride[1]\n",
    "            in_c = out_c\n",
    "\n",
    "        self.decoder_2d = nn.Sequential(*decoder_stages)\n",
    "\n",
    "        # Final output layer\n",
    "        self.final_conv = nn.Conv2d(decoder_channels[-1], out_channels, kernel_size=3, padding=1) # Output velocity channel(s)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is (B, S, T, R)\n",
    "        \n",
    "        # Add channel dimension for 3D Conv: (B, 1, S, T, R)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # 3D ConvNeXt Encoder\n",
    "        x = self.encoder_3d(x) # (B, C_3d, S', T', R')\n",
    "\n",
    "        # Flatten and Project to Transformer\n",
    "        B, C, S_prime, T_prime, R_prime = x.shape\n",
    "        x = x.permute(0, 2, 3, 4, 1) # (B, S', T', R', C_3d)\n",
    "        x = x.reshape(B, -1, C)      # (B, SeqLen, C_3d) where SeqLen = S'*T'*R'\n",
    "\n",
    "        # Project channels to transformer_embed_dim\n",
    "        x = self.encoder_to_transformer_proj(x) # (B, SeqLen, embed_dim)\n",
    "\n",
    "        # Add Positional Encoding\n",
    "        x = self.pos_embedding(x) # (B, SeqLen, embed_dim)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x) # (B, SeqLen, embed_dim)\n",
    "\n",
    "        # Map to 2D Latent Grid for Decoder\n",
    "        # Project embed_dim to transformer_output_channels (which is decoder_channels[0])\n",
    "        x = self.transformer_to_decoder_reshape_proj(x) # (B, SeqLen, C_dec_init)\n",
    "\n",
    "        # Reshape sequence to latent 2D grid: (B, C_dec_init, H_latent, W_latent)\n",
    "        # Need to permute first to get channels first: (B, C_dec_init, SeqLen)\n",
    "        x = x.permute(0, 2, 1) # (B, C_dec_init, SeqLen)\n",
    "        \n",
    "        # Reshape SeqLen into H_latent * W_latent\n",
    "        x = x.reshape(B, self.transformer_output_channels, self.H_latent, self.W_latent) # (B, C_dec_init, H_latent, W_latent)\n",
    "\n",
    "        # 2D Decoder\n",
    "        x = self.decoder_2d(x) # (B, C_final, H, W)\n",
    "\n",
    "        # Final Output Layer\n",
    "        x = self.final_conv(x) # (B, 1, H, W)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EMA Utility ---\n",
    "\n",
    "class EMA:\n",
    "    \"\"\" \n",
    "    Exponential Moving Average\n",
    "    Maintains a shadow copy of the model weights and updates them\n",
    "    using a decay rate during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "        # Register model parameters\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\" Update the shadow weights. \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        \"\"\" Copy shadow weights to the model parameters. \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data.clone() # Store original weights\n",
    "                param.data.copy_(self.shadow[name])\n",
    "\n",
    "    def restore(self):\n",
    "        \"\"\" Restore original model weights from backup. \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data.copy_(self.backup[name])\n",
    "        self.backup = {} # Clear backup after restoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define input and output dimensions\n",
    "    # (C, S, T, R) where S=sources, T=time, R=receivers\n",
    "    input_dims = (1, 64, 256, 64) # Example: 1 channel, 64 sources, 256 time, 64 receivers\n",
    "    # (C, H, W) where H=height, W=width of velocity map\n",
    "    output_dims = (1, 128, 128) # Example: 1 channel, 128x128 velocity map\n",
    "\n",
    "    # Configure model parameters\n",
    "    convnext_channels = [32, 64, 128]\n",
    "    convnext_depths = [2, 2, 2]\n",
    "    # Example strides: Initial conv (1,2,1), then stages downsample by (2,2,2), (2,2,2)\n",
    "    convnext_downsample_strides = [(1, 2, 1), (2, 2, 2), (2, 2, 2)]\n",
    "    \n",
    "    transformer_layers = 3\n",
    "    transformer_heads = 4\n",
    "    transformer_embed_dim = 128\n",
    "    transformer_ffn_dim = 512\n",
    "\n",
    "    decoder_channels = [64, 32]\n",
    "    # Example strides: Need to go from latent (H_latent, W_latent) to output (128, 128)\n",
    "    # If latent is (16, 16), need 8x upsampling. Strides (2,2), (2,2), (2,2) give 8x.\n",
    "    # Let's calculate required H_latent, W_latent\n",
    "    total_upsample_H = 1\n",
    "    total_upsample_W = 1\n",
    "    for s_h, s_w in [(2,2), (2,2)]: # Example decoder strides\n",
    "        total_upsample_H *= s_h\n",
    "        total_upsample_W *= s_w\n",
    "    \n",
    "    # Required latent size calculation based on target output and *proposed* decoder strides\n",
    "    H_latent_req = output_dims[1] // total_upsample_H\n",
    "    W_latent_req = output_dims[2] // total_upsample_W\n",
    "    \n",
    "    print(f\"Required latent decoder size based on decoder strides ({[(2,2), (2,2)]}): ({H_latent_req}, {W_latent_req})\")\n",
    "    \n",
    "    decoder_upsample_strides = [(2, 2), (2, 2)] # Example 2D decoder upsampling strides\n",
    "\n",
    "    # --- Check if dimensions match ---\n",
    "    # Calculate final 3D encoder output dimensions given input_dims and convnext_downsample_strides\n",
    "    c, s, t, r = input_dims\n",
    "    \n",
    "    # Initial conv\n",
    "    s = math.floor((s - 4 + 2*1) / convnext_downsample_strides[0][0]) + 1\n",
    "    t = math.floor((t - 4 + 2*1) / convnext_downsample_strides[0][1]) + 1\n",
    "    r = math.floor((r - 4 + 2*1) / convnext_downsample_strides[0][2]) + 1\n",
    "    \n",
    "    # Stages downsampling\n",
    "    for i in range(1, len(convnext_downsample_strides)):\n",
    "        ds_stride = convnext_downsample_strides[i]\n",
    "        s = math.floor((s - 2) / ds_stride[0]) + 1\n",
    "        t = math.floor((t - 2) / ds_stride[1]) + 1\n",
    "        r = math.floor((r - 2) / ds_stride[2]) + 1\n",
    "        \n",
    "    final_S = s\n",
    "    final_T = t\n",
    "    final_R = r\n",
    "    calculated_seq_len = final_S * final_T * final_R\n",
    "    \n",
    "    print(f\"Calculated 3D encoder output spatial dims: ({final_S}, {final_T}, {final_R})\")\n",
    "    print(f\"Calculated transformer sequence length: {calculated_seq_len}\")\n",
    "\n",
    "    # Check if calculated SeqLen matches required latent size H_latent_req * W_latent_req\n",
    "    if calculated_seq_len != H_latent_req * W_latent_req:\n",
    "         print(\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "         print(\"!! WARNING: Calculated sequence length from 3D encoder does NOT match !!\")\n",
    "         print(\"!! required latent decoder size based on decoder strides.   !!\")\n",
    "         print(\"!! This model configuration will fail due to reshape error. !!\")\n",
    "         print(f\"!! SeqLen ({calculated_seq_len}) != H_latent*W_latent ({H_latent_req * W_latent_req}) !!\")\n",
    "         print(\"!! Adjust convnext_downsample_strides or decoder_upsample_strides to match !!\")\n",
    "         print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "         \n",
    "         # Let's adjust decoder strides to make the example runnable.\n",
    "         # We need total_upsample_H * total_upsample_W such that\n",
    "         # (output_H / total_upsample_H) * (output_W / total_upsample_W) == calculated_seq_len\n",
    "         # This is complex. Let's simplify the example config to make it work.\n",
    "         # Target H, W = 128, 128\n",
    "         # Let's aim for latent (16, 16). SeqLen = 256.\n",
    "         # Need total upsample 8x in H, 8x in W. Decoder strides: (2,2), (2,2), (2,2).\n",
    "         decoder_upsample_strides = [(2,2), (2,2), (2,2)]\n",
    "         decoder_channels = [64, 32, 16] # Needs one more stage\n",
    "         \n",
    "         # Recalculate required latent size\n",
    "         total_upsample_H = math.prod([s[0] for s in decoder_upsample_strides])\n",
    "         total_upsample_W = math.prod([s[1] for s in decoder_upsample_strides])\n",
    "         H_latent_req = output_dims[1] // total_upsample_H # 128 // 8 = 16\n",
    "         W_latent_req = output_dims[2] // total_upsample_W # 128 // 8 = 16\n",
    "         required_seq_len = H_latent_req * W_latent_req # 16 * 16 = 256\n",
    "         print(f\"Adjusted decoder strides and required latent size: ({H_latent_req}, {W_latent_req}), SeqLen={required_seq_len}\")\n",
    "\n",
    "         # Now adjust ConvNeXt strides so calculated_seq_len = required_seq_len = 256\n",
    "         # Input (64, 256, 64)\n",
    "         # Need final_S * final_T * final_R = 256\n",
    "         # Let's try downsampling: S/8, T/16, R/8.\n",
    "         # (64/8) * (256/16) * (64/8) = 8 * 16 * 8 = 1024. Too big.\n",
    "         # Need more aggressive downsampling or different structure.\n",
    "         # Maybe S/8, T/32, R/8? (64/8)*(256/32)*(64/8) = 8*8*8 = 512. Still too big.\n",
    "         # Maybe S/16, T/16, R/16? (64/16)*(256/16)*(64/16) = 4*16*4 = 256. This works!\n",
    "         \n",
    "         convnext_downsample_strides = [(2, 2, 2), (2, 2, 2), (2, 2, 2)] # Total 8x downsampling initially\n",
    "         # Recalculate dimensions with new strides:\n",
    "         s_new, t_new, r_new = input_dims[1:]\n",
    "         s_new = math.floor((s_new - 4 + 2*1) / convnext_downsample_strides[0][0]) + 1 # (64-4+2)/2 + 1 = 31+1 = 32?\n",
    "         t_new = math.floor((t_new - 4 + 2*1) / convnext_downsample_strides[0][1]) + 1 # (256-4+2)/2 + 1 = 127+1 = 128?\n",
    "         r_new = math.floor((r_new - 4 + 2*1) / convnext_downsample_strides[0][2]) + 1 # (64-4+2)/2 + 1 = 31+1 = 32?\n",
    "         # Using kernel 4, stride 2 initially.\n",
    "         \n",
    "         # Let's use kernel 2, stride 2 between stages after the first block as planned.\n",
    "         # Input (64, 256, 64)\n",
    "         # Stage 1 (initial conv, kernel 4, stride 2): (31, 127, 31) using floor. Use ceil for simplicity in design? No, floor is correct.\n",
    "         # The calculation needs to be precise based on padding.\n",
    "         # Let's adjust input dims slightly or make strides simpler.\n",
    "         # Simpler strides: Initial (1,1,1) kernel 7, padding 3 -> (64, 256, 64)\n",
    "         # Then stages downsample 2x, 2x, 2x\n",
    "         # (64, 256, 64) -> (32, 128, 32) -> (16, 64, 16) -> (8, 32, 8)\n",
    "         # SeqLen = 8 * 32 * 8 = 2048. This doesn't match 256.\n",
    "         \n",
    "         # Let's use the required SeqLen (256) and H_latent, W_latent (16, 16) as fixed points\n",
    "         # and work backwards to find suitable encoder downsampling.\n",
    "         # Final encoder dims (S', T', R') must satisfy S'*T'*R' = 256.\n",
    "         # Possible (S', T', R'): (8, 4, 8), (4, 8, 8), (8, 8, 4), (16, 4, 4), etc.\n",
    "         # If input is (64, 256, 64), need total downsampling of 64/S', 256/T', 64/R'.\n",
    "         # E.g., (8, 4, 8) -> Need 8x in S, 64x in T, 8x in R.\n",
    "         # Can we achieve 64x in T with ConvNeXt strides like (1,2,1), (2,2,2), (2,2,2)?\n",
    "         # Initial (1,2,1) -> 2x in T. Then three (2,2,2) stages -> 2*2*2 = 8x in T. Total 2 * 8 = 16x in T. Not 64x.\n",
    "         \n",
    "         # The dimension matching is crucial and depends entirely on the specific layer configurations.\n",
    "         # Let's define a config that *should* work for the (64, 256, 64) input and (128, 128) output.\n",
    "         # Target latent (16, 16), SeqLen 256. Upsample needed 8x in H, 8x in W. Decoder strides (2,2), (2,2), (2,2) (3 stages).\n",
    "         decoder_upsample_strides = [(2, 2), (2, 2), (2, 2)]\n",
    "         decoder_channels = [64, 32, 16] # 3 stages\n",
    "         # Needs input to decoder be (B, 64, 16, 16)\n",
    "         \n",
    "         # Need S'*T'*R' = 256 from encoder.\n",
    "         # Let's use input (64, 128, 64) to simplify. (Different input dims)\n",
    "         input_dims = (1, 64, 128, 64)\n",
    "         # Initial stride (1,1,1) k=4, p=1 -> (61, 125, 61) - complex.\n",
    "         # Simpler: initial stride (1,1,1), k=7, p=3 -> (64, 128, 64)\n",
    "         # Then 3 stages, each downsample 2x, 2x, 2x.\n",
    "         # (64, 128, 64) -> (32, 64, 32) -> (16, 32, 16) -> (8, 16, 8)\n",
    "         # SeqLen = 8 * 16 * 8 = 1024.\n",
    "         # Need SeqLen = 256. How about (4, 8, 8)? 4*8*8=256.\n",
    "         # From (64, 128, 64) to (4, 8, 8). Total downsampling: S=16x, T=16x, R=8x.\n",
    "         # Initial (1,1,1). Stage 1 (2,2,1), Stage 2 (2,2,2), Stage 3 (4,4,4)?\n",
    "         # Let's try simpler strides:\n",
    "         # Initial (1,1,1) k=7, p=3\n",
    "         # Stage 1 (4,4,2) -> (16, 32, 32)\n",
    "         # Stage 2 (4,4,4) -> (4, 8, 8) -> SeqLen = 4*8*8=256. This works!\n",
    "         \n",
    "         convnext_channels = [64, 128, 256] # 3 stages\n",
    "         convnext_depths = [2, 2, 2]\n",
    "         convnext_downsample_strides = [(1, 1, 1), (4, 4, 2), (4, 4, 4)] # Strides *between* stages (plus initial k=7, p=3).\n",
    "         # Initial conv should have kernel and stride to transition from input_dims channels to convnext_channels[0]\n",
    "         # and *optionally* downsample. Let's make the initial conv just change channels, stride 1.\n",
    "         # And the *first* downsampling layer is after the first stage's blocks.\n",
    "         \n",
    "         print(\"\\n--- Adjusted Configuration for Dimensional Consistency ---\")\n",
    "         input_dims = (1, 64, 128, 64) # New input dims\n",
    "         output_dims = (1, 128, 128)   # Target output dims\n",
    "         \n",
    "         # ConvNeXt Encoder Config\n",
    "         # Initial conv: input_dims[0] -> convnext_channels[0], stride 1\n",
    "         # Stages: convnext_depths[i] blocks\n",
    "         # Downsampling layers *between* stages i and i+1.\n",
    "         convnext_channels = [32, 64, 128] # 3 stages\n",
    "         convnext_depths = [2, 2, 2]      # 2 blocks per stage\n",
    "         # Downsampling strides between stages:\n",
    "         # From stage 0 -> stage 1: (4, 4, 2)\n",
    "         # From stage 1 -> stage 2: (4, 4, 4)\n",
    "         convnext_inter_stage_downsample_strides = [(4, 4, 2), (4, 4, 4)] # List length = num_stages - 1\n",
    "         \n",
    "         # Decoder Config\n",
    "         # Target output (128, 128)\n",
    "         # Target latent (16, 16), SeqLen 256. Upsample 8x, 8x.\n",
    "         decoder_upsample_strides = [(2, 2), (2, 2), (2, 2)] # 3 stages\n",
    "         decoder_channels = [64, 32, 16] # Number of stages should match upsample strides length + 1 (or length)\n",
    "\n",
    "         # Let's verify the pipeline with this config:\n",
    "         # Input: (B, 1, 64, 128, 64)\n",
    "         # Initial Conv (1->32, k=3, p=1, s=1): (B, 32, 64, 128, 64) # Using k=3, p=1 for simplicity\n",
    "         # Stage 0 blocks (2 blocks): (B, 32, 64, 128, 64)\n",
    "         # Downsample 0->1 (32->64, k=2, s=(4,4,2)): (B, 64, (64-2)/4+1, (128-2)/4+1, (64-2)/2+1) = (B, 64, 16, 32, 32) - Using floor\n",
    "         # Stage 1 blocks (2 blocks): (B, 64, 16, 32, 32)\n",
    "         # Downsample 1->2 (64->128, k=2, s=(4,4,4)): (B, 128, (16-2)/4+1, (32-2)/4+1, (32-2)/4+1) = (B, 128, 4, 8, 8)\n",
    "         # Stage 2 blocks (2 blocks): (B, 128, 4, 8, 8)\n",
    "         # Final Encoder dims: S'=4, T'=8, R'=8. C_3d = 128.\n",
    "         # SeqLen = 4 * 8 * 8 = 256. Matches required 256! Latent (16, 16).\n",
    "         # Transformer embed_dim = 128 (let's match final_3d_channels for simplicity, or keep 256)\n",
    "         transformer_embed_dim = 128\n",
    "         transformer_ffn_dim = 4 * transformer_embed_dim # Standard\n",
    "         \n",
    "         # Transformer input: (B, 256, 128)\n",
    "         # Transformer output: (B, 256, 128)\n",
    "         # Project to decoder initial channels: (B, 256, 64)\n",
    "         # Reshape to (B, 64, 16, 16). This works.\n",
    "         # Decoder stages:\n",
    "         # (B, 64, 16, 16) -> ConvT(64->32, k=2, s=2) -> (B, 32, 32, 32) # Using k=s here for simplicity\n",
    "         # (B, 32, 32, 32) -> ConvT(32->16, k=2, s=2) -> (B, 16, 64, 64)\n",
    "         # (B, 16, 64, 64) -> ConvT(16->16, k=2, s=2) -> (B, 16, 128, 128) # Final decoder stage output channels\n",
    "\n",
    "         decoder_channels = [32, 16, 16] # Channels *after* each ConvTranspose stage\n",
    "         # Initial channels for 2D decoder is convnext_channels[-1] (128) -> projected?\n",
    "         # Let's set the initial channels for the *2D decoder* separately.\n",
    "         decoder_2d_start_channels = 64 # Should be <= transformer_embed_dim after proj\n",
    "\n",
    "         print(\"Using adjusted configuration:\")\n",
    "         print(f\"  Input Dims: {input_dims}\")\n",
    "         print(f\"  Output Dims: {output_dims}\")\n",
    "         print(f\"  ConvNeXt Channels: {convnext_channels}\")\n",
    "         print(f\"  ConvNeXt Depths: {convnext_depths}\")\n",
    "         # Print initial conv config assumed for calculation\n",
    "         print(f\"  ConvNeXt Initial Conv: k=3, p=1, s=1 (adjust if needed)\")\n",
    "         print(f\"  ConvNeXt Inter-Stage Downsample Strides: {convnext_inter_stage_downsample_strides}\")\n",
    "         print(f\"  Calculated Final Encoder Spatial Dims: ({final_S}, {final_T}, {final_R})\")\n",
    "         print(f\"  Calculated Transformer SeqLen: {calculated_seq_len}\")\n",
    "         print(f\"  Transformer Embed Dim: {transformer_embed_dim}\")\n",
    "         print(f\"  Decoder 2D Start Channels (after Transformer proj): {decoder_2d_start_channels}\")\n",
    "         print(f\"  Decoder Latent Size (H_latent, W_latent): ({H_latent_req}, {W_latent_req})\")\n",
    "         print(f\"  Decoder Upsample Strides (H, W): {decoder_upsample_strides}\")\n",
    "         print(f\"  Decoder Output Channels (per stage): {decoder_channels}\")\n",
    "         print(\"-\" * 30)\n",
    "\n",
    "         # Update model initialization parameters based on adjusted config\n",
    "         model = WaveformInversionModel(\n",
    "             input_dims=input_dims,\n",
    "             output_dims=output_dims,\n",
    "             convnext_channels=convnext_channels,\n",
    "             convnext_depths=convnext_depths,\n",
    "             # Need to pass the structure to the model\n",
    "             # A more flexible model class would take a list of stage configs\n",
    "             # For this demo, hardcode the initial conv and inter-stage downsampling\n",
    "             transformer_layers=transformer_layers,\n",
    "             transformer_heads=transformer_heads,\n",
    "             transformer_embed_dim=transformer_embed_dim,\n",
    "             transformer_ffn_dim=transformer_ffn_dim,\n",
    "             decoder_channels=decoder_channels, # These are the *output* channels of ConvTranspose layers\n",
    "             decoder_upsample_strides=decoder_upsample_strides,\n",
    "             # Add parameters to pass calculated H_latent, W_latent and SeqLen\n",
    "             _H_latent=H_latent_req,\n",
    "             _W_latent=W_latent_req,\n",
    "             _sequence_length=calculated_seq_len,\n",
    "             _decoder_2d_start_channels=decoder_2d_start_channels\n",
    "         )\n",
    "         \n",
    "    else:\n",
    "        # Use original configuration if it happened to match (unlikely with random choices)\n",
    "         model = WaveformInversionModel(\n",
    "             input_dims=input_dims,\n",
    "             output_dims=output_dims,\n",
    "             convnext_channels=convnext_channels,\n",
    "             convnext_depths=convnext_depths,\n",
    "             convnext_downsample_strides=convnext_downsample_strides,\n",
    "             transformer_layers=transformer_layers,\n",
    "             transformer_heads=transformer_heads,\n",
    "             transformer_embed_dim=transformer_embed_dim,\n",
    "             transformer_ffn_dim=transformer_ffn_dim,\n",
    "             decoder_channels=decoder_channels,\n",
    "             decoder_upsample_strides=decoder_upsample_strides,\n",
    "             # Add parameters to pass calculated H_latent, W_latent and SeqLen\n",
    "             _H_latent=H_latent_req,\n",
    "             _W_latent=W_latent_req,\n",
    "             _sequence_length=calculated_seq_len,\n",
    "             _decoder_2d_start_channels=decoder_channels[0] # Use first decoder channel as start\n",
    "         )\n",
    "         print(\"\\nUsing original (likely failing) configuration.\") # Will print error inside __init__\n",
    "\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    # print(model) # Can be very verbose\n",
    "\n",
    "    # Create dummy data\n",
    "    # (batch_size, num_sources, time_steps, num_receivers)\n",
    "    batch_size = 2\n",
    "    dummy_input = torch.randn(batch_size, input_dims[1], input_dims[2], input_dims[3])\n",
    "    print(f\"\\nDummy Input Shape: {dummy_input.shape}\")\n",
    "\n",
    "    # Instantiate EMA\n",
    "    ema_decay = 0.999\n",
    "    ema = EMA(model, ema_decay)\n",
    "    print(f\"EMA initialized with decay: {ema_decay}\")\n",
    "\n",
    "    # Example Forward Pass\n",
    "    with torch.no_grad(): # Typically inference/evaluation uses no_grad\n",
    "        # Optionally apply EMA shadow weights for evaluation\n",
    "        # ema.apply_shadow() \n",
    "        \n",
    "        output = model(dummy_input)\n",
    "        \n",
    "        # Restore original weights after evaluation if EMA was applied\n",
    "        # ema.restore() \n",
    "\n",
    "    print(f\"Output Shape: {output.shape}\")\n",
    "    # Expected output shape: (batch_size, 1, output_H, output_W)\n",
    "    expected_output_shape = (batch_size, output_dims[0], output_dims[1], output_dims[2])\n",
    "    assert output.shape == expected_output_shape\n",
    "    print(\"Output shape matches expected shape.\")\n",
    "\n",
    "    # Example of EMA update (in a training loop)\n",
    "    # Assume you have loss, optimizer, etc.\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    # ema.update() # <-- Call this after optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd633b05",
   "metadata": {},
   "source": [
    "### How to Use in Training:\n",
    "\n",
    "1. Instantiate the model.\n",
    "2. Instantiate the optimizer.\n",
    "3. Instantiate the EMA utility: ema = EMA(model, decay=0.999).\n",
    "4. In your training loop, after optimizer.step() (and zeroing gradients), call ema.update().\n",
    "5. For evaluation or inference:\n",
    " - Call ema.apply_shadow() to load the averaged weights.\n",
    " - Run your evaluation/inference pass: with torch.no_grad(): output = model(input).\n",
    " - Call ema.restore() to load the original training weights back if you plan to continue training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d560ad2",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "1. **LayerNorm3D:** A simple implementation of Layer Normalization for 3D tensors. Standard nn.LayerNorm works on the last normalized_shape dimensions. We adapt it to normalize over the channel dimension (C) for (B, C, D, H, W) input, similar to how LayerNorm is often used in ConvNeXt after depthwise convolutions.\n",
    "2. **ConvNeXtBlock3D:** Implements the core ConvNeXt block structure adapted to 3D:\n",
    " - Large kernel depthwise 3D convolution (nn.Conv3d with groups=dim).\n",
    " - LayerNorm3D over channels.\n",
    " - Pointwise convolutions (nn.Linear after permuting dimensions) with GELU activation, forming an inverted bottleneck structure (expand channels, then contract).\n",
    " - Optional Layer Scale (gamma).\n",
    " - Residual connection.\n",
    "3. **PositionalEncoding:** A standard sinusoidal positional encoding module for adding positional information to the sequence of tokens fed into the Transformer.\n",
    "4. **WaveformInversionModel:**\n",
    " - 3D Encoder: A nn.Sequential stack. It starts with an initial 3D convolution to adjust channel count and potentially downsample. Then, it iterates through defined stages, adding ConvNeXtBlock3Ds followed by a downsampling layer (nn.Conv3d with stride > 1) between stages. The strides and kernel sizes are critical for determining the final 3D spatial dimensions (final_S, final_T, final_R).\n",
    " - Flatten & Project: The output (B, C_3d, S', T', R') is permuted and reshaped into a sequence (B, SeqLen, C_3d). A Linear layer projects the channels C_3d to the required transformer_embed_dim. Positional encoding is added to this sequence.\n",
    " - Transformer Encoder: A standard nn.TransformerEncoder consisting of multiple TransformerEncoderLayers processes the sequence.\n",
    " - Map to 2D Latent: The output sequence (B, SeqLen, embed_dim) is first projected to decoder_2d_start_channels using a Linear layer. Then, it's permuted and reshaped from (B, decoder_2d_start_channels, SeqLen) into (B, decoder_2d_start_channels, H_latent, W_latent). This reshape requires H_latent * W_latent == SeqLen. The __init__ method includes a check and raises an error if the dimensions don't match, as getting this right with arbitrary inputs/outputs/strides requires careful planning or a more complex dynamic reshaping/projection mechanism. The example config provided after the warning is designed to meet this requirement for the specific input/output sizes.\n",
    " - 2D Decoder: A nn.Sequential stack of nn.ConvTranspose2d layers. These layers upsample the 2D grid from (H_latent, W_latent) to the target output size (H, W). The strides determine the upsampling factor at each stage.\n",
    " - Output Layer: A final nn.Conv2d maps the decoder's output channels to the single channel required for the velocity map.\n",
    " - Dimensionality Check: The __init__ includes logic to calculate the expected sequence length after the 3D encoder based on the chosen strides and compares it to the sequence length required by the 2D decoder based on its strides and the target output size. This is crucial for the model to be structurally valid.\n",
    "5. **EMA Utility:** A standard class to manage the Exponential Moving Average of model weights. It creates a shadow copy of the parameters. The update() method is called during training to move the shadow weights towards the current model weights. apply_shadow() copies the smoothed weights into the model (typically for evaluation or inference), and restore() copies the original weights back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69dfde1",
   "metadata": {},
   "source": [
    "## Architecture \n",
    "\n",
    "Okay, let's design a sample PyTorch model that combines a 3D ConvNeXt-like encoder for processing the 4D waveform data (S, T, R) per sample, a Transformer encoder for global feature integration, a decoder to reconstruct the 2D velocity map (H, W), and incorporates EMA for training stability.\n",
    "\n",
    "The main challenge is mapping the 4D input (batch_size, num_sources, time_steps, num_receivers) to the 2D output (batch_size, height, width). The 3D CNN will process the (num_sources, time_steps, num_receivers) part as a 3D volume (potentially adding a channel dimension first). It will extract features and reduce dimensions. The Transformer will process the flattened feature volume or a sequence derived from it. The decoder will take the Transformer output and reconstruct the 2D spatial grid.\n",
    "\n",
    "##### Here's a possible architecture flow:\n",
    "\n",
    "1. **Input:** (B, S, T, R) -> Add channel: (B, 1, S, T, R)\n",
    "2. **3D ConvNeXt Encoder:** Process (B, 1, S, T, R) through stages of 3D ConvNeXt blocks with downsampling. Output: (B, C_3d, S', T', R').\n",
    "3. **Flatten & Project to Sequence:** Reshape (B, C_3d, S', T', R') to (B, S'*T'*R', C_3d). Apply a Linear layer to project C_3d to Transformer's embed_dim. Add positional embeddings. Output: (B, SeqLen, embed_dim), where SeqLen = S'*T'*R'.\n",
    "4. **Transformer Encoder:** Process the sequence (B, SeqLen, embed_dim) through standard Transformer layers. Output: (B, SeqLen, embed_dim).\n",
    "5. **Map to 2D Latent Grid:** Apply a Linear layer to project embed_dim to decoder's initial channels C_dec_init. Reshape (B, SeqLen, C_dec_init) to (B, C_dec_init, H_latent, W_latent), where H_latent * W_latent = SeqLen. This reshape is a critical assumption about how the sequence maps back to space. We'll need to ensure SeqLen allows for a reasonable H_latent, W_latent.\n",
    "6. **2D Decoder:** Use 2D Transposed Convolutions (nn.ConvTranspose2d) to upsample from (B, C_dec_init, H_latent, W_latent) to (B, C_final, H, W).\n",
    "7. **Output Layer:** A final 2D Convolution to get (B, 1, H, W).\n",
    "And the EMA will be a utility applied externally during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec99956",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79383f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f87f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e68d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
