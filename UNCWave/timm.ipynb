{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c443a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "class SimpleTimmModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple model using a timm backbone for feature extraction,\n",
    "    followed by custom layers to produce a spatial output map.\n",
    "    Handles input channel adaptation and spatial resizing via interpolation.\n",
    "    \"\"\"\n",
    "    def __init__(self, timm_model_name, in_channels, target_output_hw):\n",
    "        super().__init__()\n",
    "        self.timm_model_name = timm_model_name\n",
    "        self.in_channels = in_channels\n",
    "        self.target_output_hw = target_output_hw\n",
    "\n",
    "        # --- 1. Load the backbone ---\n",
    "        # num_classes=0 ensures we get the backbone *without* the classification head.\n",
    "        # pretrained=True loads weights if available.\n",
    "        print(f\"Loading timm model: {timm_model_name} with num_classes=0 and pretrained=True\")\n",
    "        try:\n",
    "            self.backbone = timm.create_model(timm_model_name, pretrained=True, num_classes=0)\n",
    "            print(\"Backbone loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading timm model {timm_model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # --- 2. Adapt input channels if necessary ---\n",
    "        # Standard pretrained models expect 3 input channels.\n",
    "        # If our input has a different number (like 5), we need to modify the first layer.\n",
    "        # This involves finding the first Conv2d layer and replacing it.\n",
    "        original_first_conv = None\n",
    "        first_conv_name = None\n",
    "\n",
    "        # Find the first Conv2d layer in the backbone\n",
    "        for name, module in self.backbone.named_modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                original_first_conv = module\n",
    "                first_conv_name = name\n",
    "                break # Found the first one, stop\n",
    "\n",
    "        if original_first_conv is None:\n",
    "             raise AttributeError(f\"Could not find *any* Conv2d layer in {timm_model_name}. Simple adaptation for input channels is not possible for this model type (e.g., ViT).\")\n",
    "\n",
    "        print(f\"Found first convolutional layer: '{first_conv_name}' with {original_first_conv.in_channels} input channels.\")\n",
    "\n",
    "        if in_channels != original_first_conv.in_channels:\n",
    "            print(f\"Adapting first convolutional layer from {original_first_conv.in_channels} to {in_channels} input channels.\")\n",
    "            # Create a new conv layer with the desired in_channels but same properties\n",
    "            new_first_conv = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=original_first_conv.out_channels,\n",
    "                kernel_size=original_first_conv.kernel_size,\n",
    "                stride=original_first_conv.stride,\n",
    "                padding=original_first_conv.padding,\n",
    "                bias=original_first_conv.bias is not None\n",
    "            )\n",
    "\n",
    "            # Optional: Copy weights for the first 3 channels from the pretrained model\n",
    "            # This allows the model to leverage the learned features for the initial 3 channels.\n",
    "            # The weights for the new channels (5-3=2 in your case) are randomly initialized.\n",
    "            if original_first_conv.in_channels == 3:\n",
    "                 print(\"Copying weights for initial 3 input channels from pretrained model.\")\n",
    "                 with torch.no_grad():\n",
    "                     # Copy weights for the first 3 channels\n",
    "                     new_first_conv.weight.data[:, :3, :, :].copy_(original_first_conv.weight.data)\n",
    "                     # Initialize weights for the new channels (e.g., with zeros or small random values)\n",
    "                     if in_channels > 3:\n",
    "                         # Simple zero initialization for extra channels\n",
    "                          new_first_conv.weight.data[:, 3:, :, :].zero_()\n",
    "                     # Copy bias if it exists\n",
    "                     if original_first_conv.bias is not None:\n",
    "                         new_first_conv.bias.copy_(original_first_conv.bias.data)\n",
    "            else:\n",
    "                 print(\"Original model did not have 3 input channels. Not copying weights.\")\n",
    "\n",
    "\n",
    "            # Replace the original first convolutional layer in the backbone's module hierarchy\n",
    "            # This requires navigating the module structure.\n",
    "            # For instance, if name is 'conv1', parent is self.backbone and child name is 'conv1'.\n",
    "            # If name is 'features.0', parent is self.backbone.features and child name is '0'.\n",
    "            parts = first_conv_name.rsplit('.', 1)\n",
    "            if len(parts) == 1: # Top level module\n",
    "                 parent_module = self.backbone\n",
    "                 child_name = parts[0]\n",
    "            else: # Nested module\n",
    "                 parent_name = parts[0]\n",
    "                 child_name = parts[1]\n",
    "                 parent_module = self.backbone.get_submodule(parent_name)\n",
    "\n",
    "            setattr(parent_module, child_name, new_first_conv)\n",
    "            print(f\"Replaced '{first_conv_name}' layer.\")\n",
    "\n",
    "        else:\n",
    "            print(\"Input channels match backbone. No adaptation needed for the first conv layer.\")\n",
    "\n",
    "\n",
    "        # --- 3. Determine backbone output *feature map* shape ---\n",
    "        # We need to know the number of channels output by the backbone's\n",
    "        # feature extractor before any potential global pooling.\n",
    "        # timm models often have a `forward_features` method for this.\n",
    "        print(\"Determining backbone output feature map shape using a dummy tensor...\")\n",
    "        # Use a dummy input tensor with the expected shape (1 batch, your channels, example H, example W)\n",
    "        # The spatial dimensions (1000, 70) are large, the backbone will downsample significantly.\n",
    "        dummy_input = torch.randn(1, self.in_channels, 1000, 70)\n",
    "\n",
    "        # Ensure the dummy input is on the same device as the model parameters (important!)\n",
    "        # Get the device from one of the backbone's parameters\n",
    "        device = next(self.backbone.parameters()).device\n",
    "        dummy_input = dummy_input.to(device)\n",
    "        print(f\"Dummy input tensor shape: {dummy_input.shape} on device: {device}\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Pass the dummy input through the feature extraction part only\n",
    "            with torch.no_grad():\n",
    "                 backbone_features = self.backbone.forward_features(dummy_input)\n",
    "\n",
    "            # Get the shape of the output feature map\n",
    "            # Should be (1, channels, height, width)\n",
    "            self.backbone_out_channels = backbone_features.shape[1]\n",
    "            # Note: The spatial dimensions (H, W) here depend heavily on the backbone\n",
    "            # and the input size (1000x70). They will likely be much smaller than 1000x70\n",
    "            # due to the backbone's downsampling layers.\n",
    "            self.backbone_out_h = backbone_features.shape[2]\n",
    "            self.backbone_out_w = backbone_features.shape[3]\n",
    "            print(f\"Backbone `forward_features` output shape: {backbone_features.shape}\")\n",
    "            print(f\"Determined backbone output channels: {self.backbone_out_channels}\")\n",
    "\n",
    "        except Exception as e:\n",
    "             print(f\"Error determining backbone output shape using `forward_features`. Does '{timm_model_name}' have this method or does it work with input shape {dummy_input.shape}?\")\n",
    "             print(f\"Error details: {e}\")\n",
    "             # This shape is crucial, so re-raise if we can't get it\n",
    "             raise RuntimeError(\"Failed to determine backbone output feature map shape.\") from e\n",
    "\n",
    "\n",
    "        # --- 4. Add custom layers to transform backbone output to target shape ---\n",
    "        # Target output shape: (N, 1, 70, 70)\n",
    "        # Backbone output shape: (N, backbone_out_channels, backbone_out_h, backbone_out_w)\n",
    "\n",
    "        # Layer 1: Reduce channels from `backbone_out_channels` to 1.\n",
    "        # A 1x1 convolution is suitable for this.\n",
    "        self.channel_reducer = nn.Conv2d(self.backbone_out_channels, 1, kernel_size=1)\n",
    "        print(f\"Added channel reducer layer: {self.backbone_out_channels} -> 1 channels.\")\n",
    "\n",
    "        # Layer 2: Spatially transform the feature map from (backbone_out_h, backbone_out_w)\n",
    "        # to the target spatial size (70, 70).\n",
    "        # Since the backbone heavily downsamples a 1000x70 input, the output\n",
    "        # spatial size will likely be much smaller than 70x70. We need upsampling.\n",
    "        # F.interpolate is a simple way to resize. Alternatively, ConvTranspose2d could be used\n",
    "        # if learnable upsampling is desired. We'll use interpolation for simplicity.\n",
    "        # The interpolation happens within the forward pass, not as a module here.\n",
    "\n",
    "        print(f\"Model head will reduce channels to 1 and interpolate to target spatial size {self.target_output_hw}.\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the simple model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (N, in_channels, H, W).\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (N, 1, target_output_h, target_output_w).\n",
    "        \"\"\"\n",
    "        # Ensure input has 4 dimensions (N, C, H, W)\n",
    "        if x.ndim != 4:\n",
    "             raise ValueError(f\"Expected input tensor to have 4 dimensions (N, C, H, W), but got {x.ndim}\")\n",
    "\n",
    "        # 1. Pass input through the backbone's feature extraction path\n",
    "        # Use forward_features to get the output *before* any global pooling that\n",
    "        # might be present even with num_classes=0.\n",
    "        features = self.backbone.forward_features(x)\n",
    "        # Shape of features: (N, backbone_out_channels, backbone_out_h, backbone_out_w)\n",
    "        # print(f\"Backbone features shape: {features.shape}\") # Optional: uncomment for debugging\n",
    "\n",
    "        # 2. Apply the channel reduction layer\n",
    "        output = self.channel_reducer(features)\n",
    "        # Shape of output: (N, 1, backbone_out_h, backbone_out_w)\n",
    "        # print(f\"After channel reduction shape: {output.shape}\") # Optional\n",
    "\n",
    "        # 3. Interpolate to the target spatial size\n",
    "        # Use bilinear interpolation for continuous data.\n",
    "        # align_corners=False is generally recommended for bilinear interpolation\n",
    "        # unless you have a specific reason to align corners (e.g., matching pixel centers vs corners).\n",
    "        output = F.interpolate(\n",
    "            output,\n",
    "            size=self.target_output_hw,\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        # Shape of output: (N, 1, target_output_h, target_output_w)\n",
    "        # print(f\"After interpolation shape: {output.shape}\") # Optional\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b1241f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating the simple model...\n",
      "Loading timm model: resnet18 with num_classes=0 and pretrained=True\n",
      "Backbone loaded successfully.\n",
      "Found first convolutional layer: 'conv1' with 3 input channels.\n",
      "Adapting first convolutional layer from 3 to 5 input channels.\n",
      "Copying weights for initial 3 input channels from pretrained model.\n",
      "Replaced 'conv1' layer.\n",
      "Determining backbone output feature map shape using a dummy tensor...\n",
      "Dummy input tensor shape: torch.Size([1, 5, 1000, 70]) on device: cpu\n",
      "Backbone `forward_features` output shape: torch.Size([1, 512, 32, 3])\n",
      "Determined backbone output channels: 512\n",
      "Added channel reducer layer: 512 -> 1 channels.\n",
      "Model head will reduce channels to 1 and interpolate to target spatial size (70, 70).\n",
      "Model instantiated successfully.\n",
      "Model summary:\n",
      "SimpleTimmModel(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(5, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act1): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (channel_reducer): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "\n",
      "Testing forward pass with dummy input shape: torch.Size([4, 5, 1000, 70])\n",
      "Dummy model output shape: torch.Size([4, 1, 70, 70])\n",
      "Model output shape matches expected shape.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Example Usage (using your cfg structure) ---\n",
    "# Assume cfg is defined elsewhere, e.g.:\n",
    "# class Config:\n",
    "#      def __init__(self):\n",
    "#          self.backbone = 'resnet18' # Or 'efficientnet_b0', etc.\n",
    "#          self.batch_size = 4\n",
    "#          self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#\n",
    "# cfg = Config()\n",
    "# print(f\"Using device: {cfg.device}\")\n",
    "\n",
    "print(\"Instantiating the simple model...\")\n",
    "# Pass the configured backbone name, the expected input channels (5),\n",
    "# and the target output spatial size (70, 70).\n",
    "# Use the SimpleTimmModel class we defined.\n",
    "try:\n",
    "    model = SimpleTimmModel(\n",
    "        timm_model_name=cfg.backbone,\n",
    "        in_channels=5,\n",
    "        target_output_hw=(70, 70)\n",
    "    ).to(cfg.device)\n",
    "    print(\"Model instantiated successfully.\")\n",
    "    print(f\"Model summary:\\n{model}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to instantiate the model: {e}\")\n",
    "    model = None # Set model to None if instantiation failed\n",
    "\n",
    "# Check initial model output shape with a dummy input on the correct device\n",
    "if model: # Only proceed if model was instantiated\n",
    "    # if train_dl: # Assume train_dl exists and implies cfg.batch_size is valid\n",
    "        try:\n",
    "            # Create a dummy input tensor matching your described training data sample shape\n",
    "            # (batch_size, channels, height, width)\n",
    "            dummy_input = torch.randn(cfg.batch_size, 5, 1000, 70).to(cfg.device)\n",
    "            print(f\"\\nTesting forward pass with dummy input shape: {dummy_input.shape}\")\n",
    "\n",
    "            with torch.no_grad(): # No need for gradients during this shape check\n",
    "                 dummy_output = model(dummy_input)\n",
    "\n",
    "            print(f\"Dummy model output shape: {dummy_output.shape}\")\n",
    "\n",
    "            # Define the expected output shape\n",
    "            expected_output_shape = (cfg.batch_size, 1, 70, 70)\n",
    "\n",
    "            # Compare the actual output shape with the expected shape\n",
    "            if dummy_output.shape != expected_output_shape:\n",
    "                 print(f\"Warning: Model output shape {dummy_output.shape} does not match expected {expected_output_shape}.\")\n",
    "            else:\n",
    "                 print(\"Model output shape matches expected shape.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy model forward pass: {e}\")\n",
    "    # else:\n",
    "    #     print(\"\\nSkipping dummy model test as train_dl is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee594f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleTimmModel(\n",
       "  (backbone): ResNet(\n",
       "    (conv1): Conv2d(5, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (drop_block): Identity()\n",
       "        (act1): ReLU(inplace=True)\n",
       "        (aa): Identity()\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (channel_reducer): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
