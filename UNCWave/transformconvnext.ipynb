{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738acf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "はい、元のConvNeXtベースのモデル構造を理解し、時系列情報とジオフォン位置のずれ情報を取り込むためにTransformer要素を導入するコードを生成します。\n",
    "\n",
    "現在のコードを分析した結果、いくつか不明瞭な点がありますが、最も可能性の高いデータフローとモデル構造を推測し、それに沿ってTransformerを組み込みます。特に、`CustomDataset`でのデータローディングと`Net`クラスの`_update_stem`の処理に関する推測が含まれるため、実際のデータ形式によっては調整が必要になる可能性があります。\n",
    "\n",
    "**推測されるデータフローとモデル構造:**\n",
    "\n",
    "1.  **Raw Data:** `(T=1000, W=70)`または `(C=5, T=1000, W=70)` のwaveformデータと `(H=70, W=70)` のラベルデータ。\n",
    "2.  **`CustomDataset`:** 各ファイルから`N_samples_per_file`個のサンプルを読み込みます。元のコードの形状プリント`(5, 1000, 70), (70, 70)`と、\n",
    "    `in_chans=5`、出力`(70, 70)`から、各サンプルは恐らくロード後に`(5, T=1000, W=70)`と`(H=70, W=70)`のペアとして扱われていると考えられます。\n",
    "    しかし、`__getitem__`では`time_step_idx`でスライスされており、`(5, 70)`のような形状になるはずですが、ConvNeXtは`(C, H, W)`を入力とします。ここが不明瞭です。\n",
    "3.  **最も可能性の高い解釈:** データセットは`(N_samples_per_file, 5, H_in, 70)`の形状のデータと`(N_samples_per_file, 70, 70)`のラベルをロードしている。\n",
    "     ここで`H_in`は元の`T=1000`から何らかの方法で変換された固定サイズ（例: 352）。`N_samples_per_file`は、元の`T=1000`から切り出せる独立したサンプル数\n",
    "     （例: 500）。`__getitem__`は、この`(N_samples, 5, H_in, 70)`から1サンプル`(5, H_in, 70)`を抜き出している。\n",
    "4.  **`Net` (`_update_stem`):** モデルへの入力は`(B, 5, H_in, 70)`であり、`_update_stem`はConvNeXtのstem層をこの特定の`(H_in, 70)`形状に適応させている。\n",
    "    特に高さ方向(`H_in`)に対して強いダウンサンプリングを行っている。\n",
    "5.  **ConvNeXt Encoder:** stem層の後、標準的な2D ConvNeXtステージ（stride 1, 2, 2, 2）で特徴を抽出し、マルチスケール特徴マップを生成する。\n",
    "6.  **U-Net Decoder:** Encoderからのマルチスケール特徴を入力として、アップサンプリングとSkip Connectionにより解像度を上げ、最終的に`(B, 1, 70, 70)`の出力を得る。\n",
    "\n",
    "**Transformer導入方針:**\n",
    "\n",
    "上記の解釈に基づき、TransformerをConvNeXt U-Net構造に組み込みます。\n",
    "時系列情報を取り込むため、`CustomDataset`を変更し、単一の時間ステップに対応するサンプルだけでなく、\n",
    "**複数の連続する時間ステップに対応するサンプルをスタック**して入力チャンネルとして使用します。\n",
    "これにより、モデルは近傍の時間ステップの情報を参照できるようになります。\n",
    "Transformerブロックは、ConvNeXt Encoderの中間層の後に挿入し、空間的な特徴に大域的なアテンションを適用します。\n",
    "\n",
    "**実装の変更点:**\n",
    "\n",
    "1.  **`CustomDataset`の修正:** `num_input_slices`パラメータを追加し、指定された数の連続するサンプル\n",
    "（各サンプルは`(5, H_in, 70)`形状を想定）をチャンネル次元で結合して返すように変更します。入力形状は`(B, 5 * num_input_slices, H_in, 70)`になります。\n",
    "2.  **`Net`クラスの修正:**\n",
    "    *   クラス名を`NetWithTransformer`とします。\n",
    "    *   `__init__`で`num_input_slices`を受け取り、ConvNeXt backboneの`in_chans`を`5 * num_input_slices`に変更します。\n",
    "    *   `_update_stem`関数を修正し、新しい`in_chans`に対応できるように、stemの最初の畳み込み層の入力チャンネル数を変更します。\n",
    "         `H_in`は引き続き固定値として扱います（元のstem logicから推測される352を仮定）。\n",
    "    *   ConvNeXt Encoderの中間ステージ（例: Stage 0の後）の出力に、`TransformerBlock2d`を挿入します。\n",
    "        このTransformerブロックは、2D特徴マップをシーケンスに平坦化し、位置エンコーディングを加えた後、Self-Attentionを適用し、再び2D形状に戻します。\n",
    "    *   Decoderへの入力は、Transformerブロックの出力で元のEncoderステージの出力を置き換えた、マルチスケール特徴マップのリストを使用します。\n",
    "3.  **`TransformerBlock2d`と`PositionalEncoding2D`の実装:** 2D特徴マップ用のTransformerエンコーダーブロックと位置エンコーディング層を定義します。\n",
    "\n",
    "以下に修正・追加したコードを生成します。\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11df912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add necessary imports at the top\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import timm\n",
    "from timm.models.convnext import ConvNeXtBlock\n",
    "from types import MethodType\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time, glob\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "import sys # Added for stderr\n",
    "from copy import deepcopy\n",
    "from timm.models.features import FeatureListNet\n",
    "\n",
    "RUN_TRAIN = True # bfloat16 or float32 recommended\n",
    "RUN_VALID = False\n",
    "RUN_TEST  = False\n",
    "USE_DEVICE = 'GPU' #'CPU'  # 'GPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553a4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Original cfg setup - replace or update as needed\n",
    "# For this code, we add transformer specific config here\n",
    "cfg= SimpleNamespace()\n",
    "cfg.device = torch.device(\"cuda\" if torch.cuda.is_available() and torch.cuda.device_count() > 0 else \"cpu\")\n",
    "# cfg.local_rank = 0 # Assume single GPU/CPU for simplicity unless distributed training is set up\n",
    "cfg.seed = 123\n",
    "cfg.subsample = 100 #None # Set to None to use all available samples\n",
    "\n",
    "# Assuming file paths are set up correctly\n",
    "data_paths_str = \"./datasetfiles/FlatVel_A/data/*.npy\"\n",
    "label_paths_str = \"./datasetfiles/FlatVel_A/model/*.npy\"\n",
    "\n",
    "# Get all file pairs\n",
    "# cfg.file_pairs = list(zip(sorted(glob.glob(data_paths_str)), sorted(glob.glob(label_paths_str))))\n",
    "# Split file pairs for train/validation\n",
    "data_paths = sorted(glob.glob(data_paths_str))\n",
    "label_paths = sorted(glob.glob(label_paths_str))\n",
    "all_file_pairs = list(zip(data_paths, label_paths))\n",
    "# Simple split (e.g., 80% train, 20% validation)\n",
    "split_ratio = 0.8\n",
    "split_idx = int(len(all_file_pairs) * split_ratio)\n",
    "train_file_pairs = all_file_pairs[:split_idx]\n",
    "valid_file_pairs = all_file_pairs[split_idx:]\n",
    "\n",
    "\n",
    "cfg.backbone = \"convnext_small.fb_in22k_ft_in1k\"\n",
    "cfg.ema = True\n",
    "cfg.ema_decay = 0.99\n",
    "\n",
    "cfg.epochs = 4\n",
    "cfg.batch_size = 8\n",
    "cfg.batch_size_val = 8\n",
    "\n",
    "cfg.early_stopping = {\"patience\": 3, \"streak\": 0}\n",
    "cfg.logging_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71b964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- New Transformer/Dataset related config ---\n",
    "cfg.num_input_slices = 5 # Number of consecutive input samples (time slices) to stack as channels\n",
    "# Inferred input height (H_in) for a single sample based on original stem logic transforming T=1000\n",
    "# This is highly speculative and assumes the original dataset or stem implicitly maps 1000 -> 352\n",
    "cfg.inferred_input_height = 352\n",
    "cfg.input_width = 70 # Original waveform width and target output width\n",
    "\n",
    "cfg.transformer_config = {\n",
    "    'num_layers': 2,      # Number of Transformer Encoder layers\n",
    "    'num_heads': 8,       # Number of attention heads\n",
    "    'hidden_dim': None,   # Hidden dimension for Transformer (defaults to input channels at insertion point)\n",
    "    # Decoder channels and scale factors should match the structure needed to upsample\n",
    "    # from the bottleneck/skip resolutions of the *modified* encoder to 70x70.\n",
    "    # Assuming the original decoder config was correct for the original encoder output resolutions.\n",
    "    # The decoder will receive [stage3, stage2, stage1, stage0_transformer_out, stem_out].\n",
    "    # These correspond to specific spatial sizes.\n",
    "    # If stage0 output (where Transformer is inserted) has spatial size H', W', and the decoder\n",
    "    # has 5 levels with scale 2, the bottleneck spatial size is roughly H'/16, W'/16.\n",
    "    # If the final output is 70x70, and the last decoder output is 70x70 or 72x72 for cropping,\n",
    "    # the scales should match the reduction factors of the encoder stages.\n",
    "    # Let's assume the original decoder config (256, 128, 64, 32, 32) and (2,2,2,2,2)\n",
    "    # is correct for the spatial sizes output by the ConvNeXt stages [stage3, stage2, stage1, stage0, stem].\n",
    "    'decoder_channels': (256, 128, 64, 32, 32), # 5 levels matching 5 encoder features\n",
    "    'scale_factors': (2,2,2,2,2),           # 5 factors matching 5 decoder levels\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    import monai\n",
    "    from monai.networks.blocks import UpSample, SubpixelUpsample # Ensure these are imported\n",
    "except ImportError:\n",
    "    print(\"MONAI not found. Please install it ('pip install monai').\")\n",
    "    sys.exit(1) # Exit if MONAI is required and not found\n",
    "\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def set_seed(seed=cfg.seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # Removed torch.cuda.manual_seed, torch.backends.cudnn settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0362c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        cfg,\n",
    "        file_pairs,  #list of (data_path, label_path) tuples for this specific split\n",
    "        mode = \"train\", \n",
    "    ):\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.file_pairs = file_pairs\n",
    "        \n",
    "        self.data, self.labels = self._load_data_arrays()\n",
    "\n",
    "        self.samples_per_file = 500  # assuming each file has 500 time steps\n",
    "        total_samples_available = len(self.data) * self.samples_per_file\n",
    "\n",
    "        # Subsample logic\n",
    "        subsample = getattr(self.cfg, \"subsample\", None)\n",
    "        self.total_samples = min(subsample, total_samples_available) if subsample else total_samples_available\n",
    "\n",
    "        \n",
    "        # Build list of (file_idx, time_step_idx) pairs\n",
    "        self.index_map = []\n",
    "        for file_idx in range(len(self.data)):\n",
    "            for time_step_idx in range(self.samples_per_file):\n",
    "                self.index_map.append((file_idx, time_step_idx))\n",
    "                if len(self.index_map) >= self.total_samples:\n",
    "                    break\n",
    "            if len(self.index_map) >= self.total_samples:\n",
    "                break\n",
    "\n",
    "    def _load_data_arrays(self, ):\n",
    "               \n",
    "        data_arrays = []\n",
    "        label_arrays = []\n",
    "        mmap_mode = \"r\"\n",
    "\n",
    "        for data_fpath, label_fpath in tqdm(\n",
    "                        self.file_pairs, desc=f\"Loading {self.mode} data (mmap)\",\n",
    "                        disable=self.cfg.local_rank != 0):\n",
    "            try:\n",
    "                # Load the numpy arrays using memory mapping\n",
    "                arr = np.load(data_fpath, mmap_mode=mmap_mode)\n",
    "                lbl = np.load(label_fpath, mmap_mode=mmap_mode)\n",
    "                print(f\"Loaded {data_fpath}: {arr.shape}, {lbl.shape}\")\n",
    "                data_arrays.append(arr)\n",
    "                label_arrays.append(lbl)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: File not found - {data_fpath} r {label_fpath}\", file=sys.stderr)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file pari: {data_fpath}, {label_fpath}\", file=sys.stderr)\n",
    "                print(f\"Error: {e}\", file=sys.stderr)\n",
    "                continue\n",
    "\n",
    "            if self.cfg.local_rank == 0:\n",
    "                print(f\"Finished loading {len(data_arrays)} file pairs for {self.mode} mode.\")\n",
    "\n",
    "        return data_arrays, label_arrays\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # file_idx= idx // 500\n",
    "        # time_step_idx= idx % 500\n",
    "        # self.idx = idx\n",
    "\n",
    "        file_idx, time_step_idx = self.index_map[idx]\n",
    "        \n",
    "        x_full = self.data[file_idx]\n",
    "        y_full = self.labels[file_idx]\n",
    "\n",
    "        # --- Augmentations ---\n",
    "        # Apply augmentations to the full 3D blocks *before* slicing out the time step.\n",
    "        # Make copies after slicing and augmentation to ensure memory safety.\n",
    "        x_augmented = x_full\n",
    "        y_augmented = y_full\n",
    "\n",
    "        # Augs \n",
    "        if self.mode == \"train\":\n",
    "            \n",
    "            # Temporal flip\n",
    "            if np.random.random() < 0.5:\n",
    "                x_augmented = x_full[::-1, :, ::-1] # Time flip (dim 0), Spatial flip (dim 2)\n",
    "                y_augmented = y_full[..., ::-1]  # Spatial flip (dim 2) only\n",
    "\n",
    "        # --- Slicing and Copying ---\n",
    "        # Get the specific time step from the (potentially augmented) full array\n",
    "        # This reslts in a 2D array (Dim1, Dim2)\n",
    "        x_sample = x_augmented[time_step_idx, ...]\n",
    "        y_sample = y_augmented[time_step_idx, ...]\n",
    "\n",
    "        # make copies to return independent arrays/tensors.\n",
    "        # This is important especially with mmap and multiprocessing DataLoaders.\n",
    "        x_sample = x_sample.copy()\n",
    "        y_sample = y_sample.copy()\n",
    "\n",
    "        x_tensor = torch.from_numpy(x_sample).float()\n",
    "        y_tensor = torch.from_numpy(y_sample).float()\n",
    "        \n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return self.total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dd1a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main loop]- Using device: cpu\n",
      "Loading train data using mmap_mode='r'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train data (mmap): 100%|██████████| 1/1 [00:00<00:00, 294.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading 1 out of 1 file pairs successfully for train mode.\n",
      "Dataset initialized in train mode.\n",
      "Loaded 1 file pairs containing a total of 500 raw samples.\n",
      "Input shape per single slice: (5, 1000, 70)\n",
      "Output label shape: (70, 70)\n",
      "Window size for stacking: 5 slices (padding 2 on each side).\n",
      "Generated 100 effective samples for training/validation after considering windowing and subsampling.\n",
      "[NetWithTransformer - init / num_input_slices] : 5\n",
      "[NetWithTransformer - init / self.H_in] : 1000\n",
      "[NetWithTransformer - init / self.W_in] : 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'default' is not a valid Format",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1268\u001b[39m\n\u001b[32m   1263\u001b[39m          cfg.input_width = valid_ds.W_in\n\u001b[32m   1266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RUN_TRAIN \u001b[38;5;129;01mor\u001b[39;00m RUN_VALID \u001b[38;5;129;01mor\u001b[39;00m RUN_TEST:\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# Create the new model\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1268\u001b[39m     model = \u001b[43mNetWithTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m)\u001b[49m.to(cfg.device)\n\u001b[32m   1269\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[main loop]- Model created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mp.requires_grad)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m trainable parameters.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1271\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cfg.ema \u001b[38;5;129;01mand\u001b[39;00m RUN_TRAIN: \u001b[38;5;66;03m# Only initialize EMA if training is enabled\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 723\u001b[39m, in \u001b[36mNetWithTransformer.__init__\u001b[39m\u001b[34m(self, cfg, backbone, pretrained)\u001b[39m\n\u001b[32m    717\u001b[39m real_backbone = timm.create_model(\n\u001b[32m    718\u001b[39m     backbone,\n\u001b[32m    719\u001b[39m     pretrained=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    720\u001b[39m     features_only=\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# so you can access `stem`\u001b[39;00m\n\u001b[32m    721\u001b[39m )\n\u001b[32m    722\u001b[39m feature_info = real_backbone.feature_info\n\u001b[32m--> \u001b[39m\u001b[32m723\u001b[39m \u001b[38;5;28mself\u001b[39m.backbone = \u001b[43mFeatureListNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreal_backbone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_fmt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdefault\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[NetWithTransformer - init / self.backbone] : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.backbone.default_cfg[\u001b[33m'\u001b[39m\u001b[33marchitecture\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    730\u001b[39m \u001b[38;5;66;03m# Modify the stem to handle stacked input channels and the inferred height/width\u001b[39;00m\n\u001b[32m    731\u001b[39m \u001b[38;5;66;03m# This function also determines the exact shape of features after the stem and stages.\u001b[39;00m\n\u001b[32m    732\u001b[39m \u001b[38;5;66;03m# It will update self.transformer_stage_idx_in_features, transformer_in_channels, etc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\dev\\mit\\env\\Lib\\site-packages\\timm\\models\\_features.py:336\u001b[39m, in \u001b[36mFeatureListNet.__init__\u001b[39m\u001b[34m(self, model, out_indices, output_fmt, feature_concat, flatten_sequential)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    321\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    322\u001b[39m         model: nn.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m         flatten_sequential: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    327\u001b[39m ):\n\u001b[32m    328\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[33;03m        model: Model from which to extract features.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    334\u001b[39m \u001b[33;03m        flatten_sequential: Flatten first two-levels of sequential modules in model (re-writes model modules)\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_fmt\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_fmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_concat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_concat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mflatten_sequential\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflatten_sequential\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh:\\dev\\mit\\env\\Lib\\site-packages\\timm\\models\\_features.py:265\u001b[39m, in \u001b[36mFeatureDictNet.__init__\u001b[39m\u001b[34m(self, model, out_indices, out_map, output_fmt, feature_concat, flatten_sequential)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28msuper\u001b[39m(FeatureDictNet, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m    264\u001b[39m \u001b[38;5;28mself\u001b[39m.feature_info = _get_feature_info(model, out_indices)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m \u001b[38;5;28mself\u001b[39m.output_fmt = \u001b[43mFormat\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_fmt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;28mself\u001b[39m.concat = feature_concat\n\u001b[32m    267\u001b[39m \u001b[38;5;28mself\u001b[39m.grad_checkpointing = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\enum.py:757\u001b[39m, in \u001b[36mEnumType.__call__\u001b[39m\u001b[34m(cls, value, names, module, qualname, type, start, boundary, *values)\u001b[39m\n\u001b[32m    755\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _not_given:\n\u001b[32m    756\u001b[39m         value = (value, names) + values\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[38;5;66;03m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m _not_given \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    760\u001b[39m     \u001b[38;5;66;03m# no body? no data-type? possibly wrong usage\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\enum.py:1171\u001b[39m, in \u001b[36mEnum.__new__\u001b[39m\u001b[34m(cls, value)\u001b[39m\n\u001b[32m   1169\u001b[39m ve_exc = \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m is not a valid \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (value, \u001b[38;5;28mcls\u001b[39m.\u001b[34m__qualname__\u001b[39m))\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ve_exc\n\u001b[32m   1172\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1173\u001b[39m     exc = \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1174\u001b[39m             \u001b[33m'\u001b[39m\u001b[33merror in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m._missing_: returned \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m instead of None or a valid member\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1175\u001b[39m             % (\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m, result)\n\u001b[32m   1176\u001b[39m             )\n",
      "\u001b[31mValueError\u001b[39m: 'default' is not a valid Format"
     ]
    }
   ],
   "source": [
    "# --- Transformer Modules ---\n",
    "\n",
    "class ModelEMA(nn.Module):\n",
    "    def __init__(self, model, decay=0.99, device=None):\n",
    "        super().__init__()\n",
    "        self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.device = device\n",
    "        if self.device is not None:\n",
    "            self.module.to(device=device)\n",
    "\n",
    "    def _update(self, model, update_fn):\n",
    "        with torch.no_grad():\n",
    "            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
    "                if self.device is not None:\n",
    "                    model_v = model_v.to(device=self.device)\n",
    "                ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)\n",
    "\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    \"\"\"\n",
    "    2D positional encoding for Transformer.\n",
    "    Adds sin/cos embeddings based on height and width coordinates.\n",
    "    Assumes input sequence is flattened from (H, W) to (H*W).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, height, width):\n",
    "        super().__init__()\n",
    "        if d_model % 4 != 0:\n",
    "            raise ValueError(\"d_model must be divisible by 4 for 2D positional encoding\")\n",
    "\n",
    "        d_model_half = d_model // 2\n",
    "        d_model_quarter = d_model // 4\n",
    "\n",
    "        # Compute the positional encodings\n",
    "        pe = torch.zeros(d_model_half, height, width) # Combine H and W encodings\n",
    "        position_h = torch.arange(0., height).unsqueeze(1) # (H, 1)\n",
    "        position_w = torch.arange(0., width).unsqueeze(1)  # (W, 1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0., d_model_quarter, 2) * -(math.log(10000.0) / d_model_quarter)) # (d_model/8)\n",
    "\n",
    "        # PE for height dimension\n",
    "        pe[0:d_model_quarter:2, :, :] = torch.sin(position_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        pe[1:d_model_quarter:2, :, :] = torch.cos(position_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "\n",
    "        # PE for width dimension\n",
    "        pe[d_model_quarter:d_model_half:2, :, :] = torch.sin(position_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[d_model_quarter+1:d_model_half:2, :, :] = torch.cos(position_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "\n",
    "        # Extend to full d_model if needed (e.g., repeat) or truncate if odd\n",
    "        pe = pe[:d_model_half, :, :] # Ensure we have d_model_half channels\n",
    "\n",
    "        # Final PE shape (d_model_half, H, W). Need to project/match d_model later.\n",
    "        # Store as (1, d_model_half, H, W)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "        # Need a linear projection if d_model_half != feature_channels\n",
    "        # Or add PE directly to features if d_model_half == feature_channels\n",
    "        # Let's assume we add PE to the flattened sequence directly, so PE should have dim d_model.\n",
    "        # Re-calculating PE to have d_model channels:\n",
    "        pe_full = torch.zeros(d_model, height, width)\n",
    "        pe_full[:d_model_half, :, :] = pe # Use the computed pe_half\n",
    "        # Could add another set of sin/cos or zeros for the second half if needed,\n",
    "        # depending on how d_model is structured in the Transformer layer.\n",
    "        # Simplest: assume d_model is for concatenation or addition. Let's make PE have d_model channels.\n",
    "\n",
    "        pe = torch.zeros(d_model, height, width)\n",
    "        position_h = torch.arange(0., height).unsqueeze(1) # (H, 1)\n",
    "        position_w = torch.arange(0., width).unsqueeze(1)  # (W, 1)\n",
    "\n",
    "        # PE for height dimension (using first d_model/2 channels)\n",
    "        div_term_h = torch.exp(torch.arange(0., d_model//2, 2) * -(math.log(10000.0) / (d_model//2)))\n",
    "        pe[0:d_model//2:2, :, :] = torch.sin(position_h * div_term_h).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        pe[1:d_model//2:2, :, :] = torch.cos(position_h * div_term_h).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "\n",
    "        # PE for width dimension (using second d_model/2 channels)\n",
    "        div_term_w = torch.exp(torch.arange(0., d_model//2, 2) * -(math.log(10000.0) / (d_model//2)))\n",
    "        pe[d_model//2::2, :, :] = torch.sin(position_w * div_term_w).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[d_model//2+1::2, :, :] = torch.cos(position_w * div_term_w).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0)) # (1, d_model, height, width)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, C, H, W) where C should match d_model of transformer\n",
    "        # PE shape: (1, d_model, H, W)\n",
    "        # Add PE to the features. Requires C == d_model.\n",
    "        # If C != d_model, a projection is needed before adding PE.\n",
    "        # Assuming input features x are projected to d_model first.\n",
    "        # The TransformerBlock2d handles the projection. PE is added to the sequence *after* projection.\n",
    "\n",
    "        # PE needs to be reshaped to (1, H*W, d_model) to match the sequence shape\n",
    "        H, W = x.shape[-2], x.shape[-1]\n",
    "        pe_seq = self.pe[:, :, :H, :W].view(1, self.pe.shape[1], -1).permute(0, 2, 1) # (1, H*W, d_model)\n",
    "        return pe_seq\n",
    "\n",
    "\n",
    "class TransformerBlock2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies a Transformer Encoder block to a 2D feature map.\n",
    "    Input (B, C, H, W) -> Project channels -> Flatten (B, H*W, hidden_dim)\n",
    "    -> Add Positional Encoding -> Transformer Encoder -> Reshape (B, hidden_dim, H, W)\n",
    "    -> Project channels back -> Add skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_dim, num_layers=1, num_heads=8, height=None, width=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        if height is None or width is None:\n",
    "             # If height/width are not provided, cannot initialize PE.\n",
    "             # This means the block must be initialized after knowing the exact feature map size.\n",
    "             # Or use a dynamic PE (less common) or no PE.\n",
    "             # For this implementation, height and width must be provided.\n",
    "             raise ValueError(\"Height and Width must be provided for Positional Encoding and Transformer block\")\n",
    "\n",
    "        # Project input channels to hidden_dim for Transformer\n",
    "        # Use a simple Conv2d 1x1\n",
    "        self.proj_in = nn.Conv2d(in_channels, hidden_dim, kernel_size=1)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_embed = PositionalEncoding2D(hidden_dim, height, width) # PE has shape (1, H*W, hidden_dim)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        # batch_first=True means input/output shape is (batch_size, sequence_length, features)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4, # Typical feedforward dimension\n",
    "            batch_first=True,\n",
    "            norm_first=True # Pre-LayerNorm\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Project hidden_dim back to in_channels (for residual connection)\n",
    "        self.proj_out = nn.Conv2d(hidden_dim, in_channels, kernel_size=1)\n",
    "\n",
    "        # Layer norm after projection out and residual connection (optional, depends on design)\n",
    "        # self.norm = nn.LayerNorm(in_channels) # Applied to (B, C, H, W) -> needs transpose\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W) - Input feature map\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Check if spatial dimensions match the PE size (should be handled during init)\n",
    "        if H != self.height or W != self.width:\n",
    "             raise ValueError(f\"Input spatial size mismatch. Expected ({self.height}, {self.width}), got ({H}, {W})\")\n",
    "\n",
    "        # Project channels\n",
    "        x_proj = self.proj_in(x) # (B, hidden_dim, H, W)\n",
    "\n",
    "        # Flatten spatial dimensions and permute to (B, H*W, hidden_dim)\n",
    "        x_seq = x_proj.view(B, self.hidden_dim, -1).permute(0, 2, 1) # (B, H*W, hidden_dim)\n",
    "\n",
    "        # Add positional encoding (PE shape: (1, H*W, hidden_dim))\n",
    "        # PE is added broadcastingly over the batch dimension\n",
    "        x_seq = x_seq + self.pos_embed(x_proj) # Add PE to the sequence\n",
    "\n",
    "        # Transformer Encoder\n",
    "        transformer_output_seq = self.transformer_encoder(x_seq) # (B, H*W, hidden_dim)\n",
    "\n",
    "        # Reshape back to (B, hidden_dim, H, W)\n",
    "        transformer_output_reshaped = transformer_output_seq.permute(0, 2, 1).view(B, self.hidden_dim, H, W)\n",
    "\n",
    "        # Project channels back to original in_channels\n",
    "        out = self.proj_out(transformer_output_reshaped) # (B, in_channels, H, W)\n",
    "\n",
    "        # Add residual connection\n",
    "        out = out + x\n",
    "\n",
    "        # Optional final norm\n",
    "        # out = self.norm(out.permute(0, 2, 3, 1)).permute(0, 3, 1, 2) # Apply LayerNorm over channel dim\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# --- Custom Dataset ---\n",
    "\n",
    "class CustomDatasetWithSlices(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        file_pairs,  # list of (data_path, label_path) tuples for this specific split\n",
    "        mode = \"train\",\n",
    "        num_input_slices: int = 3, # Number of consecutive samples to stack\n",
    "        # inferred_input_height: int = 352, # Assumed H_in from original data processing\n",
    "        # input_width: int = 70, # W_in from original data processing\n",
    "        # output_height: int = 70, # H_out for labels\n",
    "        # output_width: int = 70,  # W_out for labels\n",
    "    ):\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.file_pairs = file_pairs\n",
    "        self.num_input_slices = num_input_slices\n",
    "        # self.inferred_input_height = inferred_input_height # Use from cfg\n",
    "        # self.input_width = input_width # Use from cfg\n",
    "        # self.output_height = output_height # Use from cfg\n",
    "        # self.output_width = output_width # Use from cfg\n",
    "\n",
    "        # Load data.\n",
    "        # ASSUMPTION: Each file pair (data.npy, model.npy) loads arrays\n",
    "        #   - data.npy contains (N_samples_per_file, 5, H_in, 70)\n",
    "        #   - model.npy contains (N_samples_per_file, 70, 70)\n",
    "        # Where N_samples_per_file is the number of pre-processed samples in that file,\n",
    "        # 5 is the fixed channel count, H_in is the fixed processed height (inferred from original stem logic),\n",
    "        # and 70 is the width.\n",
    "        self.data_arrays, self.label_arrays = self._load_data_arrays()\n",
    "\n",
    "        # Validate and store shapes based on loaded data\n",
    "        if not self.data_arrays:\n",
    "             raise RuntimeError(f\"No data files loaded for mode '{self.mode}'. Check file paths and format.\")\n",
    "\n",
    "        # Use the shape of the first loaded array to define dataset dimensions\n",
    "        first_data_shape = self.data_arrays[0].shape\n",
    "        first_label_shape = self.label_arrays[0].shape\n",
    "\n",
    "        self.samples_per_file_ = first_data_shape[0] # Number of samples per file\n",
    "        self.channels_in_single = first_data_shape[1] # Should be 5\n",
    "        self.H_in = self.data_arrays[0].shape[2]        # Assumed fixed input height (matches cfg.inferred_input_height)\n",
    "        self.W_in = self.data_arrays[0].shape[3]        # Should be 70 (matches cfg.input_width)\n",
    "        self.H_out = self.label_arrays[0].shape[2]      # Should be 70 (matches cfg.output_height)\n",
    "        self.W_out = self.label_arrays[0].shape[3]      # Should be 70 (matches cfg.output_width)\n",
    "\n",
    "        # Validate against cfg expectations (optional but good)\n",
    "        if self.channels_in_single != 5:\n",
    "             print(f\"Warning: Loaded data has {self.channels_in_single} channels, expected 5.\", file=sys.stderr)\n",
    "        # Note: Cannot strictly validate H_in against cfg.inferred_input_height here\n",
    "        # as the cfg value was just an inference based on the original stem.\n",
    "        # The loaded data's shape dictates the true H_in for the model.\n",
    "        # self.cfg.inferred_input_height = self.H_in # Update cfg with actual loaded height\n",
    "        if self.W_in != 70:\n",
    "            print(f\"Warning: Loaded data has width {self.W_in}, expected 70.\", file=sys.stderr)\n",
    "            self.cfg.input_width = self.W_in # Update cfg\n",
    "        if self.H_out != 70 or self.W_out != 70:\n",
    "            print(f\"Warning: Loaded labels have shape ({self.H_out}, {self.W_out}), expected (70, 70).\", file=sys.stderr)\n",
    "            # Update cfg with actual dimensions if needed downstream\n",
    "            self.cfg.output_height = self.H_out\n",
    "            self.cfg.output_width = self.W_out\n",
    "        \n",
    "        # Update cfg with actual loaded dimensions for model compatibility\n",
    "        self.cfg.inferred_input_height = self.H_in\n",
    "        self.cfg.input_width = self.W_in # Matches W_in\n",
    "\n",
    "        total_files = len(self.data_arrays)\n",
    "        # Total number of *original* samples across all successfully loaded files\n",
    "        total_samples_available = total_files * self.samples_per_file_\n",
    "\n",
    "        subsample = getattr(self.cfg, \"subsample\", None)\n",
    "        # Determine the total number of *effective* samples based on subsampling,\n",
    "        # but the actual count in index_map might be slightly less due to padding\n",
    "        # requirements at file boundaries.\n",
    "        effective_subsample_limit = subsample if subsample and subsample > 0 else float('inf')\n",
    "\n",
    "        # Build list of (file_idx, sample_center_idx) pairs\n",
    "        # We select num_input_slices consecutive samples centered at sample_center_idx.\n",
    "        pad = (self.num_input_slices - 1) // 2\n",
    "\n",
    "        self.index_map = []\n",
    "        current_effective_samples = 0\n",
    "        \n",
    "        for file_idx in range(total_files):\n",
    "            N_samples_in_file = self.data_arrays[file_idx].shape[0] # Number of samples in this file's array\n",
    "\n",
    "            # Iterate through possible center sample indices such that the window\n",
    "            # [center - pad, center + pad] is entirely within [0, N_samples_in_file - 1].\n",
    "            valid_start_idx = pad\n",
    "            valid_end_idx = N_samples_in_file - pad - 1 # Inclusive end index for center\n",
    "\n",
    "            # Check if there are enough samples in the file to form *at least one* window\n",
    "            if valid_end_idx < valid_start_idx:\n",
    "                print(f\"Warning: File {file_idx} (with {N_samples_in_file} samples) is too short for window size {self.num_input_slices}. Skipping file for effective samples.\", file=sys.stderr)\n",
    "                continue # Skip this file if not enough samples for any window\n",
    "\n",
    "            for sample_center_idx in range(valid_start_idx, valid_end_idx + 1):\n",
    "                 self.index_map.append((file_idx, sample_center_idx))\n",
    "                 current_effective_samples += 1\n",
    "                 # Stop if subsample limit is reached for effective samples\n",
    "                 if current_effective_samples >= effective_subsample_limit:\n",
    "                     break\n",
    "            # Stop if subsample limit is reached for effective samples\n",
    "            if current_effective_samples >= effective_subsample_limit:\n",
    "                 break\n",
    "\n",
    "        self.total_effective_samples = len(self.index_map)\n",
    "\n",
    "        print(f\"Dataset initialized in {self.mode} mode.\")\n",
    "        print(f\"Loaded {total_files} file pairs containing a total of {total_samples_available} raw samples.\")\n",
    "        print(f\"Input shape per single slice: ({self.channels_in_single}, {self.H_in}, {self.W_in})\")\n",
    "        print(f\"Output label shape: ({self.H_out}, {self.W_out})\")\n",
    "        print(f\"Window size for stacking: {self.num_input_slices} slices (padding {pad} on each side).\")\n",
    "        print(f\"Generated {self.total_effective_samples} effective samples for training/validation after considering windowing and subsampling.\")\n",
    "\n",
    "\n",
    "    def _load_data_arrays(self):\n",
    "        \"\"\"\n",
    "        Loads data and label arrays from file pairs using mmap_mode for efficiency.\n",
    "        Includes validation for expected shapes.\n",
    "        \"\"\"\n",
    "        data_arrays_list = []\n",
    "        label_arrays_list = []\n",
    "        # Use 'r' mode always for memory efficiency with large datasets\n",
    "        mmap_mode = \"r\"\n",
    "\n",
    "        print(f\"Loading {self.mode} data using mmap_mode='{mmap_mode}'...\")\n",
    "\n",
    "        # Use local_rank to ensure tqdm is only shown on the main process in DDP\n",
    "        disable_tqdm = getattr(self.cfg, 'local_rank', 0) != 0\n",
    "\n",
    "        successful_loads = 0\n",
    "        for data_fpath, label_fpath in tqdm(\n",
    "                        self.file_pairs, desc=f\"Loading {self.mode} data (mmap)\",\n",
    "                        disable=disable_tqdm):\n",
    "            try:\n",
    "                # Check if files exist before attempting to load\n",
    "                if not os.path.exists(data_fpath):\n",
    "                    print(f\"Warning: Data file not found: {data_fpath}. Skipping pair.\", file=sys.stderr)\n",
    "                    continue\n",
    "                if not os.path.exists(label_fpath):\n",
    "                    print(f\"Warning: Label file not found: {label_fpath}. Skipping pair.\", file=sys.stderr)\n",
    "                    continue\n",
    "\n",
    "                # Load data with expected shape (N_samples, Channels, H_in, W_in)\n",
    "                # For your data: (N, 5, 1000, 70)\n",
    "                arr = np.load(data_fpath, mmap_mode=mmap_mode)\n",
    "                # Load labels with expected shape (N_samples, H_out, W_out)\n",
    "                # For your data: (N, 70, 70)\n",
    "                lbl = np.load(label_fpath, mmap_mode=mmap_mode)\n",
    "\n",
    "                # --- Basic shape validation based on YOUR specified structure ---\n",
    "                expected_data_ndim = 4\n",
    "                expected_label_ndim = 4\n",
    "                expected_channels = 5 # Your specific channel count\n",
    "                expected_data_width = 70 # Your specific GeoPhones dimension\n",
    "                expected_label_height = 70 # Your specific output height\n",
    "                expected_label_width = 70  # Your specific output width\n",
    "\n",
    "                if arr.ndim != expected_data_ndim or \\\n",
    "                   arr.shape[1] != expected_channels or \\\n",
    "                   arr.shape[3] != expected_data_width:\n",
    "                     print(f\"Warning: Data file {data_fpath} has unexpected shape {arr.shape}. \"\n",
    "                           f\"Expected ndim={expected_data_ndim}, shape[1]={expected_channels} (channels), \"\n",
    "                           f\"shape[3]={expected_data_width} (width/geophones). Skipping.\", file=sys.stderr)\n",
    "                     continue\n",
    "\n",
    "                if lbl.ndim != expected_label_ndim or \\\n",
    "                   lbl.shape[2] != expected_label_height or \\\n",
    "                   lbl.shape[3] != expected_label_width:\n",
    "                     print(f\"Warning: Label file {label_fpath} has unexpected shape {lbl.shape}. \"\n",
    "                           f\"Expected ndim={expected_label_ndim}, shape[1]={expected_label_height} (height), \"\n",
    "                           f\"shape[2]={expected_label_width} (width). Skipping.\", file=sys.stderr)\n",
    "                     continue\n",
    "\n",
    "                # Validate that the number of samples (batch dimension) matches\n",
    "                if arr.shape[0] != lbl.shape[0]:\n",
    "                     print(f\"Warning: Mismatch in number of samples (batch size) between data ({arr.shape[0]}) and label ({lbl.shape[0]}) \"\n",
    "                           f\"in file pair {data_fpath}, {label_fpath}. Skipping.\", file=sys.stderr)\n",
    "                     continue\n",
    "\n",
    "                # If it passes validation, add to lists\n",
    "                data_arrays_list.append(arr)\n",
    "                label_arrays_list.append(lbl)\n",
    "                successful_loads += 1\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                # This check is now redundant with the os.path.exists check above,\n",
    "                # but keeping it doesn't hurt as a fallback.\n",
    "                print(f\"Error: File not found - {data_fpath} or {label_fpath}. Skipping pair.\", file=sys.stderr)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading or validating file pair: {data_fpath}, {label_fpath}\", file=sys.stderr)\n",
    "                print(f\"Error details: {e}\", file=sys.stderr)\n",
    "                # traceback.print_exc() # Uncomment for detailed error\n",
    "                continue\n",
    "\n",
    "        # if self.cfg.local_rank == 0: # Only print summary from main process\n",
    "        print(f\"Finished loading {successful_loads} out of {len(self.file_pairs)} file pairs successfully for {self.mode} mode.\")\n",
    "\n",
    "        return data_arrays_list, label_arrays_list\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of effective samples available in the dataset.\n",
    "        \"\"\"\n",
    "        return self.total_effective_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves a single effective sample (input and label) based on the index.\n",
    "        An effective sample consists of num_input_slices data slices stacked,\n",
    "        and the corresponding label for the center slice.\n",
    "        \"\"\"\n",
    "        if index < 0 or index >= self.total_effective_samples:\n",
    "            raise IndexError(f\"Index {index} out of bounds for dataset of size {self.total_effective_samples}\")\n",
    "\n",
    "        # Get the file index and the center sample index within that file\n",
    "        file_idx, sample_center_idx = self.index_map[index]\n",
    "\n",
    "        # Calculate the start and end indices for the window of slices\n",
    "        pad = (self.num_input_slices - 1) // 2\n",
    "        start_idx = sample_center_idx - pad\n",
    "        end_idx = sample_center_idx + pad # This is inclusive\n",
    "\n",
    "        # Retrieve the batch of consecutive data slices\n",
    "        # The shape will be (num_input_slices, Channels, H_in, W_in)\n",
    "        data_slices = self.data_arrays[file_idx][start_idx : end_idx + 1, ...]\n",
    "\n",
    "        # Retrieve the label for the *center* slice\n",
    "        # The shape will be (H_out, W_out)\n",
    "        label_slice = self.label_arrays[file_idx][sample_center_idx, ...]\n",
    "\n",
    "        # --- Stack the data slices ---\n",
    "        # The original shape is (num_input_slices, Channels, H_in, W_in)\n",
    "        # We want to combine the 'num_input_slices' and 'Channels' dimensions\n",
    "        # into a single channel dimension, resulting in (num_input_slices * Channels, H_in, W_in).\n",
    "        # This is a common way to represent stacked time series data as input channels for CNNs.\n",
    "        combined_channels = self.num_input_slices * self.channels_in_single\n",
    "        input_tensor = data_slices.reshape(combined_channels, self.H_in, self.W_in)\n",
    "\n",
    "        # Convert numpy arrays to PyTorch tensors\n",
    "        # Ensure correct data types (float32 is common for model inputs/outputs)\n",
    "        input_tensor = torch.from_numpy(input_tensor).float()\n",
    "        label_tensor = torch.from_numpy(label_slice).float() # Assuming regression output\n",
    "\n",
    "        return input_tensor, label_tensor\n",
    "\n",
    "# --- Decoder (Keep original, ensure it works with modified encoder outputs) ---\n",
    "# The UnetDecoder2d class remains the same. It takes a list of encoder features\n",
    "# and skip connections. We will feed it the features from our modified encoder.\n",
    "# (Paste ConvBnAct2d class here)\n",
    "\n",
    "class ConvBnAct2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding: int = 0,\n",
    "        stride: int = 1,\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        act_layer: nn.Module = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv= nn.Conv2d(\n",
    "            in_channels, \n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride, \n",
    "            padding=padding, \n",
    "            bias=False,\n",
    "        )\n",
    "        self.norm = norm_layer(out_channels) if norm_layer != nn.Identity else nn.Identity()\n",
    "        self.act= act_layer(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# (Paste SCSEModule2d class here)\n",
    "\n",
    "\n",
    "class SCSEModule2d(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sSE = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 1, 1), \n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.cSE(x) + x * self.sSE(x)\n",
    "    \n",
    "# (Paste Attention2d class here)\n",
    "\n",
    "\n",
    "class Attention2d(nn.Module):\n",
    "    def __init__(self, name, **params):\n",
    "        super().__init__()\n",
    "        if name is None:\n",
    "            self.attention = nn.Identity(**params)\n",
    "        elif name == \"scse\":\n",
    "            self.attention = SCSEModule2d(**params)\n",
    "        else:\n",
    "            raise ValueError(\"Attention {} is not implemented\".format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x)\n",
    "    \n",
    "\n",
    "# (Paste DecoderBlock2d class here)\n",
    "\n",
    "\n",
    "class DecoderBlock2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels,\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = None,\n",
    "        intermediate_conv: bool = False,\n",
    "        upsample_mode: str = \"deconv\",\n",
    "        scale_factor: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Upsample block\n",
    "        if upsample_mode == \"pixelshuffle\":\n",
    "            self.upsample= SubpixelUpsample(\n",
    "                spatial_dims= 2,\n",
    "                in_channels= in_channels,\n",
    "                scale_factor= scale_factor,\n",
    "            )\n",
    "        else:\n",
    "            self.upsample = UpSample(\n",
    "                spatial_dims= 2,\n",
    "                in_channels= in_channels,\n",
    "                out_channels= in_channels,\n",
    "                scale_factor= scale_factor,\n",
    "                mode= upsample_mode,\n",
    "            )\n",
    "\n",
    "        if intermediate_conv:\n",
    "            k= 3\n",
    "            c= skip_channels if skip_channels != 0 else in_channels\n",
    "            self.intermediate_conv = nn.Sequential(\n",
    "                ConvBnAct2d(c, c, k, k//2),\n",
    "                ConvBnAct2d(c, c, k, k//2),\n",
    "                )\n",
    "        else:\n",
    "            self.intermediate_conv= None\n",
    "\n",
    "        self.attention1 = Attention2d(\n",
    "            name= attention_type, \n",
    "            in_channels= in_channels + skip_channels,\n",
    "            )\n",
    "\n",
    "        self.conv1 = ConvBnAct2d(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size= 3,\n",
    "            padding= 1,\n",
    "            norm_layer= norm_layer,\n",
    "        )\n",
    "\n",
    "        self.conv2 = ConvBnAct2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size= 3,\n",
    "            padding= 1,\n",
    "            norm_layer= norm_layer,\n",
    "        )\n",
    "        self.attention2 = Attention2d(\n",
    "            name= attention_type, \n",
    "            in_channels= out_channels,\n",
    "            )\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        if self.intermediate_conv is not None:\n",
    "            if skip is not None:\n",
    "                skip = self.intermediate_conv(skip)\n",
    "            else:\n",
    "                x = self.intermediate_conv(x)\n",
    "\n",
    "        if skip is not None:\n",
    "            # print(x.shape, skip.shape)\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention1(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# (Paste UnetDecoder2d class here)\n",
    "\n",
    "class UnetDecoder2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet decoder.\n",
    "    Source: https://arxiv.org/abs/1505.04597\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: tuple[int],\n",
    "        skip_channels: tuple[int] = None,\n",
    "        decoder_channels: tuple = (256, 128, 64, 32),\n",
    "        scale_factors: tuple = (2,2,2,2),\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = None,\n",
    "        intermediate_conv: bool = False,\n",
    "        upsample_mode: str = \"deconv\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if len(encoder_channels) == 4:\n",
    "            decoder_channels= decoder_channels[1:]\n",
    "        self.decoder_channels= decoder_channels\n",
    "        \n",
    "        if skip_channels is None:\n",
    "            skip_channels= list(encoder_channels[1:]) + [0]\n",
    "\n",
    "        # Build decoder blocks\n",
    "        in_channels= [encoder_channels[0]] + list(decoder_channels[:-1])\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        for i, (ic, sc, dc) in enumerate(zip(in_channels, skip_channels, decoder_channels)):\n",
    "            # print(i, ic, sc, dc)\n",
    "            self.blocks.append(\n",
    "                DecoderBlock2d(\n",
    "                    ic, sc, dc, \n",
    "                    norm_layer= norm_layer,\n",
    "                    attention_type= attention_type,\n",
    "                    intermediate_conv= intermediate_conv,\n",
    "                    upsample_mode= upsample_mode,\n",
    "                    scale_factor= scale_factors[i],\n",
    "                    )\n",
    "            )\n",
    "\n",
    "    def forward(self, feats: list[torch.Tensor]):\n",
    "        res= [feats[0]]\n",
    "        feats= feats[1:]\n",
    "\n",
    "        # Decoder blocks\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            skip= feats[i] if i < len(feats) else None\n",
    "            res.append(\n",
    "                b(res[-1], skip=skip),\n",
    "                )\n",
    "            \n",
    "        return res\n",
    "    \n",
    "# (Paste SegmentationHead2d class here)\n",
    "\n",
    "class SegmentationHead2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        scale_factor: tuple[int] = (2,2),\n",
    "        kernel_size: int = 3,\n",
    "        mode: str = \"nontrainable\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv= nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size= kernel_size,\n",
    "            padding= kernel_size//2\n",
    "        )\n",
    "        self.upsample = UpSample(\n",
    "            spatial_dims= 2,\n",
    "            in_channels= out_channels,\n",
    "            out_channels= out_channels,\n",
    "            scale_factor= scale_factor,\n",
    "            mode= mode,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.upsample(x)\n",
    "        return x        \n",
    "        \n",
    "\n",
    "class NetWithTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A ConvNeXt U-Net style model with a Transformer block inserted after an early encoder stage.\n",
    "    Takes stacked input slices as channels to incorporate temporal context.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        backbone: str,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        num_input_slices = cfg.num_input_slices\n",
    "        print(f\"[NetWithTransformer - init / num_input_slices] : {num_input_slices}\")\n",
    "        transformer_config = cfg.transformer_config\n",
    "\n",
    "        # Assumed input dimensions for a single sample (after dataset preprocessing)\n",
    "        # These will be updated by the dataset upon loading if necessary.\n",
    "        self.H_in = cfg.inferred_input_height # Assumed input height for a single slice\n",
    "        print(f\"[NetWithTransformer - init / self.H_in] : {self.H_in}\")\n",
    "        self.W_in = cfg.input_width          # Assumed input width\n",
    "        print(f\"[NetWithTransformer - init / self.W_in] : {self.W_in}\")\n",
    "\n",
    "        # Encoder backbone (ConvNeXt stages, WITHOUT the initial stem as provided by timm)\n",
    "        # We will handle the initial layers (stem) separately.\n",
    "        # Create the full ConvNeXt model, then we will modify its stem and use its stages.\n",
    "        # Set in_chans to a temporary value (e.g., 5 or 3) here, as it will be updated in _update_stem.\n",
    "        # self.backbone = timm.create_model(\n",
    "        #     backbone,\n",
    "        #     in_chans=5, # This will be replaced in _update_stem\n",
    "        #     pretrained=pretrained,\n",
    "        #     features_only=True, # Returns list of features after stem and each stage\n",
    "        #     drop_path_rate=0.0, # Keep original drop_path_rate or set via cfg if needed\n",
    "        # )\n",
    "        real_backbone = timm.create_model(\n",
    "            backbone,\n",
    "            pretrained=False,\n",
    "            features_only=True  # so you can access `stem`\n",
    "        )\n",
    "        feature_info = real_backbone.feature_info\n",
    "        self.backbone = FeatureListNet(\n",
    "            real_backbone,\n",
    "            out_indices=[0,1,2,3],\n",
    "            output_fmt='default',\n",
    "        )\n",
    "        print(f\"[NetWithTransformer - init / self.backbone] : {self.backbone.default_cfg['architecture']}\")\n",
    "\n",
    "        # Modify the stem to handle stacked input channels and the inferred height/width\n",
    "        # This function also determines the exact shape of features after the stem and stages.\n",
    "        # It will update self.transformer_stage_idx_in_features, transformer_in_channels, etc.\n",
    "        self._update_stem(\n",
    "            self.backbone,\n",
    "            in_chans=5 * num_input_slices,\n",
    "            input_height=self.H_in,\n",
    "            input_width=self.W_in,\n",
    "        )\n",
    "\n",
    "        # Transformer Block - Initialized in _update_stem after spatial dims are known\n",
    "        # self.transformer_block = ...\n",
    "\n",
    "        # Decoder - Initialized in _update_stem after actual encoder channel counts are known\n",
    "        # self.decoder = ...\n",
    "\n",
    "        # Seg Head - Initialized in _update_stem\n",
    "        # self.seg_head = ...\n",
    "\n",
    "        # Apply replacements to backbone and decoder modules\n",
    "        self.replace_activations(self.backbone) # Apply to ConvNeXt backbone\n",
    "        self.replace_norms(self.backbone)     # Apply to ConvNeXt backbone\n",
    "        # Apply custom forward for ConvNeXt blocks if needed (copied from original code)\n",
    "        self.replace_forwards(self.backbone)\n",
    "\n",
    "        # Apply replacements to the decoder and seg head as well\n",
    "        self.replace_activations(self.decoder)\n",
    "        self.replace_norms(self.decoder)\n",
    "        self.replace_activations(self.seg_head)\n",
    "        self.replace_norms(self.seg_head)\n",
    "\n",
    "\n",
    "    def _update_stem(self, backbone, in_chans, input_height, input_width):\n",
    "        \"\"\"\n",
    "        Adapts the ConvNeXt stem to handle 'in_chans' and determines\n",
    "        the actual spatial dimensions of feature maps after each stage.\n",
    "        Initializes the Transformer block and Decoder based on these dimensions.\n",
    "        \"\"\"\n",
    "        print(f\"[NetWithTransformer - _update_stem] : inside update_stems\")\n",
    "        # if not backbone.name.startswith(\"convnext\"):\n",
    "        if not backbone.default_cfg['architecture'].startswith(\"convnext\"):\n",
    "            raise ValueError(\"Custom stem modification implemented only for convnext backbone.\")\n",
    "\n",
    "        # Find the first Conv2d layer in the stem (assuming it's in a Sequential module)\n",
    "        first_conv_layer = None\n",
    "        # Iterate through children, looking for the first Conv2d within the stem\n",
    "        # for name, module in backbone.stem.named_children():\n",
    "        real_backbone = timm.models.get_model(backbone.default_cfg['architecture'], pretrained=False)\n",
    "        for name, module in real_backbone.stem.named_children():\n",
    "            print(f\"[NetWithTransformer - name] : {name}\")\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                first_conv_layer = module\n",
    "                break\n",
    "            # Also check within children if the stem has nested Sequential/modules\n",
    "            for sub_name, sub_module in module.named_modules():\n",
    "                 if isinstance(sub_module, nn.Conv2d):\n",
    "                      first_conv_layer = sub_module\n",
    "                      break\n",
    "            if first_conv_layer: break\n",
    "\n",
    "        print(f\"[NetWithTransformer - first_conv_layer in None section] : first_conv_layer: {first_conv_layer}\")\n",
    "        if first_conv_layer is None:\n",
    "             raise RuntimeError(\"Could not find the first Conv2d layer in the ConvNeXt stem.\")\n",
    "\n",
    "        # Create a new first convolution layer with updated in_channels\n",
    "        new_first_conv = nn.Conv2d(\n",
    "            in_chans,\n",
    "            first_conv_layer.out_channels,\n",
    "            kernel_size=first_conv_layer.kernel_size,\n",
    "            stride=first_conv_layer.stride,\n",
    "            padding=first_conv_layer.padding,\n",
    "            bias=hasattr(first_conv_layer, 'bias') and first_conv_layer.bias is not None\n",
    "        )\n",
    "        # Copy weights for the first original 5 channels (if pretrained)\n",
    "        if hasattr(first_conv_layer, 'weight') and first_conv_layer.weight is not None:\n",
    "             # Simple initialization: copy weights for the original 5 channels, zero out/repeat others\n",
    "             # Or just use default initialization if complex handling is not needed\n",
    "             # For simplicity, use default Kaiming init of the new layer.\n",
    "             pass # new_first_conv.weight.data[:, :original_in_chans, ...] = first_conv_layer.weight.data\n",
    "\n",
    "\n",
    "        # Replace the original first conv layer within the stem module(s).\n",
    "        # This is fragile depending on how timm builds the stem Sequential.\n",
    "        # A safer way might be to create a new stem module from scratch and replace backbone.stem.\n",
    "        # Let's try creating a new Sequential module.\n",
    "        \n",
    "        # Assuming original stem was something like:\n",
    "        # Sequential( ReflectionPad2d, Conv2d (stride=(4,1)), Conv2d (stride=(4,1)) )\n",
    "        \n",
    "        # Reconstruct the stem with the new first convolution.\n",
    "        # This requires knowing the order and types of layers in the original stem.\n",
    "        # Based on the original code, it seems like Pad, Conv, Conv.\n",
    "        \n",
    "        new_stem_modules = []\n",
    "        conv_count = 0\n",
    "        for name, module in backbone.stem.named_children():\n",
    "             if isinstance(module, nn.Conv2d) and conv_count == 0:\n",
    "                  new_stem_modules.append(new_first_conv)\n",
    "                  conv_count += 1\n",
    "                  print(f\"Replaced first Conv2d in stem with new layer handling {in_chans} channels.\")\n",
    "             else:\n",
    "                  new_stem_modules.append(module)\n",
    "\n",
    "        if conv_count == 0:\n",
    "             # Handle cases where the first layer is not Conv2d, but a child contains it.\n",
    "             # This gets complicated. Let's assume the first Conv2d is a direct child.\n",
    "             raise RuntimeError(\"Could not find/replace the first Conv2d layer as a direct child in the ConvNeXt stem.\")\n",
    "             \n",
    "        # Replace the backbone's stem with the new sequential module\n",
    "        backbone.stem = nn.Sequential(*new_stem_modules)\n",
    "\n",
    "        print(f\"Updated ConvNeXt stem to handle {in_chans} input channels.\")\n",
    "        print(f\"Assumed model input shape: (B, {in_chans}, {input_height}, {input_width})\")\n",
    "\n",
    "\n",
    "        # --- Determine Feature Map Shapes and Initialize Transformer/Decoder ---\n",
    "        \n",
    "        # Get actual output shapes after stem and stages from the modified backbone\n",
    "        # Use dummy input with the actual expected shape (B, in_chans, input_height, input_width)\n",
    "        dummy_input_for_shape = torch.randn(1, in_chans, input_height, input_width).to(self.cfg.device)\n",
    "        \n",
    "        # Pass dummy data through backbone to get feature shapes at each output point\n",
    "        # `features_only=True` returns a list of tensors.\n",
    "        # For ConvNeXt in timm, this list usually contains outputs after stem and each stage:\n",
    "        # [stem_out, stage0_out, stage1_out, stage2_out, stage3_out].\n",
    "        \n",
    "        try:\n",
    "            # Temporarily disable gradients for shape inference\n",
    "            with torch.no_grad():\n",
    "                # The backbone's forward method with features_only=True handles the sequential pass\n",
    "                features_list = self.backbone(dummy_input_for_shape)\n",
    "                features_shapes = [f.shape for f in features_list]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass for shape inference: {e}\", file=sys.stderr)\n",
    "            print(\"Please check input dimensions, stem modification logic, and backbone compatibility.\", file=sys.stderr)\n",
    "            raise\n",
    "\n",
    "        # features_shapes list corresponds to [stem_out, stage0_out, stage1_out, stage2_out, stage3_out]\n",
    "        # The index in this list where we insert the Transformer. Stage 0 is index 1.\n",
    "        self.transformer_stage_idx_in_features = 1 \n",
    "        if self.transformer_stage_idx_in_features >= len(features_shapes):\n",
    "             raise ValueError(f\"Transformer insertion index {self.transformer_stage_idx_in_features} is out of bounds for backbone features ({len(features_shapes)} levels).\")\n",
    "\n",
    "        # Get channels and spatial size for Transformer input from the chosen stage output shape\n",
    "        transformer_in_channels = features_shapes[self.transformer_stage_idx_in_features][1]\n",
    "        transformer_spatial_h = features_shapes[self.transformer_stage_idx_in_features][2]\n",
    "        transformer_spatial_w = features_shapes[self.transformer_stage_idx_in_features][3]\n",
    "\n",
    "        print(f\"Inserting Transformer after backbone feature index {self.transformer_stage_idx_in_features}.\")\n",
    "        print(f\"Transformer input feature shape determined: (B, {transformer_in_channels}, {transformer_spatial_h}, {transformer_spatial_w})\")\n",
    "\n",
    "        # Initialize the Transformer Block\n",
    "        transformer_config = self.cfg.transformer_config\n",
    "        self.transformer_block = TransformerBlock2d(\n",
    "            in_channels=transformer_in_channels,\n",
    "            hidden_dim=transformer_config.get('hidden_dim', transformer_in_channels), # Use config or default\n",
    "            num_layers=transformer_config.get('num_layers', 1),\n",
    "            num_heads=transformer_config.get('num_heads', 8),\n",
    "            height=transformer_spatial_h, # Pass determined spatial dimensions\n",
    "            width=transformer_spatial_w,  # Pass determined spatial dimensions\n",
    "        )\n",
    "        print(\"Initialized TransformerBlock2d.\")\n",
    "\n",
    "        # --- Initialize the Decoder ---\n",
    "        # The decoder needs encoder_channels corresponding to the channel counts\n",
    "        # of the features it receives, in reverse order of spatial size (bottleneck to high-res skip).\n",
    "        # features_list is [stem, stage0, stage1, stage2, stage3]\n",
    "        # Decoder receives [stage3, stage2, stage1, stage0_transformer_out, stem]\n",
    "        # The channel counts are the second dimension of the shapes.\n",
    "        # The list of channel counts in the order the decoder expects skips/bottleneck:\n",
    "        # [stage3_chs, stage2_chs, stage1_chs, stage0_chs, stem_chs]\n",
    "        ecs = [s[1] for s in features_shapes[::-1]]\n",
    "\n",
    "        # Check if the number of encoder channels matches the decoder levels + 1 bottleneck\n",
    "        # Decoder channels tuple size should be ecs size - 1 (skips) + 1 (bottleneck)\n",
    "        # No, decoder_channels is the list of output channels *of the decoder blocks*.\n",
    "        # UnetDecoder2d's `encoder_channels` argument is the list of *input* channel counts\n",
    "        # it receives from the encoder side. This list is `ecs`.\n",
    "        # The number of levels in `decoder_channels` and `scale_factors` should match\n",
    "        # the number of decoder blocks, which is typically len(encoder_channels) - 1.\n",
    "        # ecs has 5 elements. So decoder should have 4 blocks.\n",
    "        # BUT the original decoder config (256, 128, 64, 32, 32) has 5 elements, and scale_factors has 5 elements.\n",
    "        # This suggests the original decoder might have had 5 blocks, possibly using stem output as a skip.\n",
    "        # Let's stick to the 5-level decoder as in original config.\n",
    "        # UnetDecoder2d takes `encoder_channels`, which are the channels of the features *before* the decoder blocks.\n",
    "        # The first element of `encoder_channels` is the bottleneck input channels.\n",
    "        # The remaining elements are the skip connection channels, in decreasing spatial resolution order.\n",
    "        # So `encoder_channels` = [stage3_chs, stage2_chs, stage1_chs, stage0_chs, stem_chs]. This matches `ecs`.\n",
    "\n",
    "        # Ensure decoder_channels and scale_factors lengths match\n",
    "        decoder_channels_cfg = transformer_config.get('decoder_channels', (256, 128, 64, 32)) # Default 4 levels?\n",
    "        scale_factors_cfg = transformer_config.get('scale_factors', (2,2,2,2))             # Default 4 factors?\n",
    "\n",
    "        # Check consistency between encoder feature levels and decoder levels\n",
    "        num_encoder_features = len(ecs) # 5 features: stage3, stage2, stage1, stage0, stem\n",
    "        # The decoder has num_encoder_features - 1 skip connections and 1 bottleneck input.\n",
    "        # So, typically len(decoder_channels) should be num_encoder_features - 1.\n",
    "        # Original config had 5 decoder_channels and 5 scale_factors. This is unusual.\n",
    "        # It might mean the decoder structure is slightly different, or one level was added/handled differently.\n",
    "        # Let's adjust the default decoder levels to match the common U-Net structure (N encoder features -> N-1 decoder blocks).\n",
    "        # If ecs has 5 elements, decoder should have 4 blocks.\n",
    "        # Let's use the last 4 elements of the original decoder_channels config.\n",
    "        \n",
    "        default_decoder_channels = (256, 128, 64, 32) # 4 levels for 5 encoder features\n",
    "        default_scale_factors = (2,2,2,2)\n",
    "        \n",
    "        decoder_channels_cfg = transformer_config.get('decoder_channels', default_decoder_channels)\n",
    "        scale_factors_cfg = transformer_config.get('scale_factors', default_scale_factors)\n",
    "\n",
    "        if len(decoder_channels_cfg) != num_encoder_features - 1:\n",
    "             print(f\"Warning: Number of decoder levels ({len(decoder_channels_cfg)}) does not match standard U-Net structure for {num_encoder_features} encoder features. Expected {num_encoder_features - 1}.\", file=sys.stderr)\n",
    "             # Adjust if lengths are inconsistent, or trust user config if provided explicitly\n",
    "             if transformer_config.get('decoder_channels') is None:\n",
    "                  print(\"Using default decoder channels matching encoder levels - 1.\", file=sys.stderr)\n",
    "                  decoder_channels_cfg = default_decoder_channels\n",
    "             if transformer_config.get('scale_factors') is None:\n",
    "                  print(\"Using default scale factors matching decoder levels.\", file=sys.stderr)\n",
    "                  scale_factors_cfg = default_scale_factors\n",
    "                  \n",
    "        if len(decoder_channels_cfg) != len(scale_factors_cfg):\n",
    "             raise ValueError(f\"Number of decoder channels ({len(decoder_channels_cfg)}) must match number of scale factors ({len(scale_factors_cfg)}).\")\n",
    "\n",
    "\n",
    "        self.decoder= UnetDecoder2d(\n",
    "            encoder_channels= ecs, # Use actual channel counts from modified backbone\n",
    "            decoder_channels = decoder_channels_cfg,\n",
    "            scale_factors = scale_factors_cfg,\n",
    "            norm_layer = nn.InstanceNorm2d, # Use InstanceNorm consistent with replacement\n",
    "            attention_type = 'scse', # Keep original attention\n",
    "        )\n",
    "        print(\"Initialized UnetDecoder2d.\")\n",
    "        print(f\"Decoder levels: {len(self.decoder.blocks)}\")\n",
    "        print(f\"Decoder input channels (encoder_channels): {self.decoder.encoder_channels}\")\n",
    "        print(f\"Decoder output channels (decoder_channels): {self.decoder.decoder_channels}\")\n",
    "        print(f\"Decoder scale factors: {self.decoder.blocks[0].upsample.scale_factor}\") # Check first block\n",
    "\n",
    "        # Initialize Seg Head\n",
    "        self.seg_head= SegmentationHead2d(\n",
    "            in_channels= self.decoder.decoder_channels[-1], # Last decoder block output channels\n",
    "            out_channels= 1,\n",
    "            scale_factor= 1, # Assuming the last decoder block outputs at the final spatial resolution (70x70 or 72x72)\n",
    "            mode = \"nontrainable\" # Keep original mode\n",
    "        )\n",
    "        print(\"Initialized SegmentationHead2d.\")\n",
    "\n",
    "\n",
    "    # --- Replacement Helper Methods ---\n",
    "\n",
    "    def replace_activations(self, module):\n",
    "        \"\"\"Replaces all activations with GELU recursively, avoiding Transformer block.\"\"\"\n",
    "        for name, child in module.named_children():\n",
    "            # Skip replacements within the TransformerBlock2d\n",
    "            if isinstance(child, TransformerBlock2d):\n",
    "                 continue\n",
    "\n",
    "            if isinstance(child, (\n",
    "                nn.ReLU, nn.LeakyReLU, nn.Mish, nn.Sigmoid,\n",
    "                nn.Tanh, nn.Softmax, nn.Hardtanh, nn.ELU,\n",
    "                nn.SELU, nn.PReLU, nn.CELU, nn.GELU, nn.SiLU,\n",
    "            )):\n",
    "                # Only replace if it's *not* GELU or SiLU already (common in newer models)\n",
    "                if not isinstance(child, (nn.GELU, nn.SiLU)):\n",
    "                    setattr(module, name, nn.GELU())\n",
    "            else:\n",
    "                self.replace_activations(child)\n",
    "\n",
    "    def replace_norms(self, mod):\n",
    "        \"\"\"Replaces BatchNorm2d with InstanceNorm2d recursively, avoiding Transformer block.\"\"\"\n",
    "        for name, c in mod.named_children():\n",
    "            # Skip replacements within the TransformerBlock2d\n",
    "            if isinstance(c, TransformerBlock2d):\n",
    "                 continue\n",
    "\n",
    "            # Replace BatchNorm2d with InstanceNorm2d\n",
    "            if isinstance(c, nn.BatchNorm2d):\n",
    "                 n_feats= c.num_features\n",
    "                 new = nn.InstanceNorm2d(\n",
    "                     n_feats,\n",
    "                     affine=True, # Keep affine=True as in original replacement\n",
    "                     )\n",
    "                 setattr(mod, name, new)\n",
    "                 # print(f\"Replaced BatchNorm2d {name} with InstanceNorm2d\")\n",
    "            # Optional: Replace other norms if necessary, but avoid LayerNorm within standard Transformer layers\n",
    "            # elif isinstance(c, nn.LayerNorm):\n",
    "            #      # Careful: Only replace LayerNorms that are part of the ConvNeXt/UNet structure if needed\n",
    "            #      # E.g., ConvNeXt blocks have LayerNorm.\n",
    "            #      pass # Avoid replacing LayerNorm unless specifically targeted\n",
    "\n",
    "            else:\n",
    "                # Recursively apply to children\n",
    "                self.replace_norms(c)\n",
    "\n",
    "    def replace_forwards(self, mod):\n",
    "        \"\"\"Replaces forward pass for specific modules like ConvNeXtBlock if needed.\"\"\"\n",
    "        # Original code replaced ConvNeXtBlock forward. Keep this logic.\n",
    "        for name, c in mod.named_children():\n",
    "            # Skip Transformer block\n",
    "            if isinstance(c, TransformerBlock2d):\n",
    "                 continue\n",
    "\n",
    "            if isinstance(c, ConvNeXtBlock):\n",
    "                # Apply the custom forward method\n",
    "                c.forward = MethodType(_convnext_block_forward, c)\n",
    "            else:\n",
    "                self.replace_forwards(c)\n",
    "\n",
    "    # --- Forward Pass ---\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Input batch shape is (B, 5 * N_slices, H_in, 70)\n",
    "        x = batch\n",
    "\n",
    "        # Run through the modified backbone.\n",
    "        # It returns a list of features: [stem_out, stage0_out, stage1_out, stage2_out, stage3_out].\n",
    "        feats = self.backbone(x)\n",
    "\n",
    "        # Apply Transformer block to the output of the specified stage (e.g., stage0)\n",
    "        # The feature list index for stage0 output is self.transformer_stage_idx_in_features (should be 1)\n",
    "        stage_output_for_transformer = feats[self.transformer_stage_idx_in_features]\n",
    "        transformer_output = self.transformer_block(stage_output_for_transformer)\n",
    "\n",
    "        # Replace the original stage output with the transformer output in the feature list\n",
    "        modified_feats = list(feats) # Create a copy to not modify the original list from backbone\n",
    "        modified_feats[self.transformer_stage_idx_in_features] = transformer_output\n",
    "\n",
    "        # Decoder input expects features in reverse order of spatial size (bottleneck to high-res skip)\n",
    "        # features_list from backbone: [stem_out, stage0_out, stage1_out, stage2_out, stage3_out]\n",
    "        # Original decoder expects: [stage3_out, stage2_out, stage1_out, stage0_out, stem_out]\n",
    "        # Our modified decoder input list: [stage3_out, stage2_out, stage1_out, transformer_output, stem_out]\n",
    "        \n",
    "        # The list `modified_feats` is already in the order [stem, stage0, stage1, stage2, stage3]\n",
    "        # Reversing it gives [stage3, stage2, stage1, modified_stage0, stem]\n",
    "        decoder_input_feats = modified_feats[::-1] # Reverse the list of features\n",
    "\n",
    "        # Decoder takes the list of features\n",
    "        # UnetDecoder2d expects: [bottleneck, skip3, skip2, skip1, skip0]\n",
    "        # decoder_input_feats[0] = stage3_out (bottleneck)\n",
    "        # decoder_input_feats[1] = stage2_out (skip3)\n",
    "        # decoder_input_feats[2] = stage1_out (skip2)\n",
    "        # decoder_input_feats[3] = transformer_output (replacing stage0_out, acting as skip1)\n",
    "        # decoder_input_feats[4] = stem_out (acting as skip0)\n",
    "        # This matches the structure if the decoder has 4 blocks processing skips and one bottleneck input.\n",
    "        # Our decoder_channels config has 4 levels, matching 4 blocks.\n",
    "        # The UnetDecoder expects N encoder features, uses the first as bottleneck, and the rest as N-1 skips.\n",
    "        # So ecs = [stage3, stage2, stage1, stage0, stem]. len=5.\n",
    "        # Decoder expects bottleneck ecs[0] (stage3). Skips ecs[1:] (stage2, stage1, stage0, stem).\n",
    "        # UnetDecoder2d takes encoder_channels=[stage3_chs, stage2_chs, stage1_chs, stage0_chs, stem_chs]\n",
    "        # Its first block uses stage3_chs as input, skip stage2_chs.\n",
    "        # Second block uses output of first, skip stage1_chs.\n",
    "        # ...\n",
    "        # Fourth block uses output of third, skip stem_chs.\n",
    "        # The list passed to self.decoder(list) needs to be in the order of skips/bottleneck.\n",
    "        # The `modified_feats[::-1]` is already in the correct order for `UnetDecoder2d`.\n",
    "\n",
    "        x_dec_outputs = self.decoder(decoder_input_feats)\n",
    "\n",
    "        # Seg Head takes the last decoder output (highest resolution)\n",
    "        # The last decoder output should have spatial dimensions matching the target 70x70 or slightly larger.\n",
    "        x_seg = self.seg_head(x_dec_outputs[-1])\n",
    "\n",
    "        # Final output processing (cropping, scaling)\n",
    "        # Crop 1 pixel border. Assuming seg_head output is 72x72 to get 70x70.\n",
    "        # This depends on the exact upsampling in the decoder and seg_head.\n",
    "        # If the last decoder block outputs 70x70 and seg_head has scale_factor=1, output is 70x70.\n",
    "        # Then cropping is not needed or needs adjustment.\n",
    "        # Let's keep the original cropping logic, assuming the final output is slightly larger than 70x70.\n",
    "        x_seg = x_seg[..., 1:-1, 1:-1] # Crop to 70x70\n",
    "\n",
    "        x_seg = x_seg * 1500 + 3000 # Scale output\n",
    "\n",
    "\n",
    "        if self.training:\n",
    "            return x_seg\n",
    "        else:\n",
    "             # Test-time augmentation with flip\n",
    "             # The proc_flip function re-runs the model forward path on flipped input.\n",
    "             # It must include the Transformer block in its logic.\n",
    "             p1 = self.proc_flip(x_in=batch) # Pass original batch for flip\n",
    "             x_seg = torch.mean(torch.stack([x_seg, p1]), dim=0)\n",
    "             return x_seg\n",
    "\n",
    "    def proc_flip(self, x_in):\n",
    "        \"\"\"\n",
    "        Processes input with spatial flip for test-time augmentation.\n",
    "        Assumes input is (B, C_stacked, H_in, W_in).\n",
    "        Flips spatial dimensions H_in and W_in.\n",
    "        \"\"\"\n",
    "        # x_in shape: (B, 5 * N_slices, H_in, 70)\n",
    "        # Spatial flip on H_in and W_in dimensions (-2, -1)\n",
    "        x_in_flipped = torch.flip(x_in, dims=[-2, -1])\n",
    "\n",
    "        # Run flipped input through backbone to get flipped features\n",
    "        # Returns [stem_out, stage0_out, stage1_out, stage2_out, stage3_out] (flipped)\n",
    "        feats_flipped = self.backbone(x_in_flipped)\n",
    "\n",
    "        # Apply Transformer block to the flipped stage output\n",
    "        stage_output_for_transformer_flipped = feats_flipped[self.transformer_stage_idx_in_features]\n",
    "        transformer_output_flipped = self.transformer_block(stage_output_for_transformer_flipped)\n",
    "\n",
    "        # Replace original stage output with transformer output in the flipped feature list\n",
    "        modified_feats_flipped = list(feats_flipped)\n",
    "        modified_feats_flipped[self.transformer_stage_idx_in_features] = transformer_output_flipped\n",
    "\n",
    "        # Decoder input expects features in reverse order\n",
    "        decoder_input_feats_flipped = modified_feats_flipped[::-1]\n",
    "\n",
    "        # Run flipped features through decoder\n",
    "        x_dec_outputs_flipped = self.decoder(decoder_input_feats_flipped)\n",
    "\n",
    "        # Seg Head on the last decoder output\n",
    "        x_seg_flipped = self.seg_head(x_dec_outputs_flipped[-1])\n",
    "\n",
    "        # Post-processing - Crop and Flip back the output spatially\n",
    "        # Crop should match the size expected before flipping back.\n",
    "        x_seg_flipped = x_seg_flipped[..., 1:-1, 1:-1] # Crop to 70x70\n",
    "\n",
    "        # Flip back spatial dimensions (H and W) of the output\n",
    "        # Original code only flipped W (-1). Let's stick to flipping H and W (-2, -1) for symmetry.\n",
    "        x_seg_flipped = torch.flip(x_seg_flipped, dims=[-2, -1]) # Flip H and W back\n",
    "\n",
    "        x_seg_flipped = x_seg_flipped * 1500 + 3000 # Scale\n",
    "\n",
    "        return x_seg_flipped\n",
    "\n",
    "\n",
    "# --- Helper method for ConvNeXtBlock forward replacement (from original code) ---\n",
    "# This method needs to be defined outside the class or within it if it's a static/class method.\n",
    "# Keeping it outside and using MethodType in __init__.\n",
    "def _convnext_block_forward(self, x):\n",
    "    \"\"\"Custom forward pass for ConvNeXtBlock, potentially from original code.\"\"\"\n",
    "    # This method is intended to replace the original forward of timm.models.convnext.ConvNeXtBlock\n",
    "    # It needs access to self (the ConvNeXtBlock instance).\n",
    "    # It should be copied verbatim from the original code if possible.\n",
    "    # The provided snippet has this function. Let's copy it.\n",
    "\n",
    "    shortcut = x\n",
    "    x = self.conv_dw(x)\n",
    "\n",
    "    if self.use_conv_mlp:\n",
    "        x = self.norm(x) # This norm might be targeted by replace_norms\n",
    "        x = self.mlp(x)\n",
    "    else:\n",
    "        # Original code permutes for layernorm/mlp if not use_conv_mlp\n",
    "        # The norm here is nn.LayerNorm in standard ConvNeXt\n",
    "        x = self.norm(x) # This norm is LayerNorm, replace_norms should skip it\n",
    "        x = x.permute(0, 2, 3, 1) # (B, H, W, C)\n",
    "        x = x.contiguous()\n",
    "        x = self.mlp(x)\n",
    "        x = x.permute(0, 3, 1, 2) # (B, C, H, W)\n",
    "        x = x.contiguous()\n",
    "\n",
    "    if self.gamma is not None:\n",
    "        # Gamma is for LayerScale\n",
    "        x = x * self.gamma.reshape(1, -1, 1, 1)\n",
    "\n",
    "    # Drop path and residual connection\n",
    "    x = self.drop_path(x) + self.shortcut(shortcut)\n",
    "    return x\n",
    "\n",
    "# --- End of Helper Method ---\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device based on config\n",
    "    device = torch.device(\"cuda\" if USE_DEVICE == 'GPU' and torch.cuda.is_available() else \"cpu\")\n",
    "    cfg.device = device\n",
    "    cfg.local_rank = 0 # Assuming single process\n",
    "\n",
    "    print(f\"[main loop]- Using device: {cfg.device}\")\n",
    "\n",
    "    set_seed(cfg.seed)\n",
    "\n",
    "    # File paths (assuming these are set up correctly)\n",
    "    data_paths_str = \"./datasetfiles/FlatVel_A/data/*.npy\"\n",
    "    label_paths_str = \"./datasetfiles/FlatVel_A/model/*.npy\"\n",
    "\n",
    "    # Get all file pairs\n",
    "    data_paths = sorted(glob.glob(data_paths_str))\n",
    "    label_paths = sorted(glob.glob(label_paths_str))\n",
    "    if not data_paths or not label_paths or len(data_paths) != len(label_paths):\n",
    "         print(\"[main loop]- Error: Data or label files not found or mismatch count.\", file=sys.stderr)\n",
    "         # sys.exit(1) # Exit if no data found\n",
    "\n",
    "    all_file_pairs = list(zip(data_paths, label_paths))\n",
    "\n",
    "    if not all_file_pairs:\n",
    "        print(\"[main loop]- No file pairs found. Please check data_paths_str and label_paths_str.\", file=sys.stderr)\n",
    "        RUN_TRAIN = RUN_VALID = RUN_TEST = False # Disable runs if no data\n",
    "\n",
    "\n",
    "    # Split file pairs for train/validation/test\n",
    "    # Simple split (e.g., 80% train, 20% validation/test combined)\n",
    "    split_ratio_train = 0.8\n",
    "    split_idx_train = int(len(all_file_pairs) * split_ratio_train)\n",
    "\n",
    "    train_file_pairs = all_file_pairs[:split_idx_train]\n",
    "    # Use the rest for validation for this example\n",
    "    valid_file_pairs = all_file_pairs[split_idx_train:]\n",
    "    # If a separate test set is needed, split valid_file_pairs further.\n",
    "\n",
    "\n",
    "    # Create datasets using the modified class\n",
    "    if RUN_TRAIN:\n",
    "        # train_ds = CustomDataset(cfg=cfg, file_pairs=train_file_pairs, mode=\"train\")\n",
    "        train_ds = CustomDatasetWithSlices(cfg=cfg, file_pairs=train_file_pairs, mode=\"train\", num_input_slices=cfg.num_input_slices)\n",
    "        train_dl = torch.utils.data.DataLoader(\n",
    "            train_ds,\n",
    "            batch_size= cfg.batch_size,\n",
    "            num_workers= 4 if cfg.device.type == 'cuda' else 0, # Use more workers on GPU\n",
    "            shuffle=True,\n",
    "            pin_memory=cfg.device.type == 'cuda', # Pin memory for faster GPU transfer\n",
    "            drop_last=True, # Drop last batch if batch size doesn't divide dataset size\n",
    "        )\n",
    "        # Update cfg with actual H_in from dataset\n",
    "        cfg.inferred_input_height = train_ds.H_in\n",
    "        cfg.input_width = train_ds.W_in # Update if dataset detected different width\n",
    "\n",
    "\n",
    "    if RUN_VALID or RUN_TEST: # Create validation dataset if needed for validation or testing\n",
    "        # valid_ds = CustomDataset(cfg=cfg, file_pairs=valid_file_pairs, mode=\"valid\")\n",
    "        valid_ds = CustomDatasetWithSlices(cfg=cfg, file_pairs=valid_file_pairs, mode=\"valid\", num_input_slices=cfg.num_input_slices)\n",
    "        valid_dl = torch.utils.data.DataLoader(\n",
    "            valid_ds,\n",
    "            batch_size= cfg.batch_size_val,\n",
    "            num_workers= 4 if cfg.device.type == 'cuda' else 0, # Use more workers on GPU\n",
    "            shuffle=False, # No shuffle for validation/test\n",
    "            pin_memory=cfg.device.type == 'cuda',\n",
    "            drop_last=False, # Don't drop last batch for evaluation\n",
    "        )\n",
    "        # Update cfg with actual H_in from dataset if not already set by train_ds\n",
    "        if not hasattr(cfg, 'inferred_input_height'):\n",
    "             cfg.inferred_input_height = valid_ds.H_in\n",
    "             cfg.input_width = valid_ds.W_in\n",
    "\n",
    "\n",
    "    if RUN_TRAIN or RUN_VALID or RUN_TEST:\n",
    "        # Create the new model\n",
    "        model = NetWithTransformer(cfg=cfg, backbone=cfg.backbone).to(cfg.device)\n",
    "        print(f\"[main loop]- Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters.\")\n",
    "\n",
    "        if cfg.ema and RUN_TRAIN: # Only initialize EMA if training is enabled\n",
    "            print(\"[main loop]- Initializing EMA model..\")\n",
    "            ema_model = ModelEMA(\n",
    "                model,\n",
    "                decay=cfg.ema_decay,\n",
    "                device=cfg.device, # EMA model on the same device as the main model\n",
    "            )\n",
    "        else:\n",
    "            ema_model = None\n",
    "\n",
    "        criterion = nn.L1Loss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "        best_loss= 1_000_000 # Initialize with a high value\n",
    "        # Initialize val_loss for logging on epoch 0 if validation is run\n",
    "        val_loss = float('inf') if RUN_VALID else -1.0 # Use infinity if validating, else placeholder\n",
    "\n",
    "\n",
    "        print(f\"[main loop]- Starting training for {cfg.epochs} epochs...\")\n",
    "\n",
    "        for epoch in range(1, cfg.epochs + 1): # Start from epoch 1\n",
    "            tstart= time.time()\n",
    "\n",
    "            # Train loop\n",
    "            if RUN_TRAIN:\n",
    "                model.train()\n",
    "                total_loss = []\n",
    "                train_loop = tqdm(train_dl, disable=cfg.local_rank != 0, desc=f\"Epoch {epoch} Training\")\n",
    "                for i, (x, y) in enumerate(train_loop):\n",
    "                    x = x.to(cfg.device)\n",
    "                    y = y.to(cfg.device) # y is (B, 70, 70)\n",
    "\n",
    "                    logits = model(x) # logits is (B, 1, 70, 70)\n",
    "\n",
    "                    # Criterion expects (B, 1, H, W) and (B, 1, H, W) or (B, H, W)\n",
    "                    # Add channel dim to y (B, 70, 70) -> (B, 1, 70, 70)\n",
    "                    loss = criterion(logits, y.unsqueeze(1))\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    # UD tracking - Removed for simplicity in this version\n",
    "                    # lr = optimizer.param_groups[0]['lr']\n",
    "                    # with torch.no_grad():\n",
    "                    #    pass # UD tracking omitted\n",
    "\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    total_loss.append(loss.item())\n",
    "\n",
    "                    if ema_model is not None:\n",
    "                        ema_model.update(model)\n",
    "                    \n",
    "                    # Update tqdm description\n",
    "                    train_loop.set_postfix(loss=np.mean(total_loss[-cfg.logging_steps:] if len(total_loss) >= cfg.logging_steps else total_loss)) # Removed lr from postfix\n",
    "\n",
    "\n",
    "                avg_train_loss = np.mean(total_loss)\n",
    "                print(f\"\\nEpoch {epoch} Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "            # Validation loop\n",
    "            if RUN_VALID:\n",
    "                model.eval()\n",
    "                val_logits = []\n",
    "                val_targets = []\n",
    "                valid_loop = tqdm(valid_dl, disable=cfg.local_rank != 0, desc=f\"Epoch {epoch} Validation\")\n",
    "                with torch.no_grad():\n",
    "                    for x, y in valid_loop:\n",
    "                        x = x.to(cfg.device)\n",
    "                        y = y.to(cfg.device)\n",
    "\n",
    "                        # Use EMA model if available, otherwise use main model\n",
    "                        current_model = ema_model.module if ema_model is not None else model\n",
    "                        out = current_model(x) # This includes TTA (flip average)\n",
    "\n",
    "                        val_logits.append(out.cpu())\n",
    "                        val_targets.append(y.cpu())\n",
    "\n",
    "                    val_logits= torch.cat(val_logits, dim=0) # (N_samples_val, 1, 70, 70)\n",
    "                    val_targets= torch.cat(val_targets, dim=0) # (N_samples_val, 70, 70)\n",
    "\n",
    "                    # Criterion expects (B, 1, H, W) and (B, 1, H, W) or (B, H, W)\n",
    "                    val_loss = criterion(val_logits, val_targets.unsqueeze(1)).item() # Add channel dim to targets\n",
    "\n",
    "                    print(f\"Epoch {epoch} Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "                    # Save best model (if validation loss is lower)\n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        print(f\"Validation loss improved. Saving model to 'best_model.pth'.\")\n",
    "                        # Save the state dict of the main model (or EMA model)\n",
    "                        save_model = ema_model.module if ema_model is not None else model\n",
    "                        # Save to a fixed name 'best_model.pth' to easily load the best one\n",
    "                        torch.save(save_model.state_dict(), f\"best_model.pth\")\n",
    "                        cfg.early_stopping['streak'] = 0 # Reset streak\n",
    "                    else:\n",
    "                        cfg.early_stopping['streak'] += 1 # Increment streak\n",
    "                        print(f\"Validation loss did not improve. Early stopping streak: {cfg.early_stopping['streak']}/{cfg.early_stopping['patience']}.\")\n",
    "\n",
    "\n",
    "            t_epoch = time.time() - tstart\n",
    "            print(f\"Epoch {epoch} finished in {format_time(t_epoch)}.\")\n",
    "\n",
    "            # Early Stopping check\n",
    "            if RUN_VALID and cfg.early_stopping['streak'] >= cfg.early_stopping['patience']:\n",
    "                 print(f\"Early stopping triggered after {cfg.early_stopping['patience']} epochs without improvement.\")\n",
    "                 break # Exit training loop\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "\n",
    "    # --- Testing (Evaluation on validation/test set) ---\n",
    "    # Use RUN_TEST flag to control final evaluation\n",
    "    if RUN_TEST:\n",
    "        print(\"\\nRunning final evaluation...\")\n",
    "        # Load the best model if it was saved during validation\n",
    "        test_model = NetWithTransformer(cfg=cfg, backbone=cfg.backbone).to(cfg.device)\n",
    "        best_model_path = \"best_model.pth\" # Use the fixed name\n",
    "        if os.path.exists(best_model_path):\n",
    "            print(f\"Loading best model from {best_model_path}\")\n",
    "            try:\n",
    "                test_model.load_state_dict(torch.load(best_model_path, map_location=cfg.device))\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error loading model state_dict: {e}\", file=sys.stderr)\n",
    "                print(\"This might happen if model architecture or saved state_dict mismatch.\", file=sys.stderr)\n",
    "                # Attempt partial load or skip loading if critical error\n",
    "                print(\"Attempting to load without strict matching...\", file=sys.stderr)\n",
    "                try:\n",
    "                    test_model.load_state_dict(torch.load(best_model_path, map_location=cfg.device), strict=False)\n",
    "                    print(\"Partial load successful.\")\n",
    "                except Exception as e_partial:\n",
    "                    print(f\"Partial load failed: {e_partial}\", file=sys.stderr)\n",
    "                    print(\"Proceeding with randomly initialized model (or last epoch if no best saved).\", file=sys.stderr)\n",
    "\n",
    "        else:\n",
    "            print(\"No best model found at 'best_model.pth'. Using the last state from training (if RUN_TRAIN was true).\")\n",
    "            # If training didn't save 'best_model.pth' (e.g. RUN_VALID=False),\n",
    "            # the 'model' variable holds the last trained state.\n",
    "            # If RUN_TRAIN was also false, this is a fresh model.\n",
    "\n",
    "        test_model.eval()\n",
    "        test_logits = []\n",
    "        test_targets = []\n",
    "        # Use valid_dl for testing for this example, assuming it's the evaluation set\n",
    "        test_loop = tqdm(valid_dl, disable=cfg.local_rank != 0, desc=\"Evaluating\")\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loop:\n",
    "                 x = x.to(cfg.device)\n",
    "                 y = y.to(cfg.device)\n",
    "\n",
    "                 # Model forward includes TTA (flip average) when not in training mode\n",
    "                 out = test_model(x)\n",
    "\n",
    "                 test_logits.append(out.cpu())\n",
    "                 test_targets.append(y.cpu())\n",
    "\n",
    "        test_logits = torch.cat(test_logits, dim=0) # (N_samples_test, 1, 70, 70)\n",
    "        test_targets = torch.cat(test_targets, dim=0) # (N_samples_test, 70, 70)\n",
    "\n",
    "        # Calculate final loss on the test set\n",
    "        final_test_loss = criterion(test_logits, test_targets.unsqueeze(1)).item()\n",
    "        print(f\"\\nFinal Evaluation Loss: {final_test_loss:.4f}\")\n",
    "\n",
    "        # Optional: Save predictions or visualize results\n",
    "        # print(\"Saving predictions (optional)...\")\n",
    "        # np.save(\"test_predictions.npy\", test_logits.numpy())\n",
    "        # np.save(\"test_targets.npy\", test_targets.numpy())\n",
    "\n",
    "    print(\"Script finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb66ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a0d497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892aadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd7cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
