{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738acf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "はい、ご提示いただいたConvNeXtベースのモデルに、waveformの時系列情報（T=1000）とGeoPhoneの空間情報（C=70）をより効果的に取り込むためのTransformer要素を組み込んだハイブリッドモデルのコードを生成します。\n",
    "\n",
    "現在のモデルは、入力waveform `(B, C=70, T=1000, W=70)` から特定のタイムステップにおける2Dスライス `(C=70, W=70)` （あるいは `(H=70, W=70)` と解釈される何か）を取り出して2D ConvNetに入力しているようです。これでは時系列情報（T=1000）のほとんどが失われてしまいます。また、GeoPhone数（C=70）がチャネルとして扱われるべきですが、現在のConvNeXt Encoderは `in_chans=5` となっており、入力チャネル数の扱いが不明確です。\n",
    "\n",
    "ユーザーの要望「時系列や７０か所のGeoPhoneのずれの情報も取込」を真に満たすには、Datasetから waveform の3D情報全体（または適切な時系列ウィンドウ）を取得し、モデル側で3Dデータとして処理する必要があります。\n",
    "\n",
    "ここでは、以下の構造を持つハイブリッドモデルを提案・実装します。\n",
    "1.  **Datasetの変更**: 各ファイルから waveform 全体 `(T=1000, C=70, W=70)` （これを `(D=1000, H=70, W=70)` と解釈し、チャネルを1とします `(1, 1000, 70, 70)`）と、対応する2D地層図 `(H=70, W=70)` ラベルを返すように修正します。\n",
    "2.  **3D Conv Encoder**: 入力の3D waveform `(B, 1, 1000, 70, 70)` に対して、3D Convolutional層を重ねて特徴抽出し、時間・空間次元をダウンサンプリングします。\n",
    "3.  **Transformer Bottleneck**: 3D Conv Encoderの出力である圧縮された3D特徴マップをflattenしてシーケンスに変換し、Transformer Encoderで処理します。これにより、グローバルな時空間的関係性を捉えます。\n",
    "4.  **Projection to 2D**: Transformerの出力を、2D Decoderが受け付けられる2D特徴マップの形状に変換します（例: 時間方向のPooling）。\n",
    "5.  **2D Decoder**: 既存の `UnetDecoder2d` ライクな構造を再利用し、Bottleneckからの2D特徴マップと、3D Conv Encoderの途中からのSkip Connection（これも2Dに投影）を使って、最終的な2D地層図 `(70, 70)` を生成します。\n",
    "6.  **Segmentation Head**: Decoderの出力を最終的な出力次元に変換します。\n",
    "\n",
    "これにより、 waveform の時間情報（T=1000）と空間情報（GeoPhoneの配置C=70、Spatial W=70）を3Dデータとして扱い、Conv3Dでローカルな特徴を、Transformerでグローバルな特徴を学習できます。\n",
    "\n",
    "Datasetの修正と、それに合わせた新しい `HybridModel` クラスのコードを以下に示します。既存のトレーニングループと評価ループは、新しいモデルクラスとDatasetを使用するように調整します。\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time, glob\n",
    "import numpy as np\n",
    "import sys # Add sys for stderr\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# Removed torch.distributed and torch.cuda.amp based on user's setup\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from _cfg import cfg # Replace with SimpleNamespace\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import timm\n",
    "    # from timm.models.layers import DropPath, trunc_normal_ # DropPath requires full timm or specific import\n",
    "    from timm.models.layers import trunc_normal_ # Assuming trunc_normal_ is enough\n",
    "except ImportError:\n",
    "    print(\"timm not found, installing...\")\n",
    "    os.system(\"pip install --no-deps timm -q\")\n",
    "    import timm\n",
    "    from timm.models.layers import trunc_normal_\n",
    "\n",
    "try:\n",
    "    import monai\n",
    "    from monai.networks.blocks import UpSample, SubpixelUpsample, ConvAct, Act, Norm\n",
    "    from monai.networks.layers import PatchEmbed # Might not need PatchEmbed directly, but related concepts\n",
    "except ImportError:\n",
    "    print(\"monai not found, installing...\")\n",
    "    os.system(\"pip install --no-deps monai -q\")\n",
    "    import monai\n",
    "    from monai.networks.blocks import UpSample, SubpixelUpsample, ConvAct, Act, Norm\n",
    "    from monai.networks.layers import PatchEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SimpleNamespace for cfg based on the user's snippet\n",
    "cfg= SimpleNamespace()\n",
    "cfg.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg.local_rank = 0 # Assume single GPU or CPU\n",
    "cfg.seed = 123\n",
    "cfg.subsample = None # Set to None to use all data, or e.g., 100 for a small subset\n",
    "\n",
    "data_paths_str = \"./datasetfiles/FlatVel_A/data/*.npy\"\n",
    "label_paths_str = \"./datasetfiles/FlatVel_A/model/*.npy\"\n",
    "cfg.file_pairs = list(zip(sorted(glob.glob(data_paths_str)), sorted(glob.glob(label_paths_str))))\n",
    "\n",
    "# Model parameters for the new HybridModel\n",
    "cfg.in_channels = 1 # Input waveform channel (value)\n",
    "cfg.spatial_size = (1000, 70, 70) # (D, H, W) - Time, GeoPhone Locations, Spatial Width\n",
    "cfg.encoder_dims = (32, 64, 128, 256) # Channels for 3D Conv stages\n",
    "cfg.embed_dim = 768 # Dimension for Transformer\n",
    "cfg.num_heads = 8 # Attention heads in Transformer\n",
    "cfg.mlp_ratio = 4. # MLP ratio in Transformer\n",
    "cfg.transformer_depth = 6 # Number of Transformer blocks\n",
    "cfg.decoder_channels = (256, 128, 64, 32) # Output channels of 2D Decoder blocks (4 blocks)\n",
    "cfg.decoder_attention_type = \"scse\" # For 2D Decoder\n",
    "cfg.upsample_mode = \"transpose\" # For 2D Decoder\n",
    "\n",
    "cfg.ema = True\n",
    "cfg.ema_decay = 0.99\n",
    "\n",
    "cfg.epochs = 4\n",
    "cfg.batch_size = 8\n",
    "cfg.batch_size_val = 8\n",
    "\n",
    "cfg.early_stopping = {\"patience\": 3, \"streak\": 0}\n",
    "cfg.logging_steps = 10\n",
    "\n",
    "\n",
    "def set_seed(seed=cfg.seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # Removed distributed/cuda specific seed setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d45818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CustomDataset Modification ---\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        file_pairs,\n",
    "        mode=\"train\",\n",
    "    ):\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.file_pairs = file_pairs\n",
    "\n",
    "        # Data and labels loaded as list of arrays (mmap)\n",
    "        self.data, self.labels = self._load_data_arrays()\n",
    "\n",
    "        # Assuming labels are (70, 70) per file, not time-dependent\n",
    "        # The number of samples is simply the number of files, potentially subsampled\n",
    "        total_files_available = len(self.data)\n",
    "\n",
    "        # Subsample logic applies to files, not time steps within files\n",
    "        subsample = getattr(self.cfg, \"subsample\", None)\n",
    "        self.total_samples = min(subsample, total_files_available) if subsample else total_files_available\n",
    "\n",
    "        # Index map is now just a list of file indices\n",
    "        self.index_map = list(range(self.total_samples))\n",
    "\n",
    "\n",
    "    def _load_data_arrays(self):\n",
    "        data_arrays = []\n",
    "        label_arrays = []\n",
    "        mmap_mode = \"r\"\n",
    "\n",
    "        for data_fpath, label_fpath in tqdm(\n",
    "                        self.file_pairs, desc=f\"Loading {self.mode} data (mmap)\",\n",
    "                        disable=self.cfg.local_rank != 0):\n",
    "            try:\n",
    "                # Load the numpy arrays using memory mapping\n",
    "                # Expected data shape: (T, C, W) approx (1000, 70, 70)\n",
    "                # Expected label shape: (H, W) approx (70, 70)\n",
    "                arr = np.load(data_fpath, mmap_mode=mmap_mode)\n",
    "                lbl = np.load(label_fpath, mmap_mode=mmap_mode)\n",
    "                # print(f\"Loaded {data_fpath}: data {arr.shape}, label {lbl.shape}\") # Keep this check during development\n",
    "                data_arrays.append(arr)\n",
    "                label_arrays.append(lbl)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: File not found - {data_fpath} or {label_fpath}\", file=sys.stderr)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file pair: {data_fpath}, {label_fpath}\", file=sys.stderr)\n",
    "                print(f\"Error: {e}\", file=sys.stderr)\n",
    "                continue\n",
    "\n",
    "        if self.cfg.local_rank == 0:\n",
    "            print(f\"Finished loading {len(data_arrays)} file pairs for {self.mode} mode.\") # Avoid spamming\n",
    "\n",
    "        return data_arrays, label_arrays\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = self.index_map[idx]\n",
    "\n",
    "        x_full = self.data[file_idx] # Shape (T, C, W) = (1000, 70, 70)\n",
    "        y_full = self.labels[file_idx] # Shape (H, W) = (70, 70)\n",
    "\n",
    "        # --- Augmentations (Apply to full 3D data and 2D label if applicable) ---\n",
    "        # Note: 3D augmentations are more complex. Simple spatial flips applied consistently.\n",
    "        x_augmented = x_full\n",
    "        y_augmented = y_full\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            # Temporal flip (flips time dimension T)\n",
    "            if np.random.random() < 0.5:\n",
    "                x_augmented = x_augmented[::-1, ...] # Flip time axis (dim 0)\n",
    "\n",
    "            # Spatial flip (flips W dimension) - Apply to both data (over W) and label (over W)\n",
    "            if np.random.random() < 0.5:\n",
    "                x_augmented = x_augmented[..., ::-1] # Flip W axis (dim 2)\n",
    "                y_augmented = y_augmented[:, ::-1]   # Flip W axis (dim 1) for label (H, W)\n",
    "\n",
    "            # Add more 3D augmentations if needed (e.g., shifts, rotations, scaling)\n",
    "\n",
    "        # --- Convert to Tensor and add Channel dimension for Conv3d ---\n",
    "        # PyTorch Conv3d expects (N, C_in, D, H, W)\n",
    "        # Our data is (T, C, W) = (1000, 70, 70). Map this to (D, H, W) with C_in=1.\n",
    "        # D = Time (1000), H = GeoPhone locations (70), W = Spatial width (70)\n",
    "        x_tensor = torch.from_numpy(x_augmented.copy()).float().unsqueeze(0) # Shape (1, 1000, 70, 70)\n",
    "        y_tensor = torch.from_numpy(y_augmented.copy()).float() # Shape (70, 70)\n",
    "\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cecce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transformer Block Definition ---\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        # Using nn.MultiheadAttention. batch_first=True is crucial.\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=attn_drop, bias=qkv_bias, batch_first=True)\n",
    "        self.drop_path = nn.Identity() # Replace with DropPath if timm is available\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            act_layer(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B, SeqLen, Dim)\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        # MultiheadAttention needs query, key, value. Self-attention uses x for all three.\n",
    "        # attn returns output, weights. We only need output [0].\n",
    "        x = self.attn(x, x, x)[0]\n",
    "        x = residual + self.drop_path(x)\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = residual + self.drop_path(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# --- Helper function to calculate output size of convolution ---\n",
    "def calc_conv_out_size(size_in, kernel_size, stride, padding):\n",
    "    return (size_in + 2 * padding - kernel_size) // stride + 1\n",
    "\n",
    "\n",
    "# --- Hybrid Model Class ---\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        spatial_size=(1000, 70, 70), # (D, H, W) input shape\n",
    "        encoder_dims=(32, 64, 128, 256), # Output channels of 3D Conv stages\n",
    "        embed_dim=768, # Dimension for Transformer\n",
    "        num_heads=8, # Attention heads in Transformer\n",
    "        mlp_ratio=4., # MLP ratio in Transformer\n",
    "        transformer_depth=6, # Number of Transformer blocks\n",
    "        decoder_channels: tuple = (256, 128, 64, 32), # Output channels of 2D Decoder blocks\n",
    "        decoder_attention_type: str = \"scse\", # For 2D Decoder\n",
    "        upsample_mode: str = \"transpose\", # For 2D Decoder UpSample layers\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.spatial_size = spatial_size # (D, H, W)\n",
    "        self.encoder_dims = encoder_dims\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.transformer_depth = transformer_depth\n",
    "        self.decoder_channels = decoder_channels\n",
    "\n",
    "        self.num_encoder_stages = len(encoder_dims)\n",
    "        self.conv_stages = nn.ModuleList()\n",
    "        self._skip_3d_spatial_sizes = [] # Store (D, H, W) size after each 3D conv stage\n",
    "\n",
    "        # --- 3D Conv Encoder Stages ---\n",
    "        # Downsample (D, H, W) dimensions\n",
    "        in_dim = in_channels\n",
    "        current_spatial_size = list(spatial_size) # [D, H, W]\n",
    "\n",
    "        # Example 3D Conv configurations (kernel, stride, padding)\n",
    "        # Adjust these based on desired downsampling and output sizes\n",
    "        conv3d_configs = [\n",
    "            ((4, 4, 4), (4, 4, 4), (2, 2, 2)), # Stage 1: Aggressive downsampling\n",
    "            ((2, 2, 2), (2, 2, 2), (1, 1, 1)), # Stage 2\n",
    "            ((2, 2, 2), (2, 2, 2), (1, 1, 1)), # Stage 3\n",
    "            ((2, 2, 2), (2, 2, 2), (1, 1, 1)), # Stage 4\n",
    "        ]\n",
    "        assert len(conv3d_configs) == self.num_encoder_stages\n",
    "\n",
    "        for i in range(self.num_encoder_stages):\n",
    "            k, s, p = conv3d_configs[i]\n",
    "            out_dim = encoder_dims[i]\n",
    "\n",
    "            self.conv_stages.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv3d(in_dim, out_dim, kernel_size=k, stride=s, padding=p),\n",
    "                    nn.GroupNorm(out_dim // 8, out_dim), # Using GroupNorm for 3D\n",
    "                    nn.GELU(),\n",
    "                )\n",
    "            )\n",
    "            in_dim = out_dim\n",
    "\n",
    "            # Calculate and store output spatial size\n",
    "            current_spatial_size = [ calc_conv_out_size(sz, k_i, s_i, p_i) for sz, k_i, s_i, p_i in zip(current_spatial_size, k, s, p) ]\n",
    "            self._skip_3d_spatial_sizes.append(tuple(current_spatial_size))\n",
    "            # print(f\"3D Conv Stage {i} output size (D, H, W): {current_spatial_size}\")\n",
    "\n",
    "        # Final 3D feature map size after Conv stages\n",
    "        self.bottleneck_spatial_size_3d = self._skip_3d_spatial_sizes[-1]\n",
    "        # print(f\"Bottleneck 3D size: {self.bottleneck_spatial_size_3d}\")\n",
    "        D_b, H_b, W_b = self.bottleneck_spatial_size_3d\n",
    "\n",
    "\n",
    "        # --- Transformer Bottleneck ---\n",
    "        # Flatten the spatial and temporal dimensions of the bottleneck feature map\n",
    "        # Sequence length will be D_b * H_b * W_b\n",
    "        sequence_length = D_b * H_b * W_b\n",
    "        transformer_input_channels = encoder_dims[-1] # Channels from last 3D Conv stage\n",
    "\n",
    "        # Project channels to embed_dim if necessary\n",
    "        self.transformer_input_proj = nn.Linear(transformer_input_channels, embed_dim) if transformer_input_channels != embed_dim else nn.Identity()\n",
    "\n",
    "        # Positional Embedding (learnable)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, sequence_length, embed_dim))\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "        # Transformer Encoder Layers\n",
    "        norm_layer = nn.LayerNorm # Use LayerNorm for Transformer\n",
    "        act_layer = nn.GELU # Use GELU for Transformer\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=False, drop=0., attn_drop=0., drop_path=0., # Set dropout/droppath\n",
    "                norm_layer=norm_layer, act_layer=act_layer\n",
    "            ) for i in range(transformer_depth)])\n",
    "\n",
    "        self.transformer_norm = norm_layer(embed_dim)\n",
    "\n",
    "\n",
    "        # --- Projection from Transformer output to 2D Decoder input ---\n",
    "        # Transformer output shape: (B, sequence_length, embed_dim)\n",
    "        # Reshape to (B, embed_dim, D_b, H_b, W_b) and pool over time dimension D_b\n",
    "        # to get (B, embed_dim, H_b, W_b) for 2D Decoder input (bottleneck feature)\n",
    "        self.bottleneck_projection_3d_to_2d = nn.Sequential(\n",
    "             # Reshape is done in forward pass\n",
    "             # Average pool over the Time dimension (dim 2 after permute)\n",
    "             # nn.AdaptiveAvgPool3d((1, H_b, W_b)) # Or just mean over dim 2\n",
    "             # Squeeze time dimension (dim 2)\n",
    "        )\n",
    "        self.bottleneck_2d_channels = embed_dim # Output channels of the 2D bottleneck feature\n",
    "\n",
    "\n",
    "        # --- 2D Skip Connection Projection Layers ---\n",
    "        # Project 3D features from intermediate Conv stages to 2D features for skip connections\n",
    "        # Decoder needs skips from levels 1, 2, 3 (corresponding to encoder_dims[0], [1], [2])\n",
    "        # These skips need to match spatial size (H, W) and channel count required by the decoder blocks.\n",
    "        # The decoder_channels tuple (256, 128, 64, 32) implies 4 decoder blocks.\n",
    "        # Decoder block 0 takes bottleneck + skip from level 3.\n",
    "        # Decoder block 1 takes output of block 0 + skip from level 2.\n",
    "        # Decoder block 2 takes output of block 1 + skip from level 1.\n",
    "        # Decoder block 3 takes output of block 2 + no skip (or 0 channels skip).\n",
    "\n",
    "        # Channels expected for skips by the decoder (matching decoder_channels[1:])\n",
    "        # Target channels for skips: decoder_channels[1], decoder_channels[2], decoder_channels[3]\n",
    "        target_skip_2d_channels = list(decoder_channels[1:]) # [128, 64, 32]\n",
    "\n",
    "        self.skip_projection_layers = nn.ModuleList()\n",
    "        # Iterate through 3D Conv stages outputs for skip connections (excluding the last one)\n",
    "        # We need skips from stages 0, 1, 2 (corresponding to encoder_dims[0,1,2])\n",
    "        # The spatial sizes are self._skip_3d_spatial_sizes[0, 1, 2]\n",
    "        # Pair 3D skip info with target 2D decoder skip channels\n",
    "        # Skips for decoder are typically from finer to coarser features.\n",
    "        # Decoder Block 0 needs skip from Conv3D Stage 3 -> projected 2D\n",
    "        # Decoder Block 1 needs skip from Conv3D Stage 2 -> projected 2D\n",
    "        # Decoder Block 2 needs skip from Conv3D Stage 1 -> projected 2D\n",
    "        # This aligns with skip_features_3d [stage0, stage1, stage2] and target_skip_2d_channels [128, 64, 32] (reversed).\n",
    "\n",
    "        # Iterate through 3D Conv stages for skips (from coarsest to finest)\n",
    "        # Use encoder_dims[:-1] for input channels and _skip_3d_spatial_sizes[:-1] for sizes\n",
    "        # Use target_skip_2d_channels for output channels (reversed for decoder)\n",
    "        skip_3d_info = list(zip(encoder_dims[:-1], self._skip_3d_spatial_sizes[:-1])) # [(32, sz0), (64, sz1), (128, sz2)]\n",
    "\n",
    "        # Pair 3D skip info (reversed for decoder) with target 2D channels (reversed for decoder)\n",
    "        skip_pairs = list(zip(skip_3d_info[::-1], target_skip_2d_channels)) # [((128, sz2), 128), ((64, sz1), 64), ((32, sz0), 32)]\n",
    "\n",
    "        for (skip_c_3d, skip_sz_3d), target_2d_channels in skip_pairs:\n",
    "            self.skip_projection_layers.append(\n",
    "                 nn.Sequential(\n",
    "                     nn.AdaptiveAvgPool3d((1, skip_sz_3d[1], skip_sz_3d[2])), # Pool time D_i to 1 -> (B, C_in, 1, H_i, W_i)\n",
    "                     nn.Conv2d(skip_c_3d, target_2d_channels, kernel_size=1), # Project channels -> (B, C_out, H_i, W_i)\n",
    "                 )\n",
    "            )\n",
    "\n",
    "        # --- 2D Decoder Setup for UnetDecoder2d ---\n",
    "        # UnetDecoder2d expects:\n",
    "        # encoder_channels: tuple of channels from encoder stages, starting *before* the first skip used.\n",
    "        #   Effectively channels of the input list `feats`[::-1].\n",
    "        #   feats list will be [Bottleneck_2D, Projected_Skip_Level_1_2D, Projected_Skip_Level_2_2D, Projected_Skip_Level_3_2D]\n",
    "        #   Channels: [embed_dim, target_skip_2d_channels[2], target_skip_2d_channels[1], target_skip_2d_channels[0]]\n",
    "        #   Example: [embed_dim, 32, 64, 128]\n",
    "        decoder_enc_channels_for_init = [embed_dim] + target_skip_2d_channels[::-1]\n",
    "\n",
    "        # skip_channels: tuple of channels for skip connections, starting from the first skip used.\n",
    "        #   Effectively channels of `feats`[1:] + [0].\n",
    "        #   Channels: [target_skip_2d_channels[2], target_skip_2d_channels[1], target_skip_2d_channels[0], 0]\n",
    "        #   Example: [32, 64, 128, 0]\n",
    "        decoder_skip_channels_for_init = target_skip_2d_channels[::-1] + [0]\n",
    "\n",
    "\n",
    "        # Re-calculate decoder_channels if needed, ensure consistency with decoder blocks.\n",
    "        # The decoder_channels tuple passed to UnetDecoder2d defines the *output* channels of its blocks.\n",
    "        # The input channels to its blocks are derived from encoder_channels and skip_channels.\n",
    "        # Let's use the provided decoder_channels tuple directly.\n",
    "        # Ensure the lengths match: len(decoder_channels) == num_decoder_blocks.\n",
    "        # Number of decoder blocks seems to be len(decoder_channels).\n",
    "        # UnetDecoder2d internally creates len(decoder_channels) blocks.\n",
    "        # It needs len(decoder_channels) + 1 items in `feats` (1 bottleneck, len(decoder_channels) skips).\n",
    "        # This implies we need skips from all `num_encoder_stages` levels *before* the bottleneck.\n",
    "        # With 4 encoder stages, and last one is bottleneck, we need 3 skips.\n",
    "        # If decoder_channels has length 4, UnetDecoder2d wants 4 skips + 1 bottleneck = 5 items in feats.\n",
    "        # This means we need 4 skips from the encoder levels before the bottleneck.\n",
    "        # This implies the bottleneck should come from stage 5 if encoder had 5 stages.\n",
    "        # Or, UnetDecoder2d is designed for encoders with 1 more stage than decoder blocks.\n",
    "\n",
    "        # Let's align the model structure. If decoder_channels is length 4, we need 4 decoder blocks.\n",
    "        # Decoder needs 4 skips (from encoder stages 0, 1, 2, 3) + 1 bottleneck (from encoder stage 4).\n",
    "        # This means the last 3D Conv stage (encoder_dims[-1]) should be the source of the bottleneck.\n",
    "        # And encoder_dims[:-1] (stages 0, 1, 2) are sources for skips.\n",
    "        # We seem to be missing one skip connection needed by the decoder if decoder_channels has length 4.\n",
    "\n",
    "        # Let's assume the original decoder was meant to work with the 4 encoder stages provided in `encoder_dims`.\n",
    "        # This would mean the decoder receives 1 bottleneck feature and 3 skip features.\n",
    "        # UnetDecoder2d has `decoder_channels = (256, 128, 64, 32)`, length 4. It builds 4 blocks.\n",
    "        # It expects `encoder_channels` (inputs to concat) and `skip_channels` (the skips themselves).\n",
    "        # Its loop is `for i, b in enumerate(self.blocks): skip = feats[i] if i < len(feats) else None`\n",
    "        # If feats has [bottleneck, skip1, skip2, skip3], it uses skip1 for block 0, skip2 for block 1, skip3 for block 2, and None for block 3.\n",
    "        # This implies decoder_channels length should be len(feats) - 1 = 3 if we have 3 skips + 1 bottleneck.\n",
    "        # But decoder_channels has length 4.\n",
    "        # This means UnetDecoder2d with decoder_channels len 4 expects 4 skips + 1 bottleneck.\n",
    "\n",
    "        # Re-interpreting the original UnetDecoder2d and how it's used in the base code:\n",
    "        # base code: ecs = [_[\"num_chs\"] for _ in self.backbone.feature_info][::-1] # 4 channels from ConvNeXt stages reversed\n",
    "        # decoder = UnetDecoder2d(encoder_channels=ecs, ...)\n",
    "        # decoder_channels = (256, 128, 64, 32) # len 4\n",
    "        # UnetDecoder2d init: `in_channels= [encoder_channels[0]] + list(decoder_channels[:-1])`\n",
    "        # `skip_channels= list(encoder_channels[1:]) + [0]`\n",
    "        # This means UnetDecoder2d is initialized with 4 encoder_channels.\n",
    "        # ecs example: [1024, 512, 256, 128] from ConvNeXt.\n",
    "        # encoder_channels = (1024, 512, 256, 128)\n",
    "        # decoder_channels = (256, 128, 64, 32)\n",
    "        # in_channels_to_decoder_blocks = [1024] + [256, 128, 64] = [1024, 256, 128, 64]\n",
    "        # skip_channels_for_decoder_init = [512, 256, 128] + [0] = [512, 256, 128, 0]\n",
    "        # Decoder blocks loop: uses `zip(in_channels_to_blocks, skip_channels_for_init, decoder_channels)`\n",
    "        # (1024, 512, 256), (256, 256, 128), (128, 128, 64), (64, 0, 32)\n",
    "        # Block 0 input chans: 1024 (from prev block/bottleneck) + 512 (skip) -> concat -> 1024+512. Output 256.\n",
    "        # Block 1 input chans: 256 (from prev block) + 256 (skip) -> concat -> 512. Output 128.\n",
    "        # Block 2 input chans: 128 (from prev block) + 128 (skip) -> concat -> 256. Output 64.\n",
    "        # Block 3 input chans: 64 (from prev block) + 0 (skip) -> concat -> 64. Output 32.\n",
    "\n",
    "        # This structure requires `len(encoder_channels)` features as input `feats`, where feats[0] is bottleneck, feats[1:] are skips.\n",
    "        # It uses `encoder_channels` for the *first* channel in the concatenation input to the blocks, and `skip_channels` for the *second*.\n",
    "        # This is confusing. Let's assume the standard U-Net decoder input structure:\n",
    "        # Feats list: [bottleneck_feature, skip_feature_level1, skip_feature_level2, ..., skip_feature_levelN]\n",
    "        # Decoder has N blocks.\n",
    "        # Block 0 input: concat(bottleneck, skip_feature_level1)\n",
    "        # Block 1 input: concat(Output_Block0, skip_feature_level2)\n",
    "        # ...\n",
    "        # Block N-1 input: concat(Output_BlockN-2, skip_feature_levelN)\n",
    "\n",
    "        # Re-aligning `UnetDecoder2d` to a more standard U-Net implementation structure.\n",
    "        # If `decoder_channels` has length N, there are N decoder blocks.\n",
    "        # Need 1 bottleneck feature and N skip features.\n",
    "        # So `feats` list should have length N+1.\n",
    "        # `encoder_channels` should be the list of channels of `feats`[::-1].\n",
    "        # `skip_channels` should be the list of channels of `feats`[1:] + [0].\n",
    "\n",
    "        # Let's match `decoder_channels` length (4) with the number of encoder stages (4).\n",
    "        # This implies encoder stage 3 (idx 3) is bottleneck, and stages 0, 1, 2 are skips.\n",
    "        # We need 4 skips if decoder has 4 blocks. This contradicts the 4 encoder stages -> 3 skips + 1 bottleneck idea.\n",
    "        # Perhaps the decoder is meant to work with the 4 ConvNeXt stages, and takes 4 features (bottleneck + 3 skips).\n",
    "        # If `decoder_channels` length is 4, and UnetDecoder2d builds 4 blocks, it needs 4 skips and 1 bottleneck.\n",
    "        # This requires 5 levels of features from the encoder.\n",
    "\n",
    "        # Let's assume the decoder_channels length defines the number of upsampling steps / resolution levels in the decoder.\n",
    "        # If `decoder_channels` is (256, 128, 64, 32) -> 4 levels.\n",
    "        # Decoder goes from lowest res up to highest res.\n",
    "        # Low res input comes from bottleneck. Intermediate res inputs come from skips.\n",
    "        # Upsampling factor needed: Initial H, W (70, 70) -> Final Bottleneck H, W (~4, 4) -> Target H, W (70, 70).\n",
    "        # Total spatial downsample factor in encoder: ~17.5 (70->4).\n",
    "        # Decoder needs total upsample factor ~17.5. 4 blocks with scale 2 gives 16.\n",
    "\n",
    "        # Let's assume the provided `decoder_channels` (len 4) and the number of encoder stages (len 4) are designed to match, but maybe in a slightly non-standard U-Net way as per the original `UnetDecoder2d`.\n",
    "\n",
    "        # Based on the original `UnetDecoder2d` usage:\n",
    "        # `encoder_channels=ecs` (reversed ConvNeXt stages, len 4)\n",
    "        # `skip_channels=None` initially in base code, then calculated as `list(encoder_channels[1:]) + [0]`\n",
    "        # `decoder_channels=(256, 128, 64, 32)` (len 4)\n",
    "        # This implies the decoder takes 4 input features (from encoder stages) and processes them through 4 blocks.\n",
    "\n",
    "        # Adapting this to our Hybrid model:\n",
    "        # Our 3D Conv stages provide features at 4 levels (encoder_dims).\n",
    "        # We need to convert these 4 levels of 3D features into 4 levels of 2D features for the decoder.\n",
    "        # Level 4 (finest 3D): embed_dim channels, (D_b, H_b, W_b) size. -> Pool time -> (embed_dim, H_b, W_b) 2D bottleneck.\n",
    "        # Level 3 (3D): encoder_dims[2] channels, size (_skip_3d_spatial_sizes[2]). -> Project to 2D (target chan), Pool time -> (target chan, H_3, W_3) 2D skip.\n",
    "        # Level 2 (3D): encoder_dims[1] channels, size (_skip_3d_spatial_sizes[1]). -> Project to 2D (target chan), Pool time -> (target chan, H_2, W_2) 2D skip.\n",
    "        # Level 1 (3D): encoder_dims[0] channels, size (_skip_3d_spatial_sizes[0]). -> Project to 2D (target chan), Pool time -> (target chan, H_1, W_1) 2D skip.\n",
    "\n",
    "        # We need to match the channels and spatial sizes for these 4 levels of 2D features to what UnetDecoder2d expects.\n",
    "        # The original base code uses ConvNeXt features reversed. Let's assume our 3D Conv + Transformer features correspond to these.\n",
    "        # If original ConvNeXt features (reversed) are [1024, 512, 256, 128].\n",
    "        # Our features should mimic this relative scaling if possible.\n",
    "        # Our levels: Bottleneck_2D (embed_dim=768), Projected Skip 3 (?), Skip 2 (?), Skip 1 (?)\n",
    "\n",
    "        # Let's make the projected skip channels match the decoder_channels inputs roughly.\n",
    "        # If decoder_channels = (256, 128, 64, 32) are output channels of blocks.\n",
    "        # Input channels to blocks are sum of prev output + skip.\n",
    "        # Let's simplify and assume UnetDecoder2d expects 4 input features, and we need to project our 4 levels to match expected channels.\n",
    "        # Let's try to match the number of channels in `encoder_channels` tuple passed to UnetDecoder2d.\n",
    "        # Original `ecs` had length 4. So `encoder_channels` param has length 4.\n",
    "        # Let's define the channels for our 4 levels of 2D features:\n",
    "        # Level 4 (bottleneck): embed_dim\n",
    "        # Level 3: encoder_dims[2] -> project to D1=256?\n",
    "        # Level 2: encoder_dims[1] -> project to D2=128?\n",
    "        # Level 1: encoder_dims[0] -> project to D3=64?\n",
    "\n",
    "        # Let's redefine the skip projection target channels to match the decoder_channels tuple directly.\n",
    "        # Skip projection target channels: decoder_channels[0], decoder_channels[1], decoder_channels[2]\n",
    "        # Skip from Stage 3 (encoder_dims[2]): project to decoder_channels[0] (256)\n",
    "        # Skip from Stage 2 (encoder_dims[1]): project to decoder_channels[1] (128)\n",
    "        # Skip from Stage 1 (encoder_dims[0]): project to decoder_channels[2] (64)\n",
    "\n",
    "        target_skip_2d_channels_for_proj = list(decoder_channels[:-1]) # [256, 128, 64]\n",
    "\n",
    "        self.skip_projection_layers = nn.ModuleList()\n",
    "        # Iterate through 3D Conv stages 0, 1, 2 for skips\n",
    "        # Use encoder_dims[:-1] for input channels and _skip_3d_spatial_sizes[:-1] for sizes\n",
    "        skip_3d_info = list(zip(encoder_dims[:-1], self._skip_3d_spatial_sizes[:-1])) # [(32, sz0), (64, sz1), (128, sz2)]\n",
    "\n",
    "        # Pair 3D skip info with target 2D channels for projection\n",
    "        skip_proj_pairs = list(zip(skip_3d_info, target_skip_2d_channels_for_proj)) # [((32, sz0), 256), ((64, sz1), 128), ((128, sz2), 64)]\n",
    "\n",
    "        for (skip_c_3d, skip_sz_3d), target_2d_channels in skip_proj_pairs:\n",
    "            self.skip_projection_layers.append(\n",
    "                 nn.Sequential(\n",
    "                     nn.AdaptiveAvgPool3d((1, skip_sz_3d[1], skip_sz_3d[2])), # Pool time D_i to 1 -> (B, C_in, 1, H_i, W_i)\n",
    "                     nn.Conv2d(skip_c_3d, target_2d_channels, kernel_size=1), # Project channels -> (B, C_out, H_i, W_i)\n",
    "                 )\n",
    "            )\n",
    "\n",
    "        # Now define the 4 features feeding the decoder.\n",
    "        # Feats list: [Bottleneck_2D, Projected_Skip_Level_3_2D, Projected_Skip_Level_2_2D, Projected_Skip_Level_1_2D]\n",
    "        # Channels: [embed_dim, target_skip_2d_channels_for_proj[0], target_skip_2d_channels_for_proj[1], target_skip_2d_channels_for_proj[2]]\n",
    "        # Example: [embed_dim, 256, 128, 64]\n",
    "\n",
    "        # Define encoder_channels and skip_channels for UnetDecoder2d init:\n",
    "        # encoder_channels (inputs to concat, from feats[::-1]): [target_skip_2d_channels_for_proj[2], target_skip_2d_channels_for_proj[1], target_skip_2d_channels_for_proj[0], embed_dim]\n",
    "        # Example: [64, 128, 256, embed_dim]\n",
    "        decoder_enc_channels_for_init = target_skip_2d_channels_for_proj[::-1] + [embed_dim]\n",
    "\n",
    "        # skip_channels (the skips themselves, from feats[1:] + [0]): [target_skip_2d_channels_for_proj[0], target_skip_2d_channels_for_proj[1], target_skip_2d_channels_for_proj[2], 0]\n",
    "        # Example: [256, 128, 64, 0]\n",
    "        decoder_skip_channels_for_init = target_skip_2d_channels_for_proj + [0]\n",
    "\n",
    "        # print(\"UnetDecoder2d encoder_channels (init):\", decoder_enc_channels_for_init)\n",
    "        # print(\"UnetDecoder2d skip_channels (init):\", decoder_skip_channels_for_init)\n",
    "        # print(\"UnetDecoder2d decoder_channels (init):\", decoder_channels)\n",
    "\n",
    "        # Determine scale factors for the decoder\n",
    "        # Decoder goes from bottleneck spatial size (H_b, W_b) to target (70, 70).\n",
    "        # Number of decoder blocks = len(decoder_channels) = 4.\n",
    "        # Total spatial upsample factor needed = (70 / H_b, 70 / W_b).\n",
    "        # Example: (70 / 4, 70 / 4) = (17.5, 17.5) if H_b, W_b = 4.\n",
    "        # Average upsample factor per block = (17.5)**(1/4) ~ 2.04\n",
    "        # Use float scale factors in MONAI UpSample\n",
    "\n",
    "        H_b, W_b = self.bottleneck_spatial_size_3d[1:] # H, W from the bottleneck 3D features\n",
    "        num_decoder_blocks = len(decoder_channels)\n",
    "        final_decoder_output_H = H_b * (2**num_decoder_blocks) # Assuming scale_factor=2 for all blocks\n",
    "        final_decoder_output_W = W_b * (2**num_decoder_blocks)\n",
    "        # Example: 4 * (2^4) = 4 * 16 = 64. So from (4,4) to (64,64) with scale 2.\n",
    "\n",
    "        # We need to reach (70, 70).\n",
    "        # Option 1: Calculate scale factors to directly reach 70. (70/4)**(1/4) for each step?\n",
    "        # Scale factor for block i should upsample from spatial_i to spatial_i-1.\n",
    "        # E.g., H_b -> H_b*s_1 -> H_b*s_1*s_2 ... -> 70.\n",
    "        # Total scale = Product(s_i) = 70 / H_b.\n",
    "        # If s_i are all equal, s = (70 / H_b)**(1/num_decoder_blocks)\n",
    "        # Let's calculate scale factors for each stage to reach 70x70 from bottleneck H_b, W_b\n",
    "        # The spatial size after N Conv3D stages is _skip_3d_spatial_sizes[-1]. Let's call this (D_last, H_last, W_last).\n",
    "        # Decoder input spatial size is (H_last, W_last).\n",
    "        # Decoder output size should be (70, 70).\n",
    "        # Number of decoder blocks is len(decoder_channels).\n",
    "        # Total spatial upsampling factor required is (70/H_last, 70/W_last).\n",
    "        # Let's assume the decoder upsamples by a factor of 2 in each block for simplicity and match the original code.\n",
    "        # This results in (H_last * 2^N, W_last * 2^N) spatial size after the decoder.\n",
    "        # We need to use interpolation in the final head to reach exactly (70, 70).\n",
    "\n",
    "        # Decoder spatial size sequence (if starting from H_last, W_last with scale 2 per block):\n",
    "        # (H_last, W_last) -> (H_last*2, W_last*2) -> (H_last*4, W_last*4) -> (H_last*8, W_last*8) -> (H_last*16, W_last*16)\n",
    "        # Example: (4, 4) -> (8, 8) -> (16, 16) -> (32, 32) -> (64, 64)\n",
    "\n",
    "        decoder_scale_factors = [2] * num_decoder_blocks # Use integer scale factors for decoder blocks\n",
    "\n",
    "        self.decoder = UnetDecoder2d(\n",
    "            encoder_channels=decoder_enc_channels_for_init,\n",
    "            skip_channels=decoder_skip_channels_for_init,\n",
    "            decoder_channels=decoder_channels,\n",
    "            scale_factors=decoder_scale_factors,\n",
    "            norm_layer=monai.networks.blocks.Norm(\"INSTANCE\", spatial_dims=2), # Use MONAI Norm wrapper\n",
    "            attention_type=decoder_attention_type,\n",
    "            upsample_mode=upsample_mode,\n",
    "        )\n",
    "        # Need to adjust norm_layer definition for UnetDecoder2d\n",
    "\n",
    "        # --- Segmentation Head ---\n",
    "        # Decoder last output shape: (B, decoder_channels[-1], H_dec_out, W_dec_out)\n",
    "        # H_dec_out, W_dec_out = (self.bottleneck_spatial_size_3d[1] * (2**num_decoder_blocks),\n",
    "        #                         self.bottleneck_spatial_size_3d[2] * (2**num_decoder_blocks))\n",
    "        # Example: (4 * 16, 4 * 16) = (64, 64)\n",
    "\n",
    "        # Target shape: (B, 1, 70, 70)\n",
    "        # Need to go from (H_dec_out, W_dec_out) to (70, 70) and change channels.\n",
    "        final_upsample_scale = (70 / final_decoder_output_H,\n",
    "                                70 / final_decoder_output_W)\n",
    "        # Example: (70/64, 70/64) = (1.09375, 1.09375)\n",
    "\n",
    "        self.seg_head = SegmentationHead2d(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=1,\n",
    "            scale_factor=final_upsample_scale, # Use calculated float scale for interpolation\n",
    "            kernel_size=3,\n",
    "            mode=\"nontrainable\", # Use interpolation mode\n",
    "        )\n",
    "\n",
    "        # Final activation/scaling - handled in forward pass for test-time aug\n",
    "\n",
    "\n",
    "        # Initialize weights (optional, but good practice)\n",
    "        self._init_weights()\n",
    "\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv3d, nn.Conv2d)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, (nn.LayerNorm, nn.GroupNorm, nn.InstanceNorm2d)): # Added InstanceNorm2d\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "        # Initialize positional embedding\n",
    "        if hasattr(self, 'pos_embed') and self.pos_embed is not None:\n",
    "             trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: (B, 1, 1000, 70, 70)\n",
    "\n",
    "        if not self.training:\n",
    "             # Handle test-time augmentation by averaging predictions\n",
    "             pred_normal_scaled = self._forward_unscaled(x).squeeze(1) * 1500 + 3000 # (B, 70, 70)\n",
    "\n",
    "             # Flip Time (dim 2) and W (dim 4) of input (B, 1, T, H, W)\n",
    "             x_flipped = torch.flip(x, dims=[2, 4])\n",
    "             pred_flipped_raw = self._forward_unscaled(x_flipped).squeeze(1) # (B, 70, 70)\n",
    "             # Flip W (dim 2) of prediction (B, H, W), then scale\n",
    "             pred_flipped_scaled = torch.flip(pred_flipped_raw, dims=[2]) * 1500 + 3000\n",
    "\n",
    "             # Average scaled predictions\n",
    "             averaged_pred_scaled = torch.mean(torch.stack([pred_normal_scaled, pred_flipped_scaled], dim=0), dim=0) # (B, 70, 70)\n",
    "             return averaged_pred_scaled\n",
    "        else:\n",
    "            # Training mode: just run forward pass and return unscaled output\n",
    "            return self._forward_unscaled(x).squeeze(1) # (B, 70, 70) unscaled\n",
    "\n",
    "\n",
    "    def _forward_unscaled(self, x):\n",
    "        # Helper method to run the core forward pass without final scaling/TTA\n",
    "\n",
    "        # --- 3D Conv Encoder ---\n",
    "        skip_features_3d = []\n",
    "        x_3d = x\n",
    "        for i, stage in enumerate(self.conv_stages):\n",
    "            x_3d = stage(x_3d)\n",
    "            # print(f\"3D Conv Stage {i} out shape: {x_3d.shape}\")\n",
    "            if i < self.num_encoder_stages - 1: # Save skips from all but the last stage\n",
    "                skip_features_3d.append(x_3d)\n",
    "\n",
    "        # x_3d is now the bottleneck 3D feature map (B, C_last, D_b, H_b, W_b)\n",
    "\n",
    "\n",
    "        # --- Transformer Bottleneck ---\n",
    "        B, C_last, D_b, H_b, W_b = x_3d.shape\n",
    "        # Flatten time and spatial dimensions for Transformer input\n",
    "        # Permute to (B, D_b, H_b, W_b, C_last) then flatten spatial/temporal\n",
    "        x_flat = x_3d.permute(0, 2, 3, 4, 1).contiguous().view(B, -1, C_last) # (B, D_b*H_b*W_b, C_last)\n",
    "        # print(\"Flattened shape before proj:\", x_flat.shape)\n",
    "\n",
    "        # Project channels to embed_dim\n",
    "        x_flat = self.transformer_input_proj(x_flat) # (B, sequence_length, embed_dim)\n",
    "        # print(\"Flattened shape after proj:\", x_flat.shape)\n",
    "\n",
    "\n",
    "        # Add positional embedding\n",
    "        # Ensure sequence length matches pos_embed\n",
    "        if x_flat.shape[1] != self.pos_embed.shape[1]:\n",
    "             print(f\"Warning: Positional embedding sequence length mismatch. Expected {self.pos_embed.shape[1]}, got {x_flat.shape[1]}.\")\n",
    "             # Handle mismatch, e.g., resize pos_embed or reinitialize\n",
    "             # For now, let's assume the calculated sequence_length is correct\n",
    "             # If input size varies, pos_embed needs to be applied differently (e.g., interpolation)\n",
    "             # Assuming fixed input spatial_size allows fixed pos_embed.\n",
    "             pass # pos_embed is already defined based on calculated sequence_length\n",
    "\n",
    "        x_flat = x_flat + self.pos_embed # Add positional embedding\n",
    "\n",
    "        # Apply Transformer blocks\n",
    "        for i, block in enumerate(self.transformer_blocks):\n",
    "            x_flat = block(x_flat)\n",
    "            # print(f\"Transformer Block {i} out shape: {x_flat.shape}\")\n",
    "\n",
    "        # Apply final norm\n",
    "        x_flat = self.transformer_norm(x_flat) # (B, sequence_length, embed_dim)\n",
    "\n",
    "\n",
    "        # --- Project Transformer output to 2D and pool time ---\n",
    "        # Reshape back to 3D-like structure: (B, embed_dim, D_b, H_b, W_b)\n",
    "        sequence_length_check = D_b * H_b * W_b\n",
    "        if x_flat.shape[1] != sequence_length_check:\n",
    "             print(f\"Error: Transformer output sequence length {x_flat.shape[1]} does not match expected {sequence_length_check} based on bottleneck size {self.bottleneck_spatial_size_3d}\")\n",
    "             # This indicates an error in calculation or mismatch between transformer and conv output\n",
    "             raise ValueError(\"Transformer sequence length mismatch\")\n",
    "\n",
    "        # Reshape from (B, sequence_length, embed_dim) to (B, embed_dim, D_b, H_b, W_b)\n",
    "        x_3d_like = x_flat.view(B, D_b, H_b, W_b, self.embed_dim).permute(0, 4, 1, 2, 3).contiguous()\n",
    "        # print(\"Reshaped 3D-like shape:\", x_3d_like.shape)\n",
    "\n",
    "        # Pool over the time dimension (D_b) to get 2D feature map\n",
    "        # Use mean pooling over the time dimension (dim 2)\n",
    "        x_2d_bottleneck = torch.mean(x_3d_like, dim=2).squeeze(2) # -> (B, embed_dim, H_b, W_b)\n",
    "        # print(\"2D Bottleneck shape after pooling:\", x_2d_bottleneck.shape)\n",
    "\n",
    "\n",
    "        # --- Prepare 2D Skip Connections ---\n",
    "        # Process the saved 3D skip features through projection layers\n",
    "        # skip_features_3d contains [stage0, stage1, stage2] 3D features\n",
    "        # skip_projection_layers processes skips from stage 0, 1, 2 in order.\n",
    "        skip_features_2d = []\n",
    "        # Iterate through 3D Conv stages 0, 1, 2\n",
    "        for i in range(self.num_encoder_stages - 1):\n",
    "            skip_3d = skip_features_3d[i] # Get 3D feature from stage i\n",
    "            skip_2d = self.skip_projection_layers[i](skip_3d) # Project to 2D using corresponding layer\n",
    "            skip_features_2d.append(skip_2d)\n",
    "            # print(f\"Projected Skip {i} (from stage {i}) shape: {skip_2d.shape}\")\n",
    "\n",
    "\n",
    "        # Combine bottleneck and skip features for the Decoder input\n",
    "        # Feats list for UnetDecoder2d: [bottleneck_2d, skip_level_1_2d, skip_level_2_2d, skip_level_3_2d]\n",
    "        # where skip_level_1_2d comes from Conv3D Stage 0 (coarsest spatial skip)\n",
    "        # skip_level_2_2d from Conv3D Stage 1\n",
    "        # skip_level_3_2d from Conv3D Stage 2\n",
    "        # The order in skip_features_2d is [stage0, stage1, stage2]. This is the correct order for the feats list after the bottleneck.\n",
    "        decoder_feats = [x_2d_bottleneck] + skip_features_2d\n",
    "        # print(\"Decoder feats list shapes:\", [f.shape for f in decoder_feats])\n",
    "\n",
    "        # --- 2D Decoder ---\n",
    "        # UnetDecoder2d expects feats[::-1] as input to its internal processing loop.\n",
    "        # But the loop logic `skip = feats[i]` or `skip = feats[i+1]` depends on implementation.\n",
    "        # Looking at the provided UnetDecoder2d code, the loop is `for i, b in enumerate(self.blocks): skip= feats[i] if i < len(feats) else None`.\n",
    "        # This means feats[0] is skip for block 0, feats[1] for block 1, etc.\n",
    "        # This is NOT the standard U-Net structure (bottleneck, then skips).\n",
    "        # The standard structure feeds the bottleneck as the *first* input to the *first* decoder block, and then uses skips.\n",
    "        # Re-reading the base code's UnetDecoder2d forward again:\n",
    "        # `res= [feats[0]]` -> res[0] is the first element of input `feats`.\n",
    "        # `feats= feats[1:]` -> remaining elements are skips.\n",
    "        # `for i, b in enumerate(self.blocks): skip= feats[i] if i < len(feats) else None; res.append(b(res[-1], skip=skip),)`\n",
    "        # Block 0 input: res[-1] (feats[0]), skip=feats[0] (from the `feats = feats[1:]` line). So concat(feats[0], feats[0]). This looks wrong.\n",
    "\n",
    "        # Let's assume the original UnetDecoder2d is meant to work with a list of features `feats` where `feats[0]` is the bottleneck\n",
    "        # and `feats[1:]` are the skip connections (ordered from coarsest spatial to finest spatial).\n",
    "        # And the decoder blocks process these in reverse order (finest spatial skip first).\n",
    "        # So `feats` list should be [Bottleneck_2D, Skip1_2D, Skip2_2D, Skip3_2D] where Skip1 is coarsest.\n",
    "        # Our `decoder_feats` list is [x_2d_bottleneck, skip_features_2d[0], skip_features_2d[1], skip_features_2d[2]].\n",
    "        # where skip_features_2d[0] is from stage 0 (coarsest), [1] from stage 1, [2] from stage 2.\n",
    "        # This `decoder_feats` list IS in the correct order for a standard U-Net decoder expecting [bottleneck, skip_coarsest, ..., skip_finest].\n",
    "\n",
    "        # Re-reading the original UnetDecoder2d forward again. It reverses the input `feats` list: `res= [feats[0]]; feats= feats[1:]`.\n",
    "        # `for i, b in enumerate(self.blocks): skip= feats[i] if i < len(feats) else None; res.append(b(res[-1], skip=skip),)`\n",
    "        # If `feats_in` = [B, S1, S2, S3] (B=bottleneck, S1=coarsest skip, S2, S3=finer skips)\n",
    "        # Inside decoder: `res = [B]`; `feats = [S1, S2, S3]`\n",
    "        # Block 0: skip=feats[0]=S1. Input to block 0 is concat(B, S1). Output added to res.\n",
    "        # Block 1: skip=feats[1]=S2. Input to block 1 is concat(res[-1], S2). Output added to res.\n",
    "        # Block 2: skip=feats[2]=S3. Input to block 2 is concat(res[-1], S3). Output added to res.\n",
    "        # Block 3: skip=feats[3] is out of bounds (len(feats)=3). skip=None. Input to block 3 is res[-1].\n",
    "\n",
    "        # This implies the UnetDecoder2d takes `feats` list as [bottleneck, skip_coarsest, skip_mid, skip_finest, ...]\n",
    "        # And processes them from bottleneck up, using corresponding skips.\n",
    "        # The number of blocks in UnetDecoder2d (len(decoder_channels)) must match the number of skips provided in `feats` list.\n",
    "        # len(decoder_channels) = 4. We need 4 skips + 1 bottleneck -> 5 items in `feats`.\n",
    "        # Our encoder stages give 3 skips + 1 bottleneck = 4 items.\n",
    "        # There is a mismatch in the number of features expected by the original UnetDecoder2d (with decoder_channels len 4) and the features provided by our 4-stage encoder.\n",
    "\n",
    "        # Let's assume the original decoder_channels implies 3 decoder blocks + a final upsample, or some other structure.\n",
    "        # If decoder_channels=(256, 128, 64, 32) are output channels of 4 blocks.\n",
    "        # Let's try to match the input/skip channel logic of UnetDecoder2d init again.\n",
    "        # `encoder_channels=decoder_enc_channels_for_init` (length 4)\n",
    "        # `skip_channels=decoder_skip_channels_for_init` (length 4)\n",
    "        # `decoder_channels=decoder_channels` (length 4)\n",
    "        # UnetDecoder2d uses `zip(in_channels, skip_channels, decoder_channels)` which requires all three lists to have the same length.\n",
    "        # `in_channels` internal list is `[encoder_channels[0]] + list(decoder_channels[:-1])`. Length 1 + (4-1) = 4. OK.\n",
    "        # So UnetDecoder2d init works with lists of length 4.\n",
    "\n",
    "        # The forward pass issue with `feats` list length remains.\n",
    "        # If decoder_channels len is N, UnetDecoder2d makes N blocks.\n",
    "        # The forward loop `for i, b in enumerate(self.blocks): skip = feats[i] if i < len(feats) else None`\n",
    "        # expects `feats` list to have at least N elements for skips, plus the first element which is the bottleneck.\n",
    "        # So `feats` length should be N+1.\n",
    "        # Our encoder provides 4 feature levels (1 bottleneck, 3 skips). Total 4 features.\n",
    "        # If N=4 (decoder_channels len), we need feats length 5.\n",
    "        # If N=3 (decoder_channels len 3), we need feats length 4.\n",
    "\n",
    "        # Let's adjust the decoder_channels to match the number of skips + 1 bottleneck.\n",
    "        # We have 3 skips (from 3D stages 0, 1, 2) and 1 bottleneck (from stage 3). Total 4 feature levels.\n",
    "        # Let's define the decoder with 3 blocks, using skips from stages 0, 1, 2.\n",
    "        # Bottleneck from stage 3. Skips from stages 0, 1, 2.\n",
    "        # Feats list: [Bottleneck_2D (from stage 3), Skip1_2D (from stage 0), Skip2_2D (from stage 1), Skip3_2D (from stage 2)]. Length 4.\n",
    "        # Number of decoder blocks should be 3 (using 3 skips).\n",
    "        # Let decoder_channels be (128, 64, 32) - length 3.\n",
    "        # Target skip 2D channels for projection: decoder_channels[0], decoder_channels[1], decoder_channels[2]. -> (128, 64, 32).\n",
    "\n",
    "        # Skip projection layers will project 3D stages 0, 1, 2 to 2D with channels 128, 64, 32 respectively.\n",
    "        # skip_3d_info = [(32, sz0), (64, sz1), (128, sz2)]\n",
    "        # target_skip_2d_channels_for_proj = [128, 64, 32]\n",
    "        # skip_proj_pairs = [((32, sz0), 128), ((64, sz1), 64), ((128, sz2), 32)] -> Redo projection layers with these targets.\n",
    "\n",
    "        # Decoder init:\n",
    "        # encoder_channels (from feats[::-1]): [target_skip_2d_channels_for_proj[2], target_skip_2d_channels_for_proj[1], target_skip_2d_channels_for_proj[0], embed_dim]\n",
    "        # Example: [32, 64, 128, embed_dim]. Length 4.\n",
    "        # skip_channels (from feats[1:] + [0]): [target_skip_2d_channels_for_proj[0], target_skip_2d_channels_for_proj[1], target_skip_2d_channels_for_proj[2], 0]\n",
    "        # Example: [128, 64, 32, 0]. Length 4.\n",
    "        # decoder_channels = (128, 64, 32). Length 3.\n",
    "\n",
    "        # Still mismatch in lengths passed to UnetDecoder2d init (encoder_channels, skip_channels len 4, decoder_channels len 3).\n",
    "        # The original UnetDecoder2d seems to use `encoder_channels` and `skip_channels` params with len = len(decoder_channels) + 1.\n",
    "        # And the internal `in_channels`, `skip_channels` lists derived from these also have len = len(decoder_channels).\n",
    "\n",
    "        # Let's go back to the original base code's use of UnetDecoder2d.\n",
    "        # `ecs` (reversed ConvNeXt feats): length 4. `encoder_channels` param = ecs (len 4).\n",
    "        # `skip_channels` param is derived from `ecs[1:]` -> length 3. Then `+ [0]` makes it length 4. `skip_channels` param has length 4.\n",
    "        # `decoder_channels`: length 4.\n",
    "        # All input lists to UnetDecoder2d init have length 4.\n",
    "        # UnetDecoder2d creates 4 blocks.\n",
    "\n",
    "        # This structure implies: `encoder_channels` param provides channel counts for `feats[::-1]`\n",
    "        # `skip_channels` param provides channel counts for `feats[1:] + [0]`\n",
    "        # If encoder_channels param is length N, skip_channels param is length N, decoder_channels param is length N.\n",
    "        # UnetDecoder2d creates N blocks.\n",
    "        # It needs `feats` list of length N.\n",
    "        # `feats` list is [B, S1, ..., SN-1]. B is bottleneck, S1..SN-1 are N-1 skips.\n",
    "        # With N=4: `feats` length 4. [B, S1, S2, S3]. 1 bottleneck, 3 skips.\n",
    "        # encoder_channels param (len 4): [S3_chan, S2_chan, S1_chan, B_chan]\n",
    "        # skip_channels param (len 4): [S1_chan, S2_chan, S3_chan, 0]\n",
    "        # decoder_channels param (len 4): output channels [D1, D2, D3, D4]\n",
    "\n",
    "        # So we need 4 levels of 2D features from our encoder: 1 bottleneck, 3 skips.\n",
    "        # Bottleneck from 3D Stage 3 (embed_dim)\n",
    "        # Skip from 3D Stage 2 (encoder_dims[2]) -> project to 2D\n",
    "        # Skip from 3D Stage 1 (encoder_dims[1]) -> project to 2D\n",
    "        # Skip from 3D Stage 0 (encoder_dims[0]) -> project to 2D\n",
    "\n",
    "        # Let's redefine the target 2D channels for the skips based on matching `encoder_channels` param list structure.\n",
    "        # `encoder_channels` param list: [Skip3_2D_chan, Skip2_2D_chan, Skip1_2D_chan, Bottleneck_2D_chan]\n",
    "        # Let's set these channels to match decoder_channels[::-1]: [32, 64, 128, 256].\n",
    "        # So, Bottleneck_2D chan = 256. (Need to project embed_dim to 256).\n",
    "        # Skip3_2D chan = 32. (Project encoder_dims[2]=128 to 32).\n",
    "        # Skip2_2D chan = 64. (Project encoder_dims[1]=64 to 64).\n",
    "        # Skip1_2D chan = 128. (Project encoder_dims[0]=32 to 128).\n",
    "\n",
    "        # Update bottleneck projection: embed_dim -> 256\n",
    "        self.bottleneck_2d_proj = nn.Conv2d(embed_dim, decoder_channels[-1], kernel_size=1) # Project embed_dim to 256\n",
    "\n",
    "        # Update skip projection target channels:\n",
    "        # Skip from Stage 2 (encoder_dims[2]=128): target 32\n",
    "        # Skip from Stage 1 (encoder_dims[1]=64): target 64\n",
    "        # Skip from Stage 0 (encoder_dims[0]=32): target 128\n",
    "\n",
    "        target_skip_2d_channels_for_proj = [decoder_channels[-1], decoder_channels[-2], decoder_channels[-3]] # [32, 64, 128] ? No, match order in encoder_channels param.\n",
    "        # encoder_channels param order is [Skip3_2D_chan, Skip2_2D_chan, Skip1_2D_chan, Bottleneck_2D_chan]\n",
    "        # Let's set:\n",
    "        # Bottleneck_2D chan = decoder_channels[-1] = 32. (Project embed_dim to 32).\n",
    "        # Skip3_2D chan = decoder_channels[-2] = 64. (Project encoder_dims[2]=128 to 64).\n",
    "        # Skip2_2D chan = decoder_channels[-3] = 128. (Project encoder_dims[1]=64 to 128).\n",
    "        # Skip1_2D chan = decoder_channels[-4] = 256. (Project encoder_dims[0]=32 to 256).\n",
    "\n",
    "        self.bottleneck_2d_proj = nn.Conv2d(embed_dim, decoder_channels[-1], kernel_size=1) # Project embed_dim to 32\n",
    "\n",
    "        # Skip projection layers (for stages 0, 1, 2):\n",
    "        # Skip from Stage 0 (32) -> target 256\n",
    "        # Skip from Stage 1 (64) -> target 128\n",
    "        # Skip from Stage 2 (128) -> target 64\n",
    "\n",
    "        target_skip_2d_channels_for_proj = list(decoder_channels[:-1])[::-1] # [64, 128, 256]\n",
    "        skip_3d_info_for_proj = list(zip(encoder_dims[:-1], self._skip_3d_spatial_sizes[:-1])) # [(32, sz0), (64, sz1), (128, sz2)]\n",
    "        skip_proj_pairs = list(zip(skip_3d_info_for_proj, target_skip_2d_channels_for_proj)) # [((32, sz0), 64), ((64, sz1), 128), ((128, sz2), 256)] # Order mismatch\n",
    "\n",
    "\n",
    "        # Let's match the *input* channels to the UnetDecoder2d's internal blocks.\n",
    "        # UnetDecoder2d block i input concat channels: `in_channels[i]` and `skip_channels[i]`\n",
    "        # `in_channels` internal: `[encoder_channels[0]] + list(decoder_channels[:-1])`\n",
    "        # `skip_channels` internal: `list(encoder_channels[1:]) + [0]`\n",
    "\n",
    "        # If decoder_channels len is 4, UnetDecoder2d expects encoder_channels param len 4.\n",
    "        # Let's set `encoder_channels` param list: [Chan_Input_Block0, Chan_Input_Block1_Skip, Chan_Input_Block2_Skip, Chan_Input_Block3_Skip]\n",
    "        # Where Chan_Input_Block0 is the channel of the bottleneck feature.\n",
    "        # And Chan_Input_Block i_Skip is the channel of the skip feature used in Block i.\n",
    "        # Let's set `encoder_channels` param = (embed_dim, Chan_Skip3_2D, Chan_Skip2_2D, Chan_Skip1_2D)\n",
    "        # And `skip_channels` param = (Chan_Skip3_2D, Chan_Skip2_2D, Chan_Skip1_2D, 0)\n",
    "        # Where Skip3 is from stage 2, Skip2 from stage 1, Skip1 from stage 0.\n",
    "\n",
    "        # Let's try setting target skip channels to match the decoder_channels:\n",
    "        # Skip from stage 2 (128) -> project to decoder_channels[0]=256\n",
    "        # Skip from stage 1 (64) -> project to decoder_channels[1]=128\n",
    "        # Skip from stage 0 (32) -> project to decoder_channels[2]=64\n",
    "\n",
    "        target_skip_2d_channels_for_proj = list(decoder_channels[:-1]) # [256, 128, 64]\n",
    "        skip_3d_info_for_proj = list(zip(encoder_dims[:-1], self._skip_3d_spatial_sizes[:-1])) # [(32, sz0), (64, sz1), (128, sz2)]\n",
    "        skip_proj_pairs = list(zip(skip_3d_info_for_proj[::-1], target_skip_2d_channels_for_proj)) # [((128, sz2), 256), ((64, sz1), 128), ((32, sz0), 64)]\n",
    "\n",
    "        self.skip_projection_layers = nn.ModuleList()\n",
    "        self._projected_skip_2d_channels = [] # Store the output channels of projected skips\n",
    "\n",
    "        for (skip_c_3d, skip_sz_3d), target_2d_channels in skip_proj_pairs:\n",
    "             self.skip_projection_layers.append(\n",
    "                 nn.Sequential(\n",
    "                     nn.AdaptiveAvgPool3d((1, skip_sz_3d[1], skip_sz_3d[2])), # Pool time D_i to 1\n",
    "                     nn.Conv2d(skip_c_3d, target_2d_channels, kernel_size=1), # Project channels\n",
    "                 )\n",
    "             )\n",
    "             self._projected_skip_2d_channels.append(target_2d_channels)\n",
    "        # Projected skip channels (ordered from finest 3D stage -> coarsest 3D stage): [256, 128, 64]\n",
    "\n",
    "\n",
    "        # Define encoder_channels and skip_channels for UnetDecoder2d init:\n",
    "        # encoder_channels param (len 4): [Bottleneck_2D_chan, Skip3_2D_chan, Skip2_2D_chan, Skip1_2D_chan]\n",
    "        # Based on standard U-Net, this list defines the *input channels* to the blocks' concatenation.\n",
    "        # This requires feats list [B, S1, S2, S3] (bottleneck, coarsest skip, mid, finest).\n",
    "        # Let's assume UnetDecoder2d expects `encoder_channels` list elements to be the channels of `feats`[::-1]\n",
    "        # `feats` list: [Bottleneck_2D (embed_dim), Skip1_2D (proj from stg0, target 64), Skip2_2D (proj from stg1, target 128), Skip3_2D (proj from stg2, target 256)]\n",
    "        # Feats channels: [embed_dim, 64, 128, 256]\n",
    "        # `feats[::-1]` channels: [256, 128, 64, embed_dim]\n",
    "        decoder_enc_channels_for_init = self._projected_skip_2d_channels[::-1] + [embed_dim]\n",
    "\n",
    "        # `skip_channels` param list: [Skip1_2D_chan, Skip2_2D_chan, Skip3_2D_chan, 0]\n",
    "        # Based on `feats[1:] + [0]`\n",
    "        # Feats[1:] channels: [64, 128, 256]\n",
    "        # `skip_channels` param: [64, 128, 256, 0]\n",
    "        decoder_skip_channels_for_init = self._projected_skip_2d_channels + [0]\n",
    "\n",
    "        # Recheck lengths:\n",
    "        # decoder_channels: (256, 128, 64, 32) - len 4\n",
    "        # decoder_enc_channels_for_init: len 4\n",
    "        # decoder_skip_channels_for_init: len 4\n",
    "        # This matches the expected input list lengths for UnetDecoder2d init.\n",
    "\n",
    "        # The input `feats` list to UnetDecoder2d forward needs len 4.\n",
    "        # `feats` list: [Bottleneck_2D (embed_dim), Skip1_2D (from stg0, proj to 64), Skip2_2D (from stg1, proj to 128), Skip3_2D (from stg2, proj to 256)]\n",
    "        # Bottleneck_2D: embed_dim channels, spatial (H_b, W_b)\n",
    "        # Skip1_2D: 64 channels, spatial (_skip_3d_spatial_sizes[0][1:])\n",
    "        # Skip2_2D: 128 channels, spatial (_skip_3d_spatial_sizes[1][1:])\n",
    "        # Skip3_2D: 256 channels, spatial (_skip_3d_spatial_sizes[2][1:])\n",
    "\n",
    "        # Spatial sizes need to match upsampling:\n",
    "        # Decoder block 0 goes from (H_b, W_b) to next size, uses skip from stage 2 spatial (_skip_3d_spatial_sizes[2][1:])\n",
    "        # This requires the skip spatial size to match the output size of block 0.\n",
    "        # If scale_factor=2 for block 0, output size is (H_b*2, W_b*2).\n",
    "        # So skip from stage 2 spatial size (_skip_3d_spatial_sizes[2][1:]) must be (H_b*2, W_b*2).\n",
    "        # Let's verify sizes: Bottleneck (30, 2, 2) -> H_b=2, W_b=2. Skip stage 2 (_skip_3d_spatial_sizes[2]) (61, 4, 4) -> spatial (4, 4).\n",
    "        # Decoder input (2, 2). Block 0 scale 2 -> output (4, 4). Skip stage 2 is (4, 4). Matches.\n",
    "\n",
    "        # Decoder block 1 output (4*2, 4*2) = (8, 8). Skip stage 1 (_skip_3d_spatial_sizes[1]) (124, 8, 8) -> spatial (8, 8). Matches.\n",
    "        # Decoder block 2 output (8*2, 8*2) = (16, 16). Skip stage 0 (_skip_3d_spatial_sizes[0]) (250, 17, 17) -> spatial (17, 17). Mismatch! 16 != 17.\n",
    "\n",
    "        # The spatial sizes of the 3D Conv outputs (when pooled to 2D) must match the required spatial sizes for the decoder skips.\n",
    "        # This depends on the exact kernel/stride/padding in both encoder (3D Conv) and decoder (2D UpSample).\n",
    "        # Let's re-evaluate 3D Conv spatial sizes with k=2, s=2, p=1.\n",
    "        # Input (1000, 70, 70)\n",
    "        # Stg 0: k=2, s=2, p=1 -> (500, 35, 35)\n",
    "        # Stg 1: k=2, s=2, p=1 -> (250, 17, 17)\n",
    "        # Stg 2: k=2, s=2, p=1 -> (125, 8, 8)\n",
    "        # Stg 3: k=2, s=2, p=1 -> (62, 4, 4) -> Bottleneck H,W = (4,4)\n",
    "\n",
    "        # Decoder spatial sizes (starting from (4,4), scale 2):\n",
    "        # Block 0 input (4,4) -> output (8,8). Requires Skip from stage 2 with spatial (8,8). Matches.\n",
    "        # Block 1 input (8,8) -> output (16,16). Requires Skip from stage 1 with spatial (17,17). Mismatch.\n",
    "        # Block 2 input (16,16) -> output (32,32). Requires Skip from stage 0 with spatial (35,35). Mismatch.\n",
    "        # Block 3 input (32,32) -> output (64,64). No skip needed.\n",
    "\n",
    "        # This mismatch in spatial sizes requires adjusting the encoder 3D Conv strides/kernels or using interpolation layers explicitly.\n",
    "        # A common trick is to slightly adjust padding or strides in early layers to ensure consistent spatial sizes downstream.\n",
    "        # Or use AdaptiveAvgPool2d/3d to force spatial sizes after each stage/projection.\n",
    "        # Let's use AdaptiveAvgPool2d in skip projections to force the spatial size to match the expected decoder input size.\n",
    "\n",
    "        # Expected decoder skip spatial sizes:\n",
    "        # Skip from Stage 2 (used by Block 0): H_b*s_0, W_b*s_0. If scale_factors=(s0, s1, s2, s3) = (2,2,2,2), this is (4*2, 4*2) = (8, 8).\n",
    "        # Skip from Stage 1 (used by Block 1): H_b*s_0*s_1, W_b*s_0*s_1. (4*2*2, 4*2*2) = (16, 16).\n",
    "        # Skip from Stage 0 (used by Block 2): H_b*s_0*s_1*s_2, W_b*s_0*s_1*s_2. (4*2*2*2, 4*2*2*2) = (32, 32).\n",
    "\n",
    "        # Recalculate skip projection layers to use AdaptiveAvgPool2d to target specific (H, W) sizes.\n",
    "        # target_skip_spatial_sizes = [(8, 8), (16, 16), (32, 32)] # Corresponding to skips from Stg2, Stg1, Stg0\n",
    "\n",
    "        self.skip_projection_layers = nn.ModuleList()\n",
    "        skip_3d_info_for_proj = list(zip(encoder_dims[:-1], self._skip_3d_spatial_sizes[:-1])) # [(32, sz0), (64, sz1), (128, sz2)]\n",
    "        target_skip_2d_channels_for_proj = list(decoder_channels[:-1])[::-1] # [64, 128, 256] - Channels for skips from Stg0, Stg1, Stg2.\n",
    "        target_skip_spatial_sizes = [(32, 32), (16, 16), (8, 8)] # Spatial sizes for skips from Stg0, Stg1, Stg2.\n",
    "\n",
    "        # Combine info: ( (3D_chan, 3D_size), target_2D_chan, target_2D_size )\n",
    "        skip_proj_info = list(zip(skip_3d_info_for_proj, target_skip_2d_channels_for_proj, target_skip_spatial_sizes)) # [((32, sz0), 64, (32,32)), ((64, sz1), 128, (16,16)), ((128, sz2), 256, (8,8))]\n",
    "\n",
    "        for (skip_c_3d, skip_sz_3d), target_2d_channels, target_2d_size in skip_proj_info:\n",
    "            self.skip_projection_layers.append(\n",
    "                 nn.Sequential(\n",
    "                     nn.AdaptiveAvgPool3d((1, skip_sz_3d[1], skip_sz_3d[2])), # Pool time D_i to 1 -> (B, C_in, 1, H_i, W_i)\n",
    "                     nn.Conv2d(skip_c_3d, target_2d_channels, kernel_size=1), # Project channels -> (B, C_out, H_i, W_i)\n",
    "                     nn.AdaptiveAvgPool2d(target_2d_size) # Force spatial size\n",
    "                 )\n",
    "            )\n",
    "        # Projected skip channels (ordered from Stg0 -> Stg2): [64, 128, 256]\n",
    "        self._projected_skip_2d_channels = target_skip_2d_channels_for_proj\n",
    "\n",
    "        # Decoder init params are correct based on this structure:\n",
    "        # encoder_channels param: [256, 128, 64, embed_dim]\n",
    "        # skip_channels param: [64, 128, 256, 0]\n",
    "        # decoder_channels param: (256, 128, 64, 32)\n",
    "\n",
    "        # Need to project bottleneck channel (embed_dim) to match the expected channel for the first input in `encoder_channels` param list.\n",
    "        # The first element of `encoder_channels` param (256) is used as the channel of the bottleneck feature by UnetDecoder2d's internal `in_channels`.\n",
    "        # self.bottleneck_2d_proj needs to map embed_dim -> 256.\n",
    "        self.bottleneck_2d_proj = nn.Conv2d(embed_dim, decoder_enc_channels_for_init[-1], kernel_size=1) # project embed_dim to 256\n",
    "\n",
    "\n",
    "        # Final check on seg head scale factor.\n",
    "        # Decoder outputs (H_dec_out, W_dec_out). Target (70, 70).\n",
    "        # Using scale_factors=(2,2,2,2) and starting from bottleneck (H_b, W_b)=(4,4), output is (64,64).\n",
    "        # Seg head needs to go from (64,64) to (70,70). Scale (70/64, 70/64). Correct.\n",
    "\n",
    "        # Ensure MONAI Norm wrapper is used correctly\n",
    "        decoder_norm_layer = lambda x: monai.networks.blocks.Norm(\"INSTANCE\", spatial_dims=2)(x)\n",
    "\n",
    "\n",
    "        self.decoder = UnetDecoder2d(\n",
    "            encoder_channels=decoder_enc_channels_for_init, # [256, 128, 64, embed_dim]\n",
    "            skip_channels=decoder_skip_channels_for_init, # [64, 128, 256, 0]\n",
    "            decoder_channels=decoder_channels, # (256, 128, 64, 32)\n",
    "            scale_factors=decoder_scale_factors, # [2, 2, 2, 2]\n",
    "            norm_layer=decoder_norm_layer,\n",
    "            attention_type=decoder_attention_type,\n",
    "            upsample_mode=upsample_mode,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _forward_unscaled(self, x):\n",
    "        # Helper method to run the core forward pass without final scaling/TTA\n",
    "\n",
    "        # --- 3D Conv Encoder ---\n",
    "        skip_features_3d = []\n",
    "        x_3d = x\n",
    "        for i, stage in enumerate(self.conv_stages):\n",
    "            x_3d = stage(x_3d)\n",
    "            if i < self.num_encoder_stages - 1: # Save skips from stages 0, 1, 2\n",
    "                skip_features_3d.append(x_3d)\n",
    "\n",
    "        # x_3d is now the bottleneck 3D feature map from stage 3 (B, C_last, D_b, H_b, W_b)\n",
    "\n",
    "\n",
    "        # --- Transformer Bottleneck ---\n",
    "        B, C_last, D_b, H_b, W_b = x_3d.shape\n",
    "        x_flat = x_3d.permute(0, 2, 3, 4, 1).contiguous().view(B, -1, C_last) # (B, sequence_length, C_last)\n",
    "\n",
    "        x_flat = self.transformer_input_proj(x_flat) # (B, sequence_length, embed_dim)\n",
    "\n",
    "        # Add positional embedding\n",
    "        x_flat = x_flat + self.pos_embed\n",
    "\n",
    "        # Apply Transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x_flat = block(x_flat)\n",
    "\n",
    "        # Apply final norm\n",
    "        x_flat = self.transformer_norm(x_flat) # (B, sequence_length, embed_dim)\n",
    "\n",
    "\n",
    "        # --- Project Transformer output to 2D and pool time ---\n",
    "        # Reshape back to 3D-like structure: (B, embed_dim, D_b, H_b, W_b)\n",
    "        x_3d_like = x_flat.view(B, D_b, H_b, W_b, self.embed_dim).permute(0, 4, 1, 2, 3).contiguous()\n",
    "\n",
    "        # Pool over the time dimension (D_b) to get 2D feature map\n",
    "        # Then project channels to match decoder_enc_channels_for_init[-1] (256)\n",
    "        x_2d_bottleneck_pooled = torch.mean(x_3d_like, dim=2).squeeze(2) # (B, embed_dim, H_b, W_b)\n",
    "        x_2d_bottleneck = self.bottleneck_2d_proj(x_2d_bottleneck_pooled) # (B, 256, H_b, W_b)\n",
    "        # print(\"Bottleneck 2D shape:\", x_2d_bottleneck.shape)\n",
    "\n",
    "\n",
    "        # --- Prepare 2D Skip Connections ---\n",
    "        # Process the saved 3D skip features from stages 0, 1, 2\n",
    "        skip_features_2d = []\n",
    "        for i in range(self.num_encoder_stages - 1):\n",
    "            skip_3d = skip_features_3d[i] # Get 3D feature from stage i (0, 1, or 2)\n",
    "            skip_2d = self.skip_projection_layers[i](skip_3d) # Project to 2D using corresponding layer\n",
    "            skip_features_2d.append(skip_2d)\n",
    "            # print(f\"Projected Skip {i} shape: {skip_2d.shape}\")\n",
    "\n",
    "\n",
    "        # Combine bottleneck and skip features for the Decoder input\n",
    "        # Feats list for UnetDecoder2d: [Bottleneck_2D, Skip1_2D, Skip2_2D, Skip3_2D]\n",
    "        # Skip1 from stage 0, Skip2 from stage 1, Skip3 from stage 2.\n",
    "        decoder_feats = [x_2d_bottleneck] + skip_features_2d # This order is [Bottleneck, Stg0_skip, Stg1_skip, Stg2_skip]\n",
    "        # The required order for UnetDecoder2d feats list is [Bottleneck, Skip_coarsest, Skip_mid, Skip_finest]\n",
    "        # So, skip_features_2d is [Stg0_skip, Stg1_skip, Stg2_skip], which is already ordered coarsest to finest.\n",
    "        # Correct: decoder_feats = [x_2d_bottleneck] + skip_features_2d\n",
    "        # This means feats[0]=Bottleneck, feats[1]=Stg0_skip, feats[2]=Stg1_skip, feats[3]=Stg2_skip.\n",
    "\n",
    "        # Check shapes just before decoder:\n",
    "        # print(\"Shapes feeding decoder:\")\n",
    "        # for i, f in enumerate(decoder_feats): print(f\"  feats[{i}]: {f.shape}\")\n",
    "        # Expected:\n",
    "        # feats[0]: (B, 256, H_b, W_b) eg (B, 256, 4, 4)\n",
    "        # feats[1]: (B, 64, H_stg0_proj, W_stg0_proj) eg (B, 64, 32, 32)\n",
    "        # feats[2]: (B, 128, H_stg1_proj, W_stg1_proj) eg (B, 128, 16, 16)\n",
    "        # feats[3]: (B, 256, H_stg2_proj, W_stg2_proj) eg (B, 256, 8, 8)\n",
    "\n",
    "        # This matches the expected input list structure and order for the modified UnetDecoder2d based on re-interpretation.\n",
    "\n",
    "        # --- 2D Decoder ---\n",
    "        decoder_output_features = self.decoder(decoder_feats) # Processes feats[::-1] internally\n",
    "\n",
    "        # --- Segmentation Head ---\n",
    "        # The final output of the decoder is the last element in the list.\n",
    "        seg_output = self.seg_head(decoder_output_features[-1])\n",
    "        # print(\"Seg Head output shape:\", seg_output.shape) # Should be (B, 1, 70, 70)\n",
    "\n",
    "        return seg_output # (B, 1, 70, 70) - raw, unscaled prediction\n",
    "\n",
    "\n",
    "# --- Reuse original UnetDecoder2d and SegmentationHead2d ---\n",
    "# Copy paste the original definitions or ensure they are available.\n",
    "# Assuming the original definitions are in the user's provided code block.\n",
    "\n",
    "# Dummy DropPath if timm isn't fully available\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return timm.models.layers.drop_path(x, self.drop_prob, self.training) # Rely on timm's functional drop_path\n",
    "\n",
    "\n",
    "# Replace DropPath in TransformerBlock if timm.models.layers.DropPath is not used directly\n",
    "# from timm.models.layers import DropPath # Use this if timm is fully installed and imported\n",
    "\n",
    "# Re-define TransformerBlock to use the dummy DropPath if needed\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = norm_layer(dim)\n",
    "#         self.attn = nn.MultiheadAttention(dim, num_heads, dropout=attn_drop, bias=qkv_bias, batch_first=True)\n",
    "#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity() # Use dummy DropPath\n",
    "#         self.norm2 = norm_layer(dim)\n",
    "#         mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(dim, mlp_hidden_dim),\n",
    "#             act_layer(),\n",
    "#             nn.Dropout(drop),\n",
    "#             nn.Linear(mlp_hidden_dim, dim),\n",
    "#             nn.Dropout(drop)\n",
    "#         )\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         residual = x\n",
    "#         x = self.norm1(x)\n",
    "#         x = self.attn(x, x, x)[0]\n",
    "#         x = residual + self.drop_path(x)\n",
    "#\n",
    "#         residual = x\n",
    "#         x = self.norm2(x)\n",
    "#         x = self.mlp(x)\n",
    "#         x = residual + self.drop_path(x)\n",
    "#         return x\n",
    "#\n",
    "# # Ensure DropPath is available if drop_path > 0 is used in HybridModel init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Training Loop Integration ---\n",
    "# Replace the old model initialization with the new HybridModel\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "train_ds = CustomDataset(cfg=cfg, file_pairs=cfg.file_pairs, mode=\"train\")\n",
    "# Replaced DistributedSampler with standard DataLoader and shuffle\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size= cfg.batch_size,\n",
    "    num_workers= 0, # Keep 0 for simpler debugging initially, increase for performance\n",
    "    shuffle=True, # Add shuffle for training\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "\n",
    "valid_ds = CustomDataset(cfg=cfg, file_pairs=cfg.file_pairs, mode=\"valid\")\n",
    "# Replaced DistributedSampler with standard DataLoader\n",
    "valid_dl = torch.utils.data.DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size= cfg.batch_size_val,\n",
    "    num_workers= 0, # Keep 0 for simpler debugging initially\n",
    "    shuffle=False, # No shuffle for validation\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test dataset output shape\n",
    "try:\n",
    "    x, y = next(iter(train_dl))\n",
    "    print(\"Sample data batch shape:\", x.shape, \"Sample label batch shape:\", y.shape)\n",
    "    assert x.shape[1:] == (cfg.in_channels, cfg.spatial_size[0], cfg.spatial_size[1], cfg.spatial_size[2]), f\"Data shape mismatch: Expected {(cfg.in_channels,) + cfg.spatial_size}, got {x.shape[1:]}\"\n",
    "    assert y.shape[1:] == (70, 70), f\"Label shape mismatch: Expected (70, 70), got {y.shape[1:]}\"\n",
    "    print(\"Dataset shapes confirmed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing dataset shapes: {e}\")\n",
    "    print(\"Please check CustomDataset logic and data file shapes.\")\n",
    "    # sys.exit(1) # Exit if dataset shapes are wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eafd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== Model / Optim ==========\n",
    "# Instantiate the new HybridModel\n",
    "model = HybridModel(\n",
    "    in_channels=cfg.in_channels,\n",
    "    spatial_size=cfg.spatial_size,\n",
    "    encoder_dims=cfg.encoder_dims,\n",
    "    embed_dim=cfg.embed_dim,\n",
    "    num_heads=cfg.num_heads,\n",
    "    mlp_ratio=cfg.mlp_ratio,\n",
    "    transformer_depth=cfg.transformer_depth,\n",
    "    decoder_channels=cfg.decoder_channels,\n",
    "    decoder_attention_type=cfg.decoder_attention_type,\n",
    "    upsample_mode=cfg.upsample_mode,\n",
    ").to(cfg.device)\n",
    "\n",
    "\n",
    "if cfg.ema:\n",
    "    if cfg.local_rank == 0:\n",
    "        print(\"Initializing EMA model..\")\n",
    "    # Set device explicitly to 'cpu' or 'cuda' for EMA\n",
    "    ema_model = ModelEMA(\n",
    "        model,\n",
    "        decay=cfg.ema_decay,\n",
    "        device=cfg.device, # EMA model on the same device as the main model\n",
    "    )\n",
    "else:\n",
    "    ema_model = None\n",
    "\n",
    "criterion = nn.L1Loss() # Mean Absolute Error is suitable for regression/velocity maps\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Removed GradScaler for simplicity / CPU usage\n",
    "\n",
    "# Example of tracking UD - keep or remove as needed\n",
    "parameters_conv = []\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Conv3d)) and m.weight.requires_grad: # Include Conv3d\n",
    "        p = m.weight\n",
    "        if p.ndim >= 4: # Consider 3D and 2D conv weights\n",
    "            parameters_conv.append(p)\n",
    "ud = []\n",
    "eps = 1e-8\n",
    "\n",
    "best_loss= 1_000_000\n",
    "val_loss= 1_000_000 # Initialize val_loss for logging on epoch 0\n",
    "\n",
    "\n",
    "print(f\"Starting training for {cfg.epochs} epochs...\")\n",
    "\n",
    "for epoch in range(0, cfg.epochs+1):\n",
    "    if epoch > 0: # Skip epoch 0 training loop, only validate initially\n",
    "        tstart= time.time()\n",
    "        # Removed sampler.set_epoch - not needed for standard DataLoader\n",
    "\n",
    "        # Train loop\n",
    "        model.train()\n",
    "        total_loss = []\n",
    "        # tqdm for train loop only on rank 0\n",
    "        train_loop = tqdm(train_dl, disable=cfg.local_rank != 0, desc=f\"Epoch {epoch} Training\")\n",
    "        for i, (x, y) in enumerate(train_loop):\n",
    "            x = x.to(cfg.device)\n",
    "            y = y.to(cfg.device) # Label is (B, 70, 70)\n",
    "\n",
    "            # Predict raw unscaled output (B, 70, 70)\n",
    "            logits_unscaled = model(x)\n",
    "\n",
    "            # Scale logits to match label range before calculating loss\n",
    "            # Assuming labels are in range [3000, 4500] based on original code's scaling\n",
    "            # And model predicts values that should be scaled by * 1500 + 3000\n",
    "            # So, target `y` should be UNscaled to match `logits_unscaled` range (e.g., 0-1)\n",
    "            # Unscale y: (y - 3000) / 1500\n",
    "            y_unscaled = (y - 3000) / 1500.0\n",
    "\n",
    "            # Clamp y_unscaled to [0, 1] range in case of floating point issues or label noise\n",
    "            y_unscaled = torch.clamp(y_unscaled, 0.0, 1.0)\n",
    "\n",
    "            loss = criterion(logits_unscaled, y_unscaled)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            ### Trach UD (Optional)\n",
    "            if parameters_conv: # Only track if conv parameters were found\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                with torch.no_grad():\n",
    "                    ud.append([\n",
    "                            (lr * (p.grad.std() + eps) / (p.data.std() + eps)).log10().item()\n",
    "                            if p.grad is not None else float('-inf')\n",
    "                            for p in parameters_conv\n",
    "                    ])\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            if ema_model is not None:\n",
    "                ema_model.update(model)\n",
    "\n",
    "        avg_train_loss = np.mean(total_loss)\n",
    "        if cfg.local_rank == 0:\n",
    "            print(f\"Epoch {epoch} Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Epoch {epoch} Training Time: {format_time(time.time() - tstart)}\")\n",
    "\n",
    "\n",
    "    # ========== Valid ==========\n",
    "    model.eval()\n",
    "    val_logits = [] # Store scaled predictions\n",
    "    val_targets = []\n",
    "    # tqdm for validation loop on rank 0\n",
    "    valid_loop = tqdm(valid_dl, disable=cfg.local_rank != 0, desc=f\"Epoch {epoch} Validation\")\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loop:\n",
    "            x = x.to(cfg.device)\n",
    "            # y is already scaled [3000, 4500]\n",
    "            y = y.to(cfg.device)\n",
    "\n",
    "            # Model returns scaled prediction during eval\n",
    "            if ema_model is not None:\n",
    "                 # Access the underlying model from EMA wrapper\n",
    "                 # EMA model should be in eval mode already\n",
    "                 out = ema_model.module(x)\n",
    "            else:\n",
    "                out = model(x) # Model returns scaled output in eval mode\n",
    "\n",
    "            val_logits.append(out.cpu())\n",
    "            val_targets.append(y.cpu())\n",
    "\n",
    "        val_logits= torch.cat(val_logits, dim=0) # (N, 70, 70) scaled\n",
    "        val_targets= torch.cat(val_targets, dim=0) # (N, 70, 70) scaled\n",
    "\n",
    "        # Calculate loss on scaled values during validation\n",
    "        val_loss = criterion(val_logits, val_targets).item()\n",
    "\n",
    "    if cfg.local_rank == 0:\n",
    "        print(f\"Epoch {epoch} Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            cfg.early_stopping[\"streak\"] = 0\n",
    "            # Save best model (main model and EMA if exists)\n",
    "            print(\"Validation loss improved, saving model...\")\n",
    "            # Ensure directory exists\n",
    "            os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"./checkpoints/best_model_epoch{epoch:03d}.pth\")\n",
    "            if ema_model is not None:\n",
    "                 torch.save(ema_model.module.state_dict(), f\"./checkpoints/best_ema_model_epoch{epoch:03d}.pth\")\n",
    "        else:\n",
    "            cfg.early_stopping[\"streak\"] += 1\n",
    "            print(f\"Validation loss did not improve. Streak: {cfg.early_stopping['streak']}\")\n",
    "            if cfg.early_stopping[\"streak\"] >= cfg.early_stopping[\"patience\"]:\n",
    "                print(f\"Early stopping triggered after {cfg.early_stopping['patience']} epochs without improvement.\")\n",
    "                break # Exit training loop\n",
    "\n",
    "    if epoch == 0 and cfg.epochs > 0: # If training is actually planned, print val loss for epoch 0 before first train\n",
    "         print(f\"Epoch {epoch} Validation Loss: {val_loss:.4f} (Initial Eval)\")\n",
    "\n",
    "    # Removed barrier/all_reduce - assuming single process training\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- Re-define UnetDecoder2d and SegmentationHead2d here if they weren't in the original block ---\n",
    "# (Copy paste from the user's original code)\n",
    "class ConvBnAct2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding: int = 0,\n",
    "        stride: int = 1,\n",
    "        norm_layer: nn.Module = nn.Identity, # Using nn.Identity placeholder\n",
    "        act_layer: nn.Module = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv= nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=False, # Bias is usually false when followed by Norm\n",
    "        )\n",
    "        # Check if norm_layer is a type or an instance\n",
    "        if isinstance(norm_layer, type) and issubclass(norm_layer, nn.Module):\n",
    "             self.norm = norm_layer(out_channels) if norm_layer != nn.Identity else nn.Identity()\n",
    "        elif isinstance(norm_layer, nn.Module):\n",
    "             self.norm = norm_layer # Assume it's an initialized instance\n",
    "        else:\n",
    "             self.norm = nn.Identity() # Default if not a valid norm type/instance\n",
    "\n",
    "\n",
    "        self.act= act_layer(inplace=True) if act_layer != nn.Identity else nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SCSEModule2d(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
    "            nn.GELU(), # Changed Tanh to GELU for consistency\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sSE = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 1, 1),\n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.cSE(x) + x * self.sSE(x)\n",
    "\n",
    "class Attention2d(nn.Module):\n",
    "    def __init__(self, name, **params):\n",
    "        super().__init__()\n",
    "        if name is None:\n",
    "            self.attention = nn.Identity(**params)\n",
    "        elif name == \"scse\":\n",
    "            # Pass in_channels to SCSEModule2d\n",
    "            self.attention = SCSEModule2d(**params)\n",
    "        else:\n",
    "            raise ValueError(\"Attention {} is not implemented\".format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x)\n",
    "\n",
    "class DecoderBlock2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels, # Input channels from previous decoder block / bottleneck\n",
    "        skip_channels, # Input channels from skip connection\n",
    "        out_channels, # Output channels of this block\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = None,\n",
    "        intermediate_conv: bool = False, # Not used in the base code's UnetDecoder2d\n",
    "        upsample_mode: str = \"deconv\", # Used in base code\n",
    "        scale_factor: int = 2, # Used in base code\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Upsample block. Input channels should be `in_channels`. Output channels depends on mode.\n",
    "        # If mode is \"deconv\", out_channels is required. If \"pixelshuffle\", in_channels is required.\n",
    "        # MONAI UpSample requires out_channels for \"deconv\".\n",
    "        # Let's assume the deconv layer immediately changes channels to the desired size before concatenation.\n",
    "        # A common pattern is Deconv -> Concatenate Skip -> Conv -> Conv.\n",
    "        # The base code's UnetDecoder2d uses UpSample, then concat, then Conv1, Conv2.\n",
    "        # UpSample output channels become part of the concatenation input.\n",
    "        # If upsample_mode is \"deconv\", UpSample produces `out_channels` features at larger spatial size.\n",
    "        # If upsample_mode is not \"deconv\", UpSample produces `in_channels` features at larger spatial size.\n",
    "        # The base code's UpSample definition in UnetDecoder2d is `UpSample(..., in_channels=ic, out_channels=ic, ...)`\n",
    "        # So UpSample outputs `ic` channels. This makes sense before concatenation.\n",
    "\n",
    "        self.upsample = UpSample(\n",
    "            spatial_dims= 2,\n",
    "            in_channels= in_channels,\n",
    "            out_channels= in_channels if upsample_mode != \"deconv\" else in_channels, # Or specify out_channels?\n",
    "            scale_factor= scale_factor,\n",
    "            mode= upsample_mode,\n",
    "            # Deconv mode requires out_channels explicitly\n",
    "            # Let's check MONAI UpSample definition:\n",
    "            # __init__(self, spatial_dims, in_channels, out_channels=None, scale_factor=2, mode='nearest', align_corners=None):\n",
    "            # If mode == \"deconv\", out_channels should be specified for ConvTranspose.\n",
    "            # If out_channels is None, it defaults to in_channels.\n",
    "            # Let's assume deconv outputs `in_channels` features by default.\n",
    "            # Or maybe it should output a number of channels suitable for concatenation?\n",
    "            # A common deconv block is Deconv(in, out) -> Relu/Norm -> Concat(skip). The 'out' is the channels after deconv.\n",
    "            # In the base code, `in_channels` to the block is `ic`, `skip_channels` is `sc`.\n",
    "            # The concat layer input channels are `ic + sc`. First conv is `ConvBnAct2d(ic + sc, dc, ...)`.\n",
    "            # The UpSample is `UpSample(..., in_channels=ic, out_channels=ic, mode=upsample_mode)`.\n",
    "            # So UpSample outputs `ic` channels at larger size. Then it's concatenated with `sc` skip channels.\n",
    "            # This structure implies UpSample should output `in_channels` features.\n",
    "\n",
    "            # For deconv, need to adjust if `in_channels` isn't the desired output channels.\n",
    "            # If using ConvTranspose2d, the output channels of transpose conv become the input to concatenation.\n",
    "            # Let's assume UpSample handles this and `in_channels` is the input to UpSample, and it outputs features\n",
    "            # ready for concatenation with `skip_channels`.\n",
    "            # If mode is 'deconv', MONAI uses ConvTranspose. The output channels of ConvTranspose should probably be `in_channels`\n",
    "            # or some other number to match the skip channels after concat.\n",
    "            # A common pattern in UNet: Deconv(in_C, in_C/2) -> Concat(skip_C=in_C/2) -> Conv(in_C, out_C).\n",
    "            # Base code uses `ConvBnAct2d(in_channels + skip_channels, out_channels, ...)` after concat.\n",
    "            # This suggests UpSample outputs `in_channels` features.\n",
    "\n",
    "        # If upsample_mode == \"deconv\", need to specify out_channels for ConvTranspose.\n",
    "        # What should the out_channels of the transpose conv be?\n",
    "        # It should probably be `in_channels` unless we want channel reduction there.\n",
    "        # Let's assume `in_channels` features are produced by UpSample.\n",
    "\n",
    "        if upsample_mode == \"pixelshuffle\":\n",
    "             # PixelShuffle expects input channels to be C * scale_factor^2\n",
    "             # If input has `in_channels` and scale is `scale_factor`, output channels will be `in_channels / scale_factor^2`\n",
    "             # This seems reverse of what's needed for Unet.\n",
    "             # Let's assume UpSample('pixelshuffle') maps `in_channels` to `in_channels` at larger size.\n",
    "             # MONAI SubpixelUpsample takes `in_channels` and maps to `in_channels`.\n",
    "            self.upsample= SubpixelUpsample(\n",
    "                spatial_dims= 2,\n",
    "                in_channels= in_channels,\n",
    "                scale_factor= scale_factor,\n",
    "            )\n",
    "        else:\n",
    "            # Defaulting to 'transpose' (deconv) or 'nearest', 'bilinear'.\n",
    "            # MONAI UpSample with 'transpose' mode needs out_channels for ConvTranspose.\n",
    "            # What should out_channels be? If input is `in_channels` and skip is `skip_channels`,\n",
    "            # maybe the transpose conv outputs `in_channels` features to be concatenated with `skip_channels`.\n",
    "            # Let's stick to the original code's apparent intent where UpSample outputs `in_channels`.\n",
    "            self.upsample = UpSample(\n",
    "                spatial_dims= 2,\n",
    "                in_channels= in_channels,\n",
    "                out_channels= in_channels, # Assuming output channels are the same as input channels for upsampling\n",
    "                scale_factor= scale_factor,\n",
    "                mode= upsample_mode,\n",
    "            )\n",
    "\n",
    "\n",
    "        # Intermediate conv is not used in original UnetDecoder2d\n",
    "        # self.intermediate_conv = None\n",
    "\n",
    "\n",
    "        # Attention applied after concatenation (or just after upsample if no skip)\n",
    "        # Total input channels to attention: `in_channels + skip_channels`\n",
    "        attention1_in_channels = in_channels + skip_channels if skip_channels > 0 else in_channels # If skip_channels is 0, only use upsampled features\n",
    "        self.attention1 = Attention2d(\n",
    "            name= attention_type,\n",
    "            in_channels= attention1_in_channels,\n",
    "            )\n",
    "\n",
    "        # First convolution after concatenation\n",
    "        self.conv1 = ConvBnAct2d(\n",
    "            attention1_in_channels, # Input channels: upsampled + skip\n",
    "            out_channels,\n",
    "            kernel_size= 3,\n",
    "            padding= 1,\n",
    "            norm_layer= norm_layer,\n",
    "            act_layer= nn.GELU, # Use GELU consistently\n",
    "        )\n",
    "\n",
    "        # Second convolution\n",
    "        self.conv2 = ConvBnAct2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size= 3,\n",
    "            padding= 1,\n",
    "            norm_layer= norm_layer,\n",
    "            act_layer= nn.GELU, # Use GELU consistently\n",
    "        )\n",
    "\n",
    "        # Attention applied after the second convolution\n",
    "        self.attention2 = Attention2d(\n",
    "            name= attention_type,\n",
    "            in_channels= out_channels,\n",
    "            )\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        # x is feature from previous decoder block / bottleneck\n",
    "        # skip is feature from encoder skip connection\n",
    "\n",
    "        # Upsample\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        # Concatenate with skip connection\n",
    "        if skip is not None:\n",
    "            # print(f\"  DecoderBlock: Upsampled shape {x.shape}, Skip shape {skip.shape}\")\n",
    "            # Ensure spatial sizes match for concatenation\n",
    "            if x.shape[-2:] != skip.shape[-2:]:\n",
    "                print(f\"Warning: Spatial size mismatch in DecoderBlock. Upsampled: {x.shape[-2:]}, Skip: {skip.shape[-2:]}. Using interpolation on skip.\")\n",
    "                # Interpolate skip to match upsampled size\n",
    "                skip = F.interpolate(skip, size=x.shape[-2:], mode='nearest') # Or 'bilinear'\n",
    "\n",
    "            x = torch.cat([x, skip], dim=1) # Concatenate along channel dimension\n",
    "            # print(f\"  DecoderBlock: After concat shape {x.shape}\")\n",
    "            x = self.attention1(x) # Apply attention after concat\n",
    "            # print(f\"  DecoderBlock: After attention1 shape {x.shape}\")\n",
    "\n",
    "        elif self.attention1.attention is not nn.Identity:\n",
    "             # If no skip but attention1 exists, apply it to x\n",
    "             x = self.attention1(x)\n",
    "\n",
    "\n",
    "        # Apply convolutions\n",
    "        x = self.conv1(x)\n",
    "        # print(f\"  DecoderBlock: After conv1 shape {x.shape}\")\n",
    "        x = self.conv2(x)\n",
    "        # print(f\"  DecoderBlock: After conv2 shape {x.shape}\")\n",
    "\n",
    "        # Apply second attention\n",
    "        x = self.attention2(x)\n",
    "        # print(f\"  DecoderBlock: After attention2 shape {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetDecoder2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet decoder adapted from the provided code.\n",
    "    Assumes input `feats` list is [bottleneck_feature, skip_coarsest, ..., skip_finest].\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: tuple[int], # Channels corresponding to feats[::-1]\n",
    "        skip_channels: tuple[int], # Channels corresponding to feats[1:] + [0]\n",
    "        decoder_channels: tuple = (256, 128, 64, 32), # Output channels of decoder blocks\n",
    "        scale_factors: tuple = (2,2,2,2), # Upsampling factors for blocks\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = None,\n",
    "        intermediate_conv: bool = False, # Not used, kept for compatibility\n",
    "        upsample_mode: str = \"deconv\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_channels = encoder_channels # Channels of feats[::-1]\n",
    "        self.skip_channels = skip_channels       # Channels of feats[1:] + [0]\n",
    "        self.decoder_channels = decoder_channels # Output channels of blocks\n",
    "        self.scale_factors = scale_factors\n",
    "        self.norm_layer = norm_layer\n",
    "        self.attention_type = attention_type\n",
    "        self.upsample_mode = upsample_mode\n",
    "\n",
    "        # Number of decoder blocks\n",
    "        num_blocks = len(decoder_channels)\n",
    "        assert len(encoder_channels) == num_blocks, \"encoder_channels length must match decoder_channels length\"\n",
    "        assert len(skip_channels) == num_blocks, \"skip_channels length must match decoder_channels length\"\n",
    "        assert len(scale_factors) == num_blocks, \"scale_factors length must match decoder_channels length\"\n",
    "\n",
    "\n",
    "        # Build decoder blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        # UnetDecoder2d's internal logic creates input channel lists based on encoder_channels and decoder_channels\n",
    "        # in_channels_to_blocks = [encoder_channels[0]] + list(decoder_channels[:-1]) # Channels from previous block / bottleneck\n",
    "        # skip_channels_for_blocks = list(encoder_channels[1:]) + [0] # Channels from skip connections\n",
    "\n",
    "        # Let's re-implement the blocks loop directly based on standard U-Net flow\n",
    "        # Block 0 input: bottleneck + skip_coarsest\n",
    "        # Block 1 input: block0_output + skip_mid1\n",
    "        # ...\n",
    "        # Block N-1 input: blockN-2_output + skip_finest\n",
    "\n",
    "        # This requires knowing the input channels and skip channels for EACH block.\n",
    "        # Let's define the input channels to each block's concatenation explicitly.\n",
    "        # Block i takes input from block i-1 (or bottleneck for i=0) and skip feature i.\n",
    "        # Input channel to Block 0 concat: [Channels from bottleneck] + [Channels from skip_coarsest]\n",
    "        # Input channel to Block i concat: [Channels from Block i-1 Output] + [Channels from skip_i]\n",
    "        # Output channel of Block i is decoder_channels[i].\n",
    "\n",
    "        # Let's trace the channels based on the `encoder_channels` and `skip_channels` params and `decoder_channels` outputs\n",
    "        # These lists define the channels of the features passed to the forward method `feats`.\n",
    "        # feats = [bottleneck, skip1, skip2, ..., skipN] where skip1 is coarsest. len(feats) = N+1.\n",
    "        # UnetDecoder2d params len = N+1 (seems wrong based on base code).\n",
    "        # Let's go back to base code logic: params len = len(decoder_channels) = N.\n",
    "        # encoder_channels param (len N): defines channels of feats[::-1]. [S_N, ..., S_1, B]\n",
    "        # skip_channels param (len N): defines channels of feats[1:] + [0]. [S_1, S_2, ..., S_N, 0]\n",
    "        # decoder_channels param (len N): defines output channels. [D_1, ..., D_N]\n",
    "\n",
    "        # This implies `feats` list has length N+1.\n",
    "        # feats = [Bottleneck, Skip1, Skip2, ..., SkipN]\n",
    "        # len(feats) = N+1.\n",
    "        # But UnetDecoder2d forward loop is `for i, b in enumerate(self.blocks)` (N blocks).\n",
    "        # `skip = feats[i] if i < len(feats) else None`.\n",
    "        # This means skip for block 0 is feats[0] (Bottleneck)? Skip for block 1 is feats[1] (Skip1)?\n",
    "        # This structure is confusing and seems non-standard.\n",
    "\n",
    "        # Let's make a reasonable assumption that the provided UnetDecoder2d class is intended for a standard U-Net flow:\n",
    "        # Takes a list of features `feats = [bottleneck, skip_coarsest, ..., skip_finest]`\n",
    "        # Processes N blocks, where block i uses `feats[i]` as skip (for i > 0).\n",
    "        # The first input to the first block comes from `feats[0]`.\n",
    "\n",
    "        # Let's define the blocks based on this standard interpretation.\n",
    "        # Number of blocks = len(decoder_channels).\n",
    "        num_blocks = len(decoder_channels)\n",
    "        # Need num_blocks skips + 1 bottleneck = num_blocks + 1 features in `feats`.\n",
    "        # Ensure input list `feats` to forward method has length num_blocks + 1.\n",
    "\n",
    "        # Channel tracking for standard U-Net decoder:\n",
    "        # Block 0 input channels: Channels of bottleneck + Channels of skip_coarsest (feats[1])\n",
    "        # Output channels: decoder_channels[0]\n",
    "        # Block 1 input channels: Channels of Block 0 Output + Channels of skip_mid1 (feats[2])\n",
    "        # Output channels: decoder_channels[1]\n",
    "        # ...\n",
    "        # Block N-1 input channels: Channels of Block N-2 Output + Channels of skip_finest (feats[N])\n",
    "        # Output channels: decoder_channels[N-1]\n",
    "\n",
    "        # We need to tell each DecoderBlock its input channels (after concat).\n",
    "        # DecoderBlock init needs `in_channels` (from previous block/bottleneck) and `skip_channels`.\n",
    "        # Let's pass the required channels directly.\n",
    "\n",
    "        # Input channel to Block 0: from bottleneck. Channels = Channels of feats[0].\n",
    "        # Skip channel for Block 0: from skip_coarsest. Channels = Channels of feats[1].\n",
    "        # Output channel of Block 0: decoder_channels[0].\n",
    "\n",
    "        # Input channel to Block 1: from Block 0 output. Channels = decoder_channels[0].\n",
    "        # Skip channel for Block 1: from skip_mid1. Channels = Channels of feats[2].\n",
    "        # Output channel of Block 1: decoder_channels[1].\n",
    "\n",
    "        # Input channel to Block i: Channels of decoder_channels[i-1].\n",
    "        # Skip channel for Block i: Channels of feats[i+1].\n",
    "        # Output channel of Block i: decoder_channels[i].\n",
    "\n",
    "        # This requires knowing the channels of the `feats` list *before* calling UnetDecoder2d.\n",
    "        # The `encoder_channels` and `skip_channels` params in UnetDecoder2d's __init__ seem intended\n",
    "        # to *derive* these channels, based on the input list `feats`.\n",
    "        # Let's assume:\n",
    "        # `encoder_channels` param = channels of `feats` in forward pass: [B_chan, S1_chan, S2_chan, ..., SN_chan]\n",
    "        # `skip_channels` param = channels of `feats`[1:] + [0]: [S1_chan, S2_chan, ..., SN_chan, 0]\n",
    "        # `decoder_channels` param = output channels: [D1_chan, ..., DN_chan]\n",
    "\n",
    "        # Based on this assumption, let's redefine the blocks:\n",
    "        # Block 0: in_channels = encoder_channels[0] (bottleneck). skip_channels = skip_channels[0] (skip1). out_channels = decoder_channels[0].\n",
    "        # Block 1: in_channels = decoder_channels[0] (prev output). skip_channels = skip_channels[1] (skip2). out_channels = decoder_channels[1].\n",
    "        # Block i: in_channels = decoder_channels[i-1]. skip_channels = skip_channels[i]. out_channels = decoder_channels[i].\n",
    "\n",
    "        input_channels_to_blocks = [encoder_channels[0]] + list(decoder_channels[:-1])\n",
    "        skip_channels_for_blocks = list(skip_channels) # Use the skip_channels param directly\n",
    "\n",
    "        assert len(input_channels_to_blocks) == num_blocks, \"Input channel calculation error\"\n",
    "        assert len(skip_channels_for_blocks) == num_blocks, \"Skip channel list length mismatch\"\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "             ic = input_channels_to_blocks[i]\n",
    "             sc = skip_channels_for_blocks[i]\n",
    "             dc = decoder_channels[i]\n",
    "             sf = scale_factors[i]\n",
    "\n",
    "             self.blocks.append(\n",
    "                 DecoderBlock2d(\n",
    "                     ic, sc, dc,\n",
    "                     norm_layer= self.norm_layer,\n",
    "                     attention_type= self.attention_type,\n",
    "                     intermediate_conv= intermediate_conv, # Kept False\n",
    "                     upsample_mode= self.upsample_mode,\n",
    "                     scale_factor= sf,\n",
    "                 )\n",
    "             )\n",
    "\n",
    "\n",
    "    def forward(self, feats: list[torch.Tensor]):\n",
    "        # feats: list of tensors [bottleneck_feature, skip_coarsest, ..., skip_finest]\n",
    "        # Number of blocks = len(self.blocks) = len(self.decoder_channels)\n",
    "        # Expected len(feats) = len(self.decoder_channels) + 1\n",
    "\n",
    "        num_blocks = len(self.blocks)\n",
    "        if len(feats) != num_blocks + 1:\n",
    "             print(f\"Error: UnetDecoder2d expected {num_blocks+1} features but got {len(feats)}\")\n",
    "             # Adjusting input list based on expectation for demo purposes\n",
    "             # If we have 4 features (1 bottleneck, 3 skips) and 4 blocks are expected\n",
    "             # The original base code seemed to feed only 4 features to a 4-block decoder?\n",
    "             # Let's assume the original base code's use of feats was `feats = [f_stg4, f_stg3, f_stg2, f_stg1]`.\n",
    "             # And its decoder had 4 blocks.\n",
    "             # UnetDecoder2d loop: res = [feats[0]]; feats = feats[1:]\n",
    "             # Block 0: skip=feats[0] (original feats[1]), input=res[-1] (original feats[0]) -> concat(f_stg4, f_stg3)\n",
    "             # Block 1: skip=feats[1] (original feats[2]), input=res[-1] (output block 0) -> concat(output_b0, f_stg2)\n",
    "             # ...\n",
    "             # Block 3: skip=feats[3] (original feats[4] - out of bounds), input=res[-1] -> concat(output_b2, None)\n",
    "\n",
    "             # Let's align with the original base code's UnetDecoder2d forward pass logic.\n",
    "             # It seems to take `feats` as [feature_level_N, feature_level_N-1, ..., feature_level_1]\n",
    "             # where level N is the coarsest skip, and level 1 is the finest skip.\n",
    "             # And it treats `feats[0]` as the bottleneck feature *for the first block's input*, and `feats[1:]` as the skips.\n",
    "             # This implies the input `feats` list should be [bottleneck_input_to_block0, skip_for_block0, skip_for_block1, ...]\n",
    "             # No, the base code was `res = [feats[0]]` and `feats= feats[1:]` and `skip=feats[i]`.\n",
    "             # This means feats[0] is res[-1] for block 0, and feats[0] (after slicing) is skip for block 0.\n",
    "             # This is not standard U-Net.\n",
    "\n",
    "             # Let's assume the standard U-Net flow where `feats` is [bottleneck, skip_coarsest, ..., skip_finest]\n",
    "             # And the loop is adjusted.\n",
    "             # This requires modifying the UnetDecoder2d forward pass.\n",
    "             # Option 1: Modify UnetDecoder2d to match standard U-Net.\n",
    "             # Option 2: Prepare `feats` list in `HybridModel.forward` to match the current `UnetDecoder2d` forward logic.\n",
    "             # Based on `res = [feats[0]]; feats = feats[1:]; for i, b in enumerate(self.blocks): skip = feats[i] if i < len(feats) else None; res.append(b(res[-1], skip=skip))`.\n",
    "             # If `feats_in` = [B, S1, S2, S3] (B=bottleneck, S1=coarsest, S2, S3=finest)\n",
    "             # res = [B]\n",
    "             # feats = [S1, S2, S3]\n",
    "             # Block 0: skip=feats[0]=S1. Input=res[-1]=B. Block0(B, S1). res=[B, Output_B0]\n",
    "             # Block 1: skip=feats[1]=S2. Input=res[-1]=Output_B0. Block1(Output_B0, S2). res=[B, Output_B0, Output_B1]\n",
    "             # Block 2: skip=feats[2]=S3. Input=res[-1]=Output_B1. Block2(Output_B1, S3). res=[B, Output_B0, Output_B1, Output_B2]\n",
    "             # Block 3: skip=feats[3] (index out of bounds). skip=None. Input=res[-1]=Output_B2. Block3(Output_B2, None). res=[B, Output_B0, Output_B1, Output_B2, Output_B3]\n",
    "             # This seems to be the intended logic of the provided UnetDecoder2d.\n",
    "             # It takes feats=[Bottleneck, Skip1, Skip2, Skip3] and has 4 blocks.\n",
    "             # This perfectly matches the 4 levels of features (1 bottleneck, 3 skips) produced by our 4-stage encoder.\n",
    "\n",
    "             # So the `decoder_feats` list constructed in `HybridModel.forward` is correct:\n",
    "             # `decoder_feats = [x_2d_bottleneck] + skip_features_2d` where `skip_features_2d` is [Stg0_skip, Stg1_skip, Stg2_skip].\n",
    "             # This means `feats` list will be [Bottleneck, Stg0_skip, Stg1_skip, Stg2_skip].\n",
    "             # And UnetDecoder2d is initialized with `encoder_channels` and `skip_channels` reflecting the channels of this `feats` list.\n",
    "             # This means `encoder_channels` param is [Stg2_skip_chan, Stg1_skip_chan, Stg0_skip_chan, Bottleneck_chan].\n",
    "             # And `skip_channels` param is [Stg0_skip_chan, Stg1_skip_chan, Stg2_skip_chan, 0].\n",
    "\n",
    "             # Let's trace channels again with this understanding:\n",
    "             # Feats list: [Bottleneck(embed_dim), Stg0_skip(proj to 64), Stg1_skip(proj to 128), Stg2_skip(proj to 256)]\n",
    "             # Feats channels: [embed_dim, 64, 128, 256]\n",
    "\n",
    "             # UnetDecoder2d `encoder_channels` param = [256, 128, 64, embed_dim]\n",
    "             # UnetDecoder2d `skip_channels` param = [64, 128, 256, 0]\n",
    "             # UnetDecoder2d `decoder_channels` param = [256, 128, 64, 32]\n",
    "\n",
    "             # UnetDecoder2d internal loop:\n",
    "             # res = [feats[0]] (feats[0] is Bottleneck, channel embed_dim)\n",
    "             # feats = feats[1:] (feats is now [Stg0_skip, Stg1_skip, Stg2_skip])\n",
    "\n",
    "             # Block 0:\n",
    "             # ic = input_channels_to_blocks[0] = encoder_channels[0] = 256\n",
    "             # sc = skip_channels_for_blocks[0] = skip_channels[0] = 64\n",
    "             # dc = decoder_channels[0] = 256\n",
    "             # Block input concat channels: res[-1] (embed_dim) + skip (feats[0]=Stg0_skip, chan 64).\n",
    "             # DecoderBlock2d expects in_channels=embed_dim, skip_channels=64. Actual call: Block0(res[-1], feats[0]).\n",
    "             # The `in_channels`, `skip_channels` params in DecoderBlock2d init must match the *actual* channels passed during forward.\n",
    "\n",
    "             # This setup is very confusing. The most likely scenario is that the original `UnetDecoder2d` expects `feats` list channels\n",
    "             # and uses `encoder_channels` and `skip_channels` params to define block input/skip channels *indirectly*.\n",
    "\n",
    "             # Let's assume the most standard U-Net structure where `feats` is [bottleneck, skip1, skip2, ...] (from coarsest to finest)\n",
    "             # And the decoder iterates from finest spatial up.\n",
    "             # Requires `feats = [Bottleneck, Skip_finest, Skip_mid, Skip_coarsest]` order in the list for the forward pass.\n",
    "             # And UnetDecoder2d block 0 uses Bottleneck and Skip_finest. Block 1 uses Block0 output and Skip_mid.\n",
    "\n",
    "             # Let's prepare `decoder_feats` in `HybridModel.forward` in the order [Bottleneck_2D, Skip_finest_2D, Skip_mid_2D, Skip_coarsest_2D]\n",
    "             # Skip_finest is from Stage 2 (index 2). Skip_mid from Stage 1 (index 1). Skip_coarsest from Stage 0 (index 0).\n",
    "             # `skip_features_2d` is [Stg0_skip, Stg1_skip, Stg2_skip]\n",
    "             # Corrected `decoder_feats`: [x_2d_bottleneck, skip_features_2d[2], skip_features_2d[1], skip_features_2d[0]]\n",
    "             # This matches `feats` order: [Bottleneck, Skip_Stg2, Skip_Stg1, Skip_Stg0]\n",
    "\n",
    "             # Then, UnetDecoder2d needs params matching this `feats` list structure.\n",
    "             # `encoder_channels` param: [Chan(feats[0]), Chan(feats[1]), Chan(feats[2]), Chan(feats[3])] = [embed_dim, 256, 128, 64] ? No.\n",
    "             # The internal logic of UnetDecoder2d is key. Based on its code:\n",
    "             # `res = [feats[0]]`. `feats = feats[1:]`. `skip = feats[i]`.\n",
    "             # If feats_in = [B, S1, S2, S3] (B=bottleneck, S1=coarsest, S2, S3=finest).\n",
    "             # res = [B]. feats = [S1, S2, S3].\n",
    "             # Block 0: skip=feats[0]=S1. Input=res[-1]=B. Block0(B, S1). res=[B, Output_B0].\n",
    "             # Block 1: skip=feats[1]=S2. Input=res[-1]=Output_B0. Block1(Output_B0, S2). res=[B, Output_B0, Output_B1].\n",
    "             # Block 2: skip=feats[2]=S3. Input=res[-1]=Output_B1. Block2(Output_B1, S3). res=[B, Output_B0, Output_B1, Output_B2].\n",
    "             # Block 3: skip=feats[3] (OOB). skip=None. Input=res[-1]=Output_B2. Block3(Output_B2, None). res=[..., Output_B3].\n",
    "\n",
    "             # The `feats` list should be ordered [Bottleneck, Skip_coarsest, Skip_mid, Skip_finest].\n",
    "             # Our `skip_features_2d` is already [Stg0_skip, Stg1_skip, Stg2_skip] (coarsest to finest).\n",
    "             # So `decoder_feats = [x_2d_bottleneck] + skip_features_2d` is indeed correct for this interpretation.\n",
    "\n",
    "             # Final structure seems consistent with this interpretation of UnetDecoder2d.\n",
    "\n",
    "             # The problem is the parameters `encoder_channels` and `skip_channels` passed to UnetDecoder2d init.\n",
    "             # They seem to define the *expected* input channels to blocks' concatenation.\n",
    "             # Let's follow the original base code's logic for these params:\n",
    "             # `encoder_channels_param` = `ecs` (reversed encoder chans) = [S3_chan, S2_chan, S1_chan, B_chan] (finest to coarsest)\n",
    "             # `skip_channels_param` = `ecs[1:] + [0]` = [S2_chan, S1_chan, B_chan, 0] (mid-finest to bottleneck)\n",
    "             # `decoder_channels_param` = [D1, D2, D3, D4] (output chans)\n",
    "\n",
    "             # This contradicts how feats are used in forward.\n",
    "             # Let's ignore the parameter names in UnetDecoder2d init and just pass the necessary channels for the blocks based on\n",
    "             # the standard U-Net flow that the forward pass *seems* to imply (despite the confusing implementation).\n",
    "\n",
    "             # Redefine UnetDecoder2d init and forward to be standard U-Net.\n",
    "             # Input: feats = [bottleneck, skip1, ..., skipN] (skip1 is coarsest, skipN finest). len = N+1.\n",
    "             # Decoder has N blocks (len decoder_channels).\n",
    "             # Block 0: input concat(feats[0], feats[N]). Output decoder_channels[0].\n",
    "             # Block 1: input concat(Output Block 0, feats[N-1]). Output decoder_channels[1].\n",
    "             # Block i: input concat(Output Block i-1, feats[N-i]). Output decoder_channels[i].\n",
    "\n",
    "             # This requires the skips in `feats` to be in reverse order (finest to coarsest).\n",
    "             # `feats = [Bottleneck, Skip_finest, Skip_mid, Skip_coarsest]`.\n",
    "             # So `decoder_feats = [x_2d_bottleneck] + skip_features_2d[::-1]`.\n",
    "\n",
    "             # Let's adjust UnetDecoder2d forward to use skips from feats[1:] reversed.\n",
    "             # This matches standard U-Net and makes channel logic clearer.\n",
    "\n",
    "             pass # Modified UnetDecoder2d forward below\n",
    "\n",
    "\n",
    "class UnetDecoder2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet decoder - Modified for standard U-Net skip connection order.\n",
    "    Input `feats` list is [bottleneck_feature, skip_coarsest, ..., skip_finest].\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        decoder_channels: tuple = (256, 128, 64, 32), # Output channels of decoder blocks\n",
    "        scale_factors: tuple = (2,2,2,2), # Upsampling factors for blocks\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = None,\n",
    "        upsample_mode: str = \"deconv\",\n",
    "        # Removed encoder_channels and skip_channels from init, will determine block channels internally\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder_channels = decoder_channels\n",
    "        self.scale_factors = scale_factors\n",
    "        self.norm_layer = norm_layer\n",
    "        self.attention_type = attention_type\n",
    "        self.upsample_mode = upsample_mode\n",
    "\n",
    "        # Number of decoder blocks\n",
    "        num_blocks = len(decoder_channels)\n",
    "        assert len(scale_factors) == num_blocks, \"scale_factors length must match decoder_channels length\"\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        # Block definitions are based on the channels provided in the forward pass `feats`.\n",
    "        # Need to determine input/skip channels dynamically in forward or pass them to init.\n",
    "        # Let's pass required channels during init based on the expected `feats` list structure.\n",
    "\n",
    "        # Expected feats list: [bottleneck, skip1, skip2, skip3] (skip1 coarsest, skip3 finest)\n",
    "        # Need to know channels and spatial sizes of these expected feats.\n",
    "        # This is computed in HybridModel.init and HybridModel._forward_unscaled.\n",
    "\n",
    "        # Let's pass the list of feature channels directly to UnetDecoder2d init.\n",
    "        # `feature_channels`: list of channels of the expected `feats` list [B_chan, S1_chan, S2_chan, ..., SN_chan]\n",
    "\n",
    "        # Modify UnetDecoder2d init signature\n",
    "        pass # Modified class signature below\n",
    "\n",
    "\n",
    "class UnetDecoder2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet decoder - Modified for standard U-Net skip connection order and channel definition.\n",
    "    Input `feats` list is [bottleneck_feature, skip_coarsest, ..., skip_finest].\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_channels: list[int], # Channels of the expected `feats` list [B_chan, S1_chan, ..., SN_chan]\n",
    "        decoder_channels: tuple = (256, 128, 64, 32), # Output channels of decoder blocks\n",
    "        scale_factors: tuple = (2,2,2,2), # Upsampling factors for blocks\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = None,\n",
    "        upsample_mode: str = \"deconv\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_channels = feature_channels # Channels of [bottleneck, skip1, ..., skipN]\n",
    "        self.decoder_channels = decoder_channels # Output channels of blocks\n",
    "        self.scale_factors = scale_factors\n",
    "        self.norm_layer = norm_layer\n",
    "        self.attention_type = attention_type\n",
    "        self.upsample_mode = upsample_mode\n",
    "\n",
    "        num_blocks = len(decoder_channels)\n",
    "        # Need num_blocks skips + 1 bottleneck = num_\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c0318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
