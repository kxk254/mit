{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c09a0d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "RUN_TRAIN = True # bfloat16 or float32 recommended\n",
    "RUN_VALID = False\n",
    "RUN_TEST  = False\n",
    "USE_DEVICE = 'GPU' #'CPU'  # 'GPU'\n",
    "\n",
    "import random\n",
    "import os, sys\n",
    "import time, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset # Use standard DataLoader/Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from _cfg import cfg\n",
    "from copy import deepcopy\n",
    "from types import MethodType\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from timm.models.convnext import ConvNeXtBlock\n",
    "\n",
    "from monai.networks.blocks import UpSample, SubpixelUpsample\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Install einops if not present\n",
    "try:\n",
    "    from einops import rearrange\n",
    "except ImportError:\n",
    "    !pip install einops -q\n",
    "    from einops import rearrange\n",
    "\n",
    "\n",
    "try: \n",
    "    import monai\n",
    "except: \n",
    "    !pip install --no-deps monai -q\n",
    "\n",
    "data_paths_str = \"./datasetfiles/FlatVel_A/data/*.npy\"\n",
    "label_paths_str = \"./datasetfiles/FlatVel_A/model/*.npy\"\n",
    "\n",
    "\n",
    "cfg= SimpleNamespace()\n",
    "cfg.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg.local_rank = 0\n",
    "cfg.seed = 123\n",
    "cfg.subsample = 100 #None\n",
    "\n",
    "cfg.file_pairs = list(zip(sorted(glob.glob(\"./datasetfiles/FlatVel_A/data/*.npy\")), sorted(glob.glob(\"./datasetfiles/FlatVel_A/model/*.npy\"))))\n",
    "# cfg.file_pairs = list(zip(data_paths, label_paths))\n",
    "data_paths = sorted(glob.glob(\"./datasetfiles/FlatVel_A/data/*.npy\"))\n",
    "label_paths = sorted(glob.glob(\"./datasetfiles/FlatVel_A/model/*.npy\"))\n",
    "cfg.backbone = \"convnext_small.fb_in22k_ft_in1k\"\n",
    "cfg.ema = True\n",
    "cfg.ema_decay = 0.99\n",
    "\n",
    "cfg.epochs = 4\n",
    "cfg.batch_size = 8  # 16\n",
    "cfg.batch_size_val = 8 # 16\n",
    "\n",
    "cfg.early_stopping = {\"patience\": 3, \"streak\": 0}\n",
    "cfg.logging_steps = 10\n",
    "\n",
    "# --- New Configs for Transformer and AMP ---\n",
    "cfg.num_transformer_layers = 1 # Number of transformer layers in the added module\n",
    "cfg.transformer_dropout = 0.1\n",
    "cfg.use_amp = True # Enable Automatic Mixed Precision\n",
    "cfg.amp_dtype = 'bfloat16' # 'bfloat16' or 'float16' - bfloat16 recommended if supported\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66918173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#     # --- Rewriting CustomDataset to return a time window ---\n",
    "#     # This assumes each file is long enough (>= 1000 time steps).\n",
    "#     # If files are shorter, padding or different sampling strategy is needed.\n",
    "#     # Let's assume files are long enough for now.\n",
    "#     # The index_map should store (file_idx, start_time_idx_of_window).\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         cfg,\n",
    "#         file_pairs,\n",
    "#         mode = \"train\",\n",
    "#         time_window_size = 1000 # New parameter for the temporal dimension\n",
    "#     ):\n",
    "#         self.cfg = cfg\n",
    "#         self.mode = mode\n",
    "#         self.file_pairs = file_pairs\n",
    "#         self.time_window_size = time_window_size\n",
    "\n",
    "#         self.data, self.labels = self._load_data_arrays()\n",
    "#         print(f\"[CustomDataset - init ]:self.data shape: {self.data[0].shape} | self.labels shape {self.labels[0].shape}\")\n",
    "\n",
    "#         if not self.data:\n",
    "#              self.total_samples = 0\n",
    "#              self.index_map = []\n",
    "#              print(f\"Dataset '{self.mode}' created with 0 total samples (no data loaded).\")\n",
    "#              return # Exit init if no data\n",
    "\n",
    "#         self.samples_per_file = self.data[0].shape[0] # Total time steps in a file\n",
    "#         self.geophones_per_file = self.data[0].shape[2] # Number of geophones\n",
    "#         print(f\"[CustomDataset - init ]:samples_per_file : {self.samples_per_file} | geophones_per_file  {self.geophones_per_file}\")\n",
    "\n",
    "#         # Calculate valid starting indices for windows\n",
    "#         # A window of size `time_window_size` starting at `i` is valid if `i + time_window_size <= samples_per_file`.\n",
    "#         valid_starts_per_file = self.samples_per_file - self.time_window_size + 1\n",
    "#         print(f\"[CustomDataset - init ]:valid_starts_per_file : {valid_starts_per_file} \")\n",
    "#         if valid_starts_per_file <= 0:\n",
    "#              print(f\"Warning: Files are too short ({self.samples_per_file} steps) for window size ({time_window_size}). Reducing window size or skipping file.\")\n",
    "#              # For now, let's skip files that are too short\n",
    "#              # A more robust approach would pad or handle short files explicitly\n",
    "#              valid_starts_per_file = 0 # No valid windows in this file\n",
    "\n",
    "#         # total_possible_windows = len(self.data) * max(0, valid_starts_per_file)\n",
    "\n",
    "#         # Subsample logic\n",
    "#         # subsample = getattr(self.cfg, \"subsample\", None)\n",
    "#         # self.total_samples = min(subsample, total_possible_windows) if subsample is not None else total_possible_windows\n",
    "#         self.total_samples = sum([d.shape[0] for d in self.data])\n",
    "#         print(f\"[CustomDataset - init ]:total_samples: {self.total_samples} | self data shape {self.data[0].shape} | length of self data {len(self.data)}\")\n",
    "\n",
    "#         # Build list of (file_idx, window_start_idx) pairs\n",
    "#         self.index_map = []\n",
    "#         for file_idx in range(len(self.data)):\n",
    "#             current_file_valid_starts = self.data[file_idx].shape[0] - self.time_window_size + 1\n",
    "#             if current_file_valid_starts > 0:\n",
    "#                 # For training, randomly sample start times for augmentation\n",
    "#                 if self.mode == \"train\":\n",
    "#                     # Sample start indices for this file up to a reasonable number, or all if few valid starts\n",
    "#                     num_starts_to_sample = min(current_file_valid_starts, 100) # Sample up to 100 windows per file during training\n",
    "#                     start_indices = np.random.choice(current_file_valid_starts, num_starts_to_sample, replace=False)\n",
    "#                 else: # For validation/test, use a fixed set of start indices (e.g., just the first valid window)\n",
    "#                     # Using just the first window might not be representative.\n",
    "#                     # Let's use a few evenly spaced windows or just the first.\n",
    "#                     # For simplicity, let's use just the first valid window for non-train modes.\n",
    "#                     start_indices = [0] # Use the first possible start index\n",
    "\n",
    "#                 for start_idx in start_indices:\n",
    "#                     self.index_map.append((file_idx, start_idx))\n",
    "#                     if len(self.index_map) >= self.total_samples:\n",
    "#                         break\n",
    "#             if len(self.index_map) >= self.total_samples:\n",
    "#                 break # Stop if total samples reached\n",
    "#         print(f\"[CustomDataset - init ]:len(self.index_map): {len(self.index_map)} \")\n",
    "\n",
    "#         if self.cfg.local_rank == 0:\n",
    "#              print(f\"Dataset '{self.mode}' created with {len(self.index_map)} total window samples.\")\n",
    "#              if len(self.data) > 0:\n",
    "#                  print(f\"Sample original file shape: {self.data[0].shape} (Time, Channels, Geophones)\")\n",
    "#                  # Expected sample shape is (Channels, Time, Geophones) -> (5, 1000, 70)\n",
    "#                  # Labels are (Map_Height, Map_Width) -> (70, 70) per time step.\n",
    "#                  # Need to check label dimension consistency. Labels should be (Samples_per_file, 70, 70).\n",
    "#                  if self.labels[0].shape != (self.samples_per_file, 70, 70):\n",
    "#                      print(f\"Error: Unexpected label shape {self.labels[0].shape}. Expected ({self.samples_per_file}, 70, 70).\")\n",
    "\n",
    "\n",
    "#     def _load_data_arrays(self, ):\n",
    "\n",
    "#         data_arrays = []\n",
    "#         label_arrays = []\n",
    "#         mmap_mode = \"r\" # Use read-only memory map\n",
    "\n",
    "#         # Only load a subset if subsample is much smaller than total possible samples\n",
    "#         # This avoids memory mapping huge amounts if only a few samples are needed.\n",
    "#         # However, the current logic always memory maps all files listed in file_pairs.\n",
    "#         # For simplicity, we'll keep mmap_mode=\"r\" as it's memory efficient for large files.\n",
    "\n",
    "#         for data_fpath, label_fpath in tqdm(\n",
    "#                         self.file_pairs, desc=f\"Loading {self.mode} data (mmap)\",\n",
    "#                         disable=self.cfg.local_rank != 0 or not self.file_pairs):\n",
    "#             try:\n",
    "#                 # Load the numpy arrays using memory mapping\n",
    "#                 arr = np.load(data_fpath, mmap_mode=mmap_mode)\n",
    "#                 lbl = np.load(label_fpath, mmap_mode=mmap_mode)\n",
    "#                 print(f\"[CustomDataset - load data arrays ]:arr shape: {arr.shape} | lbl shape: {lbl.shape}\")\n",
    "#                 lbl = np.squeeze(lbl, axis=1)\n",
    "#                 print(f\"[CustomDataset - load data arrays ]:lbl after squeeze shape: {lbl.shape}\")\n",
    "#                 # print(f\"Loaded {data_fpath}: {arr.shape}, {lbl.shape}\") # Too verbose\n",
    "#                 data_arrays.append(arr)\n",
    "#                 label_arrays.append(lbl)\n",
    "#             except FileNotFoundError:\n",
    "#                 print(f\"Error: File not found - {data_fpath} or {label_fpath}\", file=sys.stderr)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error loading file pair: {data_fpath}, {label_fpath}\", file=sys.stderr)\n",
    "#                 print(f\"Error: {e}\", file=sys.stderr)\n",
    "#                 continue\n",
    "\n",
    "#         if self.cfg.local_rank == 0 and self.file_pairs:\n",
    "#             print(f\"Finished loading {len(data_arrays)} file pairs for {self.mode} mode.\")\n",
    "\n",
    "#         return data_arrays, label_arrays\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         file_idx, start_time_idx = self.index_map[idx]\n",
    "#         print(f\"[CustomDataset - getitem ]:file_idx {file_idx} | file_idx{start_time_idx}\")\n",
    "\n",
    "#         # Access the data window and corresponding label (assuming label is the same map for all time steps in a file)\n",
    "#         # If the label changes per time step, the label loading/indexing needs adjustment.\n",
    "#         # The requested output is (B, 1, 70, 70), suggesting a single map per input window.\n",
    "#         # Let's assume the geological map (label) is constant for all time steps within a single data file.\n",
    "#         # So we load the label only once per file and return the same label for any window from that file.\n",
    "\n",
    "#         x_full_file = self.data[file_idx] # (Samples_per_file, Channels, Geophones) -> (S, C, W) = (S, 5, 70)\n",
    "#         y_full_file = self.labels[file_idx] # (Samples_per_file, Map_Height, Map_Width) -> (S, H', W') = (S, 70, 70)\n",
    "#         print(f\"[CustomDataset - getitem ]:x_full_file shape {x_full_file.shape} | y_full_file shape: {y_full_file.shape}\")\n",
    "#         # y_full_file = np.squeeze(y_full_file, axis=1)\n",
    "#         # print(f\"[CustomDataset - getitem ]:np.squeeze(y_full_file, axis=1)-shape: {y_full_file.shape}\")\n",
    "\n",
    "#         # Slice the temporal window from the data\n",
    "#         x_sample = x_full_file[start_time_idx : start_time_idx + self.time_window_size, ...] # (time_window_size, Channels, Geophones) -> (1000, 5, 70)\n",
    "#         print(f\"[CustomDataset - getitem ]:x_sample shape: {x_sample.shape}\")\n",
    "\n",
    "#         # Get the corresponding label. Assuming constant label per file, just take one slice (e.g., the first).\n",
    "#         # If labels vary per time step, this needs careful re-design.\n",
    "#         # Let's assume the label is the map *at the end* of the time window, or an aggregate.\n",
    "#         # Since the output is a single map (70, 70), let's assume the label file contains N identical copies of the map or N maps corresponding to time steps.\n",
    "#         # Given the simple dataset init, it's likely N identical maps. Let's take the map at the *end* of the window.\n",
    "#         y_sample = y_full_file[start_time_idx + self.time_window_size - 1, ...] # (Map_Height, Map_Width) -> (70, 70)\n",
    "#         print(f\"[CustomDataset - getitem ]:y_sample shape: {y_sample.shape}\")\n",
    "#         # Or if labels are constant per file: y_sample = y_full_file[0, ...]\n",
    "\n",
    "#         # --- Augmentations (apply to window and label) ---\n",
    "#         x_augmented = x_sample\n",
    "#         y_augmented = y_sample\n",
    "       \n",
    "#         if self.mode == \"train\":\n",
    "#              # Temporal flip (data only) - Flips the window sequence\n",
    "#              if np.random.random() < 0.5:\n",
    "#                   x_augmented = x_augmented[::-1, :, :] # Flip Time (dim 0)\n",
    "\n",
    "#              # Spatial flip (data and label) - Flips geophones/map width\n",
    "#              if np.random.random() < 0.5:\n",
    "#                   x_augmented = x_augmented[:, :, ::-1] # Flip Geophones (dim 2)\n",
    "#                   y_augmented = y_augmented[:, ::-1]    # Flip Map Width (dim 1) for label (70, 70)\n",
    "\n",
    "#         # make copies\n",
    "#         x_sample = x_augmented.copy()\n",
    "#         y_sample = y_augmented.copy()\n",
    "\n",
    "#         # Convert numpy to torch tensors\n",
    "#         # x_tensor needs to be (Channels, Time, Geophones) -> (5, 1000, 70)\n",
    "#         x_tensor = torch.from_numpy(x_sample).float() #.permute(1, 0, 2) # From (1000, 5, 70) to (5, 1000, 70)\n",
    "#         # y_tensor needs to be (1, Map_Height, Map_Width) -> (1, 70, 70)\n",
    "#         y_tensor = torch.from_numpy(y_sample).float().unsqueeze(0) # From (70, 70) to (1, 70, 70)\n",
    "\n",
    "\n",
    "#         return x_tensor, y_tensor\n",
    "\n",
    "#     def __len__(self, ):\n",
    "#         print(\"[CustomDataset - __len__]: self.total_samples: \", self.total_samples)\n",
    "#         return self.total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    # --- Rewriting CustomDataset to return a time window ---\n",
    "    # This assumes each file is long enough (>= 1000 time steps).\n",
    "    # If files are shorter, padding or different sampling strategy is needed.\n",
    "    # Let's assume files are long enough for now.\n",
    "    # The index_map should store (file_idx, start_time_idx_of_window).\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        file_pairs,\n",
    "        mode = \"train\",\n",
    "    ):\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.file_pairs = file_pairs\n",
    "\n",
    "        self.data, self.labels = self._load_data_arrays()\n",
    "        print(f\"[CustomDataset - init ]:self.data shape: {self.data[0].shape} | self.labels shape {self.labels[0].shape}\")\n",
    "\n",
    "        if not self.data:\n",
    "             self.total_samples = 0\n",
    "             self.index_map = []\n",
    "             print(f\"Dataset '{self.mode}' created with 0 total samples (no data loaded).\")\n",
    "             return # Exit init if no data\n",
    "\n",
    "        self.samples_per_file = self.data[0].shape[0] # Total time steps in a file\n",
    "        self.index_map = [] # Number of geophones\n",
    "        print(f\"[CustomDataset - init ]:samples_per_file : {self.samples_per_file}\")  #500\n",
    "\n",
    "        # Build list of (file_idx, window_start_idx) pairs\n",
    "        for file_idx, file_data in enumerate(self.data):\n",
    "            print(f\"[CustomDataset - init ]:file_idx : {file_idx} \") \n",
    "            for b_idx in range(self.samples_per_file):\n",
    "                self.index_map.append((file_idx, b_idx))\n",
    "        \n",
    "        self.total_samples = len(self.index_map)\n",
    "        print(f\"[CustomDataset - init ]:total_samples : {self.total_samples}\")  #500\n",
    "        print(f\"[CustomDataset - init ]:index_map : {self.index_map}\")  #500\n",
    "\n",
    "    def _load_data_arrays(self, ):\n",
    "\n",
    "        data_arrays = []\n",
    "        label_arrays = []\n",
    "        mmap_mode = \"r\" # Use read-only memory map\n",
    "\n",
    "        # Only load a subset if subsample is much smaller than total possible samples\n",
    "        # This avoids memory mapping huge amounts if only a few samples are needed.\n",
    "        # However, the current logic always memory maps all files listed in file_pairs.\n",
    "        # For simplicity, we'll keep mmap_mode=\"r\" as it's memory efficient for large files.\n",
    "\n",
    "        for data_fpath, label_fpath in tqdm(\n",
    "                        self.file_pairs, desc=f\"Loading {self.mode} data (mmap)\",\n",
    "                        disable=self.cfg.local_rank != 0 or not self.file_pairs):\n",
    "            try:\n",
    "                # Load the numpy arrays using memory mapping\n",
    "                arr = np.load(data_fpath, mmap_mode=mmap_mode)\n",
    "                lbl = np.load(label_fpath, mmap_mode=mmap_mode)\n",
    "                print(f\"[CustomDataset - load data arrays ]:arr shape: {arr.shape} | lbl shape: {lbl.shape}\")\n",
    "                lbl = np.squeeze(lbl, axis=1)\n",
    "                print(f\"[CustomDataset - load data arrays ]:lbl after squeeze shape: {lbl.shape}\")\n",
    "                # print(f\"Loaded {data_fpath}: {arr.shape}, {lbl.shape}\") # Too verbose\n",
    "                data_arrays.append(arr)\n",
    "                label_arrays.append(lbl)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: File not found - {data_fpath} or {label_fpath}\", file=sys.stderr)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file pair: {data_fpath}, {label_fpath}\", file=sys.stderr)\n",
    "                print(f\"Error: {e}\", file=sys.stderr)\n",
    "                continue\n",
    "\n",
    "        if self.cfg.local_rank == 0 and self.file_pairs:\n",
    "            print(f\"Finished loading {len(data_arrays)} file pairs for {self.mode} mode.\")\n",
    "\n",
    "        return data_arrays, label_arrays\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, start_time_idx = self.index_map[idx]\n",
    "        print(f\"[CustomDataset - getitem ]:file_idx {file_idx} | file_idx{start_time_idx}\")\n",
    "\n",
    "        # Access the data window and corresponding label (assuming label is the same map for all time steps in a file)\n",
    "        # If the label changes per time step, the label loading/indexing needs adjustment.\n",
    "        # The requested output is (B, 1, 70, 70), suggesting a single map per input window.\n",
    "        # Let's assume the geological map (label) is constant for all time steps within a single data file.\n",
    "        # So we load the label only once per file and return the same label for any window from that file.\n",
    "\n",
    "        x_sample = self.data[file_idx][start_time_idx] # (Samples_per_file, Channels, Geophones) -> (S, C, W) = (S, 5, 70)\n",
    "        y_sample = self.labels[file_idx][start_time_idx] # (Samples_per_file, Map_Height, Map_Width) -> (S, H', W') = (S, 70, 70)\n",
    "        print(f\"[CustomDataset - getitem ]:x_full_file shape {x_sample.shape} | y_full_file shape: {y_sample.shape}\")\n",
    "        # y_full_file = np.squeeze(y_full_file, axis=1)\n",
    "        # print(f\"[CustomDataset - getitem ]:np.squeeze(y_full_file, axis=1)-shape: {y_full_file.shape}\")\n",
    "\n",
    "        # --- Augmentations (apply to window and label) ---\n",
    "        x_augmented = x_sample.copy()\n",
    "        y_augmented = y_sample.copy()\n",
    "       \n",
    "        if self.mode == \"train\":\n",
    "            # Temporal flip (e.g., flipping across time dimension - dim 1)\n",
    "            if np.random.random() < 0.5:\n",
    "                x_augmented = x_augmented[:, ::-1, :].copy()  # Flip Time (dim 1) and copy\n",
    "\n",
    "            # Spatial flip (geophones and map width)\n",
    "            if np.random.random() < 0.5:\n",
    "                x_augmented = x_augmented[:, :, ::-1].copy()  # Flip Geophones (dim 2)\n",
    "                y_augmented = y_augmented[:, ::-1].copy()     # Flip Map Width (dim 1)\n",
    "\n",
    "        print(f\"[CustomDataset - getitem ]:x_augmented shape {x_augmented.shape} | y_augmented shape: {y_augmented.shape}\")\n",
    "        # Convert numpy to torch tensors\n",
    "        # x_tensor needs to be (Channels, Time, Geophones) -> (5, 1000, 70)\n",
    "        x_tensor = torch.from_numpy(x_augmented).float() #.permute(1, 0, 2) # From (1000, 5, 70) to (5, 1000, 70)\n",
    "        # y_tensor needs to be (1, Map_Height, Map_Width) -> (1, 70, 70)\n",
    "        y_tensor = torch.from_numpy(y_augmented).float().unsqueeze(0) # From (70, 70) to (1, 70, 70)\n",
    "        print(f\"[CustomDataset - getitem ]:x_tensor shape {x_tensor.shape} | y_tensor shape: {y_tensor.shape}\")\n",
    "\n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "    def __len__(self, ):\n",
    "        print(\"[CustomDataset - __len__]: self.total_samples: \", self.total_samples)\n",
    "        return self.total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5d1dd",
   "metadata": {},
   "source": [
    "**Original Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd072912",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "## EMA + Ensemble ##\n",
    "####################\n",
    "\n",
    "class ModelEMA(nn.Module):\n",
    "    def __init__(self, model, decay=0.99, device=None):\n",
    "        super().__init__()\n",
    "        # Create EMA model on CPU, then move to specified device\n",
    "        self.module = deepcopy(model).cpu() # Ensure deepcopy is on CPU first\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.device = device\n",
    "        if self.device is not None:\n",
    "            self.module.to(device=device)\n",
    "            # Move buffers like running_mean/var for BatchNorm if they exist (though we replace norms)\n",
    "            # This loop handles moving state_dict items\n",
    "            # print(f\"Moving EMA model to device: {device}\")\n",
    "            # for k, v in self.module.state_dict().items():\n",
    "            #     print(f\"  Moving {k} to {device}\")\n",
    "            #     self.module.state_dict()[k].copy_(v.to(device)) # This copy might be slow\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _update(self, model, update_fn):\n",
    "         # Ensure model params are on the target device (might be on different device than EMA if using DDP)\n",
    "         # We assume model is on cfg.device\n",
    "         model_state_dict = model.state_dict()\n",
    "         for ema_v, model_k in zip(self.module.state_dict().values(), model_state_dict.keys()):\n",
    "              model_v = model_state_dict[model_k].to(device=self.device) # Move model param to EMA device\n",
    "              ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)\n",
    "\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models).eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = None\n",
    "\n",
    "        for m in self.models:\n",
    "            # Ensure models are on the correct device and handle AMP if needed\n",
    "            with torch.autocast(device_type=x.device.type, dtype=torch.bfloat16 if x.device.type == 'cuda' and torch.cuda.is_bf16_supported() else torch.float16, enabled=x.device.type == 'cuda'): # Assuming AMP config is applied upstream or model handles it internally\n",
    "                logits = m(x)\n",
    "\n",
    "            if output is None:\n",
    "                output = logits\n",
    "            else:\n",
    "                output += logits\n",
    "\n",
    "        output /= len(self.models)\n",
    "        return output\n",
    "    \n",
    "#############\n",
    "## Decoder ##\n",
    "#############\n",
    "\n",
    "class ConvBnAct2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding: int = 0,\n",
    "        stride: int = 1,\n",
    "        norm_layer: nn.Module = nn.Identity, # Expecting a class, not an instance\n",
    "        act_layer: nn.Module = nn.ReLU,     # Expecting a class\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv= nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=False, # Usually False when using BatchNorm/InstanceNorm\n",
    "        )\n",
    "        # Instantiate norm_layer here\n",
    "        self.norm = norm_layer(out_channels) if norm_layer != nn.Identity else nn.Identity()\n",
    "        # Instantiate act_layer here\n",
    "        self.act= act_layer(inplace=True) if act_layer != nn.Identity else nn.Identity() # Handle Identity activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SCSEModule2d(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
    "            nn.GELU(), # Use GELU consistent with model\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sSE = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 1, 1),\n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.cSE(x) + x * self.sSE(x)\n",
    "\n",
    "class Attention2d(nn.Module):\n",
    "    def __init__(self, name, **params):\n",
    "        super().__init__()\n",
    "        if name is None or name.lower() == \"identity\":\n",
    "            self.attention = nn.Identity() # Use nn.Identity for clarity\n",
    "        elif name.lower() == \"scse\":\n",
    "            self.attention = SCSEModule2d(**params)\n",
    "        else:\n",
    "            raise ValueError(\"Attention {} is not implemented\".format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x)\n",
    "\n",
    "class DecoderBlock2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels,\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = None,\n",
    "        intermediate_conv: bool = False,\n",
    "        upsample_mode: str = \"deconv\",\n",
    "        scale_factor: int = 2,\n",
    "        act_layer: nn.Module = nn.GELU, # Use GELU consistent with model\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Upsample block\n",
    "        if upsample_mode == \"pixelshuffle\":\n",
    "            # SubpixelUpsample handles channel adjustment internally\n",
    "            self.upsample= SubpixelUpsample(\n",
    "                spatial_dims= 2,\n",
    "                in_channels= in_channels,\n",
    "                scale_factor= scale_factor,\n",
    "            )\n",
    "            upsample_out_channels = in_channels // (scale_factor ** 2) # Calculate output channels\n",
    "        else: # \"deconv\", \"bicubic\", \"bilinear\", \"nearest\", \"avgpool\", \"nontrainable\"\n",
    "             # For \"deconv\", need to set out_channels explicitly if different from in_channels\n",
    "             # For others, out_channels is same as in_channels\n",
    "             # Assuming deconv is primary mode and might change channels\n",
    "            upsample_out_channels = in_channels if upsample_mode != \"deconv\" else in_channels # Let's keep it simple for now, assume deconv might change channels or leave them same\n",
    "            # Re-check MONAI UpSample source: it takes out_channels for deconv\n",
    "            if upsample_mode == \"deconv\":\n",
    "                 # Deconv output channels usually match input channels for skip connection concatenation\n",
    "                 # Let's make deconv out_channels explicit: in_channels -> out_channels\n",
    "                 # But wait, the total input to conv1 is upsampled_channels + skip_channels.\n",
    "                 # If upsampling changes channels, this needs to be handled.\n",
    "                 # Standard practice is upsampling brings feature map to spatial size of skip, channels might match input to block or be related to skip.\n",
    "                 # Let's assume for 'deconv', MONAI's UpSample can handle channel adjustment or keeps it `in_channels`.\n",
    "                 # Re-reading MONAI UpSample: `out_channels` is only used for \"nontrainable\" upsample. For \"deconv\", it infers.\n",
    "                 # Okay, let's assume upsample output channels are `in_channels` for deconv too, before concatenation.\n",
    "                 self.upsample = UpSample(\n",
    "                    spatial_dims= 2,\n",
    "                    in_channels= in_channels,\n",
    "                    scale_factor= scale_factor,\n",
    "                    mode= upsample_mode,\n",
    "                )\n",
    "                 upsample_out_channels = in_channels # Assuming deconv keeps channels same for now\n",
    "\n",
    "            else: # nontrainable modes\n",
    "                 self.upsample = UpSample(\n",
    "                    spatial_dims= 2,\n",
    "                    in_channels= in_channels,\n",
    "                    out_channels= in_channels, # Non-trainable modes need explicit out_channels (same as in_channels for skip)\n",
    "                    scale_factor= scale_factor,\n",
    "                    mode= upsample_mode,\n",
    "                 )\n",
    "                 upsample_out_channels = in_channels\n",
    "\n",
    "        if intermediate_conv:\n",
    "            k= 3\n",
    "            # Intermediate conv applies to the skip connection OR the upsampled features if skip is None\n",
    "            # If skip is not None, it applies to skip_channels -> skip_channels\n",
    "            # If skip is None, it applies to upsampled_out_channels -> upsampled_out_channels\n",
    "            intermediate_in_channels = skip_channels if skip_channels != 0 else upsample_out_channels\n",
    "            intermediate_out_channels = intermediate_in_channels # Usually keeps channels same\n",
    "            self.intermediate_conv = nn.Sequential(\n",
    "                ConvBnAct2d(intermediate_in_channels, intermediate_out_channels, k, k//2, norm_layer=norm_layer, act_layer=act_layer),\n",
    "                ConvBnAct2d(intermediate_out_channels, intermediate_out_channels, k, k//2, norm_layer=norm_layer, act_layer=act_layer),\n",
    "                )\n",
    "        else:\n",
    "            self.intermediate_conv= None\n",
    "\n",
    "        # Input to the first conv after upsampling and concatenation is upsampled_out_channels + skip_channels\n",
    "        conv1_in_channels = upsample_out_channels + skip_channels\n",
    "\n",
    "        self.attention1 = Attention2d(\n",
    "            name= attention_type,\n",
    "            in_channels= conv1_in_channels,\n",
    "            )\n",
    "\n",
    "        self.conv1 = ConvBnAct2d(\n",
    "            conv1_in_channels,\n",
    "            out_channels,\n",
    "            kernel_size= 3,\n",
    "            padding= 1,\n",
    "            norm_layer= norm_layer,\n",
    "            act_layer= act_layer,\n",
    "        )\n",
    "\n",
    "        self.conv2 = ConvBnAct2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size= 3,\n",
    "            padding= 1,\n",
    "            norm_layer= norm_layer,\n",
    "            act_layer= act_layer,\n",
    "        )\n",
    "        self.attention2 = Attention2d(\n",
    "            name= attention_type,\n",
    "            in_channels= out_channels,\n",
    "            )\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        # x is the input from the previous decoder block (or deepest encoder feature)\n",
    "        # skip is the feature map from the corresponding encoder stage\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        if self.intermediate_conv is not None:\n",
    "            if skip is not None:\n",
    "                # Apply intermediate conv to skip connection\n",
    "                skip = self.intermediate_conv(skip)\n",
    "            else:\n",
    "                 # Should not happen in a standard U-Net skip connection flow, but handle if used differently\n",
    "                 # If skip is None, intermediate conv might apply to upsampled feature map\n",
    "                 # The original code's intermediate conv logic is a bit ambiguous here.\n",
    "                 # Let's stick to the common case: intermediate_conv applies to skip.\n",
    "                 pass # Do nothing if intermediate_conv exists but skip is None\n",
    "\n",
    "        if skip is not None:\n",
    "            # Ensure spatial sizes match before concatenation\n",
    "            # print(f\"Upsampled shape: {x.shape}, Skip shape: {skip.shape}\")\n",
    "            # If shapes don't match exactly (due to padding/strides), need cropping/padding\n",
    "            # Assuming timm/monai handle this correctly or input sizes are compatible\n",
    "            if x.shape[-2:] != skip.shape[-2:]:\n",
    "                 # Center crop the larger one or pad the smaller one\n",
    "                 # Let's center crop the skip connection if it's larger\n",
    "                 target_h, target_w = x.shape[-2:]\n",
    "                 skip_h, skip_w = skip.shape[-2:]\n",
    "                 if skip_h > target_h or skip_w > target_w:\n",
    "                     # print(f\"Warning: Cropping skip connection from {skip.shape} to match {x.shape}\")\n",
    "                     # Calculate cropping amounts\n",
    "                     crop_h = max(0, skip_h - target_h)\n",
    "                     crop_w = max(0, skip_w - target_w)\n",
    "                     # Apply center crop\n",
    "                     skip = skip[:, :, crop_h // 2 : crop_h // 2 + target_h, crop_w // 2 : crop_w // 2 + target_w]\n",
    "                 elif skip_h < target_h or skip_w < target_w:\n",
    "                      # print(f\"Warning: Padding skip connection from {skip.shape} to match {x.shape}\")\n",
    "                      # Calculate padding amounts\n",
    "                      pad_h = max(0, target_h - skip_h)\n",
    "                      pad_w = max(0, target_w - skip_w)\n",
    "                      # Apply zero padding\n",
    "                      skip = F.pad(skip, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention1(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetDecoder2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet decoder based on timm features_only output order (deep to shallow).\n",
    "    Source: https://arxiv.org/abs/1505.04597\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: list[int], # Expects channels from deep to shallow [C_s3, C_s2, C_s1, C_s0]\n",
    "        decoder_channels: list = (256, 128, 64, 32), # Deep to shallow decoder channels\n",
    "        scale_factors: list = (2,2,2,2), # Upsample scale factor for each block\n",
    "        norm_layer: nn.Module = nn.InstanceNorm2d, # Use InstanceNorm consistent with encoder replacement\n",
    "        attention_type: str = None,\n",
    "        intermediate_conv: bool = False,\n",
    "        upsample_mode: str = \"deconv\",\n",
    "        act_layer: nn.Module = nn.GELU, # Use GELU\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Validate input channel counts\n",
    "        num_encoder_stages = len(encoder_channels)\n",
    "        num_decoder_blocks = len(decoder_channels)\n",
    "\n",
    "        if num_decoder_blocks != num_encoder_stages:\n",
    "             # The first encoder channel is the input to the first decoder block\n",
    "             # The rest are skips. So we need num_decoder_blocks = num_encoder_stages.\n",
    "             # If encoder_channels = [C_s3, C_s2, C_s1, C_s0] (4 stages)\n",
    "             # Decoder needs 4 blocks: s3->skip(s2), s2_dec->skip(s1), s1_dec->skip(s0), s0_dec->skip(None or final).\n",
    "             # The original code sets decoder_channels to (256, 128, 64, 32).\n",
    "             # If len(encoder_channels) == 4, it uses decoder_channels[1:] = (128, 64, 32). This implies 3 decoder blocks?\n",
    "             # This seems mismatched. Let's align them: num_decoder_blocks = num_encoder_stages.\n",
    "             # And the decoder_channels should specify the output channels of each decoder block (deep to shallow).\n",
    "             # Let's correct the logic based on a standard U-Net structure.\n",
    "             # Input to Decoder: [s_N, s_{N-1}, ..., s_0] where s_N is deepest.\n",
    "             # Block 1 input: s_N, skip s_{N-1}, output d_{N-1}\n",
    "             # Block 2 input: d_{N-1}, skip s_{N-2}, output d_{N-2}\n",
    "             # ...\n",
    "             # Block N input: d_1, skip s_0, output d_0\n",
    "             # Decoder output channels: [d_{N-1}, d_{N-2}, ..., d_0] (deepest block output -> shallowest block output)\n",
    "             # The `decoder_channels` parameter should be the list of *output* channels for each block (deep to shallow).\n",
    "             # If `decoder_channels` = (256, 128, 64, 32), these are the output channels of the 4 blocks.\n",
    "             # Block 0 (deepest): Input C_s3, Skip C_s2, Output 256\n",
    "             # Block 1: Input 256, Skip C_s1, Output 128\n",
    "             # Block 2: Input 128, Skip C_s0, Output 64\n",
    "             # Block 3: Input 64, Skip 0, Output 32 (assuming 4 encoder stages, the last block might not have a skip or use a placeholder)\n",
    "\n",
    "             if num_decoder_blocks == num_encoder_stages - 1:\n",
    "                 # Special case: if decoder_channels is one shorter, it might skip the first (deepest) block output channel?\n",
    "                 # No, the original code has `if len(encoder_channels) == 4: decoder_channels= decoder_channels[1:]`\n",
    "                 # This suggests if there are 4 encoder features, the decoder *channels* list is truncated.\n",
    "                 # If ecs = [C_s3, C_s2, C_s1, C_s0], len=4. decoder_channels becomes (128, 64, 32), len=3.\n",
    "                 # This would mean 3 decoder blocks. Where does s3 go? It would be the input to the first block.\n",
    "                 # Block 0: Input C_s3, Skip C_s2, Output 128\n",
    "                 # Block 1: Input 128, Skip C_s1, Output 64\n",
    "                 # Block 2: Input 64, Skip C_s0, Output 32\n",
    "                 # This seems plausible for a 4-stage encoder outputting 4 features.\n",
    "                 print(f\"Warning: Number of decoder blocks ({num_decoder_blocks}) does not match encoder stages ({num_encoder_stages}). Adjusting decoder channels.\")\n",
    "                 # Let's follow the original code's implicit intent: if 4 encoder stages, use 3 decoder blocks with channels (128, 64, 32)\n",
    "                 # The input to the first decoder block is the output of the deepest encoder stage (feats[0] / s3).\n",
    "                 # The skips are feats[1], feats[2], feats[3] (s2, s1, s0).\n",
    "\n",
    "                 # Input channels for decoder blocks:\n",
    "                 # Block 0: encoder_channels[0] (C_s3)\n",
    "                 # Block 1: decoder_channels[0] (128)\n",
    "                 # Block 2: decoder_channels[1] (64)\n",
    "                 # Block 3 (if exists): decoder_channels[2] (32)\n",
    "                 in_channels = [encoder_channels[0]] + list(decoder_channels[:-1])\n",
    "\n",
    "                 # Skip channels for decoder blocks:\n",
    "                 # Block 0: encoder_channels[1] (C_s2)\n",
    "                 # Block 1: encoder_channels[2] (C_s1)\n",
    "                 # Block 2: encoder_channels[3] (C_s0)\n",
    "                 # Block 3: 0 (no skip)\n",
    "                 skip_channels_list = encoder_channels[1:] + [0] # C_s2, C_s1, C_s0, 0\n",
    "\n",
    "                 # The number of blocks is determined by the length of decoder_channels list\n",
    "                 actual_decoder_channels = list(decoder_channels)\n",
    "                 # If len(encoder_channels) == 4 and original decoder_channels length is 4\n",
    "                 # The original code does `decoder_channels = decoder_channels[1:]` -> len becomes 3\n",
    "                 # This implies 3 decoder blocks (128, 64, 32) for 4 encoder stages [s3, s2, s1, s0].\n",
    "                 # Block 0: input C_s3, skip C_s2, output 128\n",
    "                 # Block 1: input 128, skip C_s1, output 64\n",
    "                 # Block 2: input 64, skip C_s0, output 32\n",
    "                 # This is a bit unconventional. Let's make the number of decoder channels *explicitly* define the blocks.\n",
    "                 # If encoder has N stages, timm gives N features. Decoder should have N blocks.\n",
    "                 # Let's define decoder_channels as a list of output channels for N blocks.\n",
    "                 # If `encoder_channels = [C_s3, C_s2, C_s1, C_s0]` (len 4)\n",
    "                 # and `decoder_channels_out = (256, 128, 64, 32)` (len 4)\n",
    "                 # Block 0: input C_s3, skip C_s2, output 256\n",
    "                 # Block 1: input 256, skip C_s1, output 128\n",
    "                 # Block 2: input 128, skip C_s0, output 64\n",
    "                 # Block 3: input 64, skip 0, output 32\n",
    "                 # This makes more sense. Let's redefine based on this standard structure.\n",
    "\n",
    "             # Redefined structure:\n",
    "             # encoder_channels: [C_s3, C_s2, C_s1, C_s0] (deep to shallow)\n",
    "             # decoder_channels_out: [DC0, DC1, DC2, DC3] (deep to shallow block outputs)\n",
    "             # Number of blocks = len(encoder_channels) = len(decoder_channels_out)\n",
    "             # Block 0: input encoder_channels[0] (C_s3), skip encoder_channels[1] (C_s2), output decoder_channels_out[0] (DC0)\n",
    "             # Block i: input decoder_channels_out[i-1] (DC_{i-1}), skip encoder_channels[i+1] (C_{s_{N-(i+1)}}), output decoder_channels_out[i] (DC_i)\n",
    "             # Last Block (N-1): input decoder_channels_out[N-2] (DC_{N-2}), skip encoder_channels[N] (C_s0), output decoder_channels_out[N-1] (DC_{N-1})\n",
    "             # The skip for the *last* block (corresponding to the shallowest encoder feature) is the 2nd to last element in the encoder_channels list\n",
    "             # Example: enc=[s3, s2, s1, s0], dec_out=[d0, d1, d2, d3]\n",
    "             # Block 0: in s3, skip s2, out d0\n",
    "             # Block 1: in d0, skip s1, out d1\n",
    "             # Block 2: in d1, skip s0, out d2\n",
    "             # Block 3: in d2, skip NONE, out d3 -> This structure doesn't match typical U-Net where last block connects to s0.\n",
    "             # A typical U-Net decoder block takes the *previous decoder output* and the *corresponding encoder skip*.\n",
    "             # Input to decoder block i: output of block i-1. Skip for block i: feature from encoder stage i.\n",
    "             # If encoder features are [s0, s1, s2, s3] (shallow to deep)\n",
    "             # Block 0 (deepest): input s3, skip s2, output d2\n",
    "             # Block 1: input d2, skip s1, output d1\n",
    "             # Block 2: input d1, skip s0, output d0\n",
    "             # This requires encoder features in *shallow to deep* order for skips.\n",
    "\n",
    "             # Let's reconcile with the original code's `ecs= [_[\"num_chs\"] for _ in self.backbone.feature_info][::-1]`\n",
    "             # This means `ecs` is [C_s3, C_s2, C_s1, C_s0].\n",
    "             # The `forward` takes `feats` and does `res= [feats[0]]`, then loops using `feats[i]` as skip.\n",
    "             # This implies `feats` is also [s3, s2, s1, s0].\n",
    "             # `res=[s3]`\n",
    "             # loop i=0: `skip=feats[0]=s3`. `b(s3, s3)`. WRONG.\n",
    "             # loop i=1: `skip=feats[1]=s2`. `b(output_block0, s2)`. This is the correct skip connection logic.\n",
    "\n",
    "             # Correct UnetDecoder2d forward logic matching original code's loop structure:\n",
    "             # Takes `feats: list[torch.Tensor]` which is [s3, s2, s1, s0] (deep to shallow).\n",
    "             # `x = feats[0]` (s3) # Input to the first decoder block\n",
    "             # `res = [x]` # Store the initial input (s3) - maybe remove this?\n",
    "             # Loop through decoder blocks i=0 to num_blocks-1\n",
    "             # Block i takes input from block i-1's output (or s3 for block 0)\n",
    "             # Block i takes skip from encoder stage i+1 in the `feats` list\n",
    "             # (feats[1] for block 0, feats[2] for block 1, etc.)\n",
    "\n",
    "             # Let's simplify the decoder structure in `Net` and Decoder `forward`:\n",
    "             # Net:\n",
    "             # `feats = self.backbone(x)` # [s3, s2, s1, s0]\n",
    "             # `trans_s3 = self.transformer_module(feats[0])` # Process s3\n",
    "             # `decoder_input = trans_s3` # Deepest feature is input to first decoder block\n",
    "             # `decoder_skips = feats[1:]` # s2, s1, s0 are the skips\n",
    "             # `decoder_out_list = self.decoder(decoder_input, decoder_skips)` # Pass input and skips explicitly\n",
    "             # `seg_head_input = decoder_out_list[-1]`\n",
    "\n",
    "             # Modified UnetDecoder2d `__init__`:\n",
    "             # `encoder_channels_deep_to_shallow`: [C_s3, C_s2, C_s1, C_s0]\n",
    "             # `decoder_channels_out`: [DC0, DC1, DC2, DC3] (output channels of blocks, deep to shallow)\n",
    "             # num_blocks = len(decoder_channels_out)\n",
    "             # Input channels for blocks: [encoder_channels_deep_to_shallow[0], DC0, DC1, DC2]\n",
    "             # Skip channels for blocks: [encoder_channels_deep_to_shallow[1], encoder_channels_deep_to_shallow[2], encoder_channels_deep_to_shallow[3], 0] # for 4 stages\n",
    "\n",
    "             if num_decoder_blocks != num_encoder_stages:\n",
    "                 print(f\"Warning: Number of decoder block outputs ({num_decoder_blocks}) does not match number of encoder stages ({num_encoder_stages}). Adjusting number of decoder blocks to match encoder stages.\")\n",
    "                 # Assume decoder_channels specifies output channels for each block corresponding to each encoder stage feature *as a skip*.\n",
    "                 # The number of decoder blocks should equal the number of encoder features used *as skips* + 1 (for the deepest feature).\n",
    "                 # If encoder has N stages (0 to N-1), features [s_N-1, ..., s_0] are returned.\n",
    "                 # s_N-1 is deepest (input to block 0). s_0 is shallowest (skip for block N-1).\n",
    "                 # Number of decoder blocks = N.\n",
    "                 # Let's ensure decoder_channels has length N.\n",
    "                 if len(decoder_channels) != num_encoder_stages:\n",
    "                     print(f\"Adjusting decoder_channels length from {len(decoder_channels)} to {num_encoder_stages}.\")\n",
    "                     # This requires deciding how to adjust channels if the lengths don't match.\n",
    "                     # For simplicity, let's require `len(decoder_channels) == len(encoder_channels)`.\n",
    "                     raise ValueError(f\"Number of decoder_channels ({len(decoder_channels)}) must match number of encoder stages ({num_encoder_stages}) for this decoder structure.\")\n",
    "\n",
    "\n",
    "             self.decoder_channels = decoder_channels # Store output channels of blocks\n",
    "\n",
    "             # Input channels for blocks: C_s3 (for block 0), then DC0, DC1, DC2...\n",
    "             in_channels = [encoder_channels[0]] + list(decoder_channels[:-1])\n",
    "\n",
    "             # Skip channels for blocks: C_s2, C_s1, C_s0, ... (last skip might be 0)\n",
    "             # Skips are feats[1], feats[2], ..., feats[N-1]. Last block skip is 0.\n",
    "             skip_channels_list = encoder_channels[1:] + [0]\n",
    "\n",
    "             self.blocks = nn.ModuleList()\n",
    "             for i in range(num_decoder_blocks):\n",
    "                  self.blocks.append(\n",
    "                      DecoderBlock2d(\n",
    "                          in_channels= in_channels[i],\n",
    "                          skip_channels= skip_channels_list[i], # Corresponding skip channel\n",
    "                          out_channels= decoder_channels[i], # Output channel of this block\n",
    "                          norm_layer= norm_layer,\n",
    "                          attention_type= attention_type,\n",
    "                          intermediate_conv= intermediate_conv,\n",
    "                          upsample_mode= upsample_mode,\n",
    "                          scale_factor= scale_factors[i],\n",
    "                      )\n",
    "                  )\n",
    "\n",
    "\n",
    "    # Corrected UnetDecoder2d forward signature and logic:\n",
    "    # Takes the deepest feature map as main input, and the rest as a list of skips\n",
    "    def forward(self, deep_feature: torch.Tensor, skip_features: list[torch.Tensor]):\n",
    "        # deep_feature: output of the deepest encoder stage (s3 in our example)\n",
    "        # skip_features: list of shallower encoder features [s2, s1, s0]\n",
    "\n",
    "        x = deep_feature\n",
    "        decoder_outputs = [x] # Store intermediate decoder block outputs (optional, but maybe useful)\n",
    "\n",
    "        # Loop through decoder blocks\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            # The skip connection for block i comes from skip_features[i]\n",
    "            skip = skip_features[i] if i < len(skip_features) else None # Handle case where last block has no skip\n",
    "            x = block(x, skip=skip)\n",
    "            decoder_outputs.append(x)\n",
    "\n",
    "        # Return all block outputs or just the final one depending on use case\n",
    "        # The SegmentationHead takes the *last* decoder block output.\n",
    "        # Let's return the list of outputs as before, but the last one is the final result.\n",
    "        return decoder_outputs # [input_s3, output_block0, output_block1, ...]\n",
    "\n",
    "\n",
    "class SegmentationHead2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        scale_factor: tuple[int] | int = 1, # Use tuple or int for clarity\n",
    "        kernel_size: int = 3,\n",
    "        mode: str = \"nontrainable\",\n",
    "        norm_layer: nn.Module = nn.Identity, # Add norm/act options consistent with blocks\n",
    "        act_layer: nn.Module = nn.Identity,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv= ConvBnAct2d( # Use the standard ConvBnAct2d block\n",
    "            in_channels, out_channels, kernel_size= kernel_size,\n",
    "            padding= kernel_size//2, norm_layer=norm_layer, act_layer=act_layer,\n",
    "        )\n",
    "        # Use a tuple scale_factor even if it's 1,1 for 2D\n",
    "        if isinstance(scale_factor, int):\n",
    "             scale_factor = (scale_factor, scale_factor)\n",
    "\n",
    "        # Only add upsample if scale_factor > 1\n",
    "        if scale_factor[0] > 1 or scale_factor[1] > 1:\n",
    "             self.upsample = UpSample(\n",
    "                 spatial_dims= 2,\n",
    "                 in_channels= out_channels, # Upsample operates on the output of the conv\n",
    "                 out_channels= out_channels,\n",
    "                 scale_factor= scale_factor,\n",
    "                 mode= mode,\n",
    "             )\n",
    "        else:\n",
    "             self.upsample = nn.Identity() # No upsampling needed\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.upsample(x) # Identity if scale_factor is 1\n",
    "        return x\n",
    "\n",
    "\n",
    "#############\n",
    "## Encoder ##\n",
    "#############\n",
    "\n",
    "# The original _convnext_block_forward modification might not be needed\n",
    "# if we are using features_only=True and modifying the stem manually.\n",
    "# Let's remove it and the replace_forwards call.\n",
    "# def _convnext_block_forward(self, x):\n",
    "#     # ... (original timm code) ...\n",
    "#     pass\n",
    "\n",
    "\n",
    "class TemporalSpatialTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies transformer attention along temporal and spatial dimensions\n",
    "    of a 4D feature map (B, C, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, seq_len_h: int, seq_len_w: int, num_layers: int = 1, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_len_h = seq_len_h\n",
    "        self.seq_len_w = seq_len_w\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Learnable 1D positional embeddings for Height and Width dimensions\n",
    "        self.pos_embed_h = nn.Parameter(torch.randn(1, 1, seq_len_h, 1, embed_dim)) # Shape (1, 1, H, 1, E)\n",
    "        self.pos_embed_w = nn.Parameter(torch.randn(1, 1, 1, seq_len_w, embed_dim)) # Shape (1, 1, 1, W, E)\n",
    "        # Initialize Positional Embeddings\n",
    "        trunc_normal_(self.pos_embed_h, std=.02)\n",
    "        trunc_normal_(self.pos_embed_w, std=.02)\n",
    "\n",
    "\n",
    "        # Transformer Encoder Layers\n",
    "        # Use nn.TransformerEncoder which is a stack of TransformerEncoderLayer\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=8, # Typical number of heads, could make configurable\n",
    "            dim_feedforward=embed_dim * 4, # Typical feedforward dim\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\", # Use GELU\n",
    "            batch_first=False, # Transformer expects (Sequence, Batch, Embed)\n",
    "            norm_first=True, # Pre-LayerNorm is common in recent transformers\n",
    "        )\n",
    "        self.temporal_transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "        self.spatial_transformer = nn.TransformerEncoder(transformer_layer, num_layers=num_layers) # Share layers or separate? Separate for clarity.\n",
    "        # If sharing, need to be careful about state. Let's make copies of the layers.\n",
    "        transformer_layers_h = [deepcopy(transformer_layer) for _ in range(num_layers)]\n",
    "        transformer_layers_w = [deepcopy(transformer_layer) for _ in range(num_layers)]\n",
    "        self.temporal_transformer = nn.TransformerEncoder(nn.ModuleList(transformer_layers_h), num_layers=num_layers)\n",
    "        self.spatial_transformer = nn.TransformerEncoder(nn.ModuleList(transformer_layers_w), num_layers=num_layers)\n",
    "\n",
    "\n",
    "        # Add LayerNorm before the transformer blocks if norm_first=False in TransformerEncoderLayer\n",
    "        # Since norm_first=True, LayerNorm is handled within the layer/encoder.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        assert C == self.embed_dim, f\"Input channel mismatch: {C} != {self.embed_dim}\"\n",
    "        # The input H and W should match the expected sequence lengths,\n",
    "        # but they might differ slightly due to padding/striding in the stem/backbone.\n",
    "        # Let's pad the input spatially if needed to match expected seq_len_h/w for PE and transformer.\n",
    "        if H != self.seq_len_h or W != self.seq_len_w:\n",
    "             # print(f\"Warning: Feature map spatial size ({H},{W}) does not match expected ({self.seq_len_h},{self.seq_len_w}) for Transformer. Padding/Cropping.\")\n",
    "             # Calculate padding/cropping\n",
    "             pad_h = max(0, self.seq_len_h - H)\n",
    "             pad_w = max(0, self.seq_len_w - W)\n",
    "             crop_h = max(0, H - self.seq_len_h)\n",
    "             crop_w = max(0, W - self.seq_len_w)\n",
    "\n",
    "             if pad_h > 0 or pad_w > 0:\n",
    "                 x = F.pad(x, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
    "             if crop_h > 0 or crop_w > 0:\n",
    "                  x = x[:, :, crop_h // 2 : crop_h // 2 + self.seq_len_h, crop_w // 2 : crop_w // 2 + self.seq_len_w]\n",
    "\n",
    "             # Update H, W after padding/cropping\n",
    "             B, C, H, W = x.shape\n",
    "             assert H == self.seq_len_h and W == self.seq_len_w, \"Padding/cropping failed to match dimensions.\"\n",
    "\n",
    "\n",
    "        identity = x # Residual connection\n",
    "\n",
    "\n",
    "        # Apply positional embeddings (broadcasts over B and the missing spatial dimension)\n",
    "        # PE shape (1, 1, H, 1, E) -> (B, C, H, W) needs (B, C, H, W) PE\n",
    "        # PE_h (1, 1, H, 1, E) should be (1, E, H, 1) for 4D tensor adding\n",
    "        # PE_w (1, 1, 1, W, E) should be (1, E, 1, W) for 4D tensor adding\n",
    "        # Let's reshape PE params to (1, E, H, 1) and (1, E, 1, W)\n",
    "        pos_embed_h_4d = self.pos_embed_h.squeeze(dim=3).permute(0, 4, 2, 1) # (1, E, H, 1)\n",
    "        pos_embed_w_4d = self.pos_embed_w.squeeze(dim=2).permute(0, 4, 1, 2) # (1, E, 1, W)\n",
    "\n",
    "\n",
    "        # --- Temporal Attention ---\n",
    "        # Input (B, C, H, W). Treat each (C, W) slice as a batch element, sequence length H.\n",
    "        # Reshape to (H, B*W, C) for Transformer\n",
    "        x_temp = x + pos_embed_h_4d.to(x.dtype) # Add PE for H dimension (broadcasts over B, W)\n",
    "        x_temp = x_temp.permute(2, 0, 3, 1).reshape(H, B * W, C) # (H, B*W, C)\n",
    "        x_temp = self.temporal_transformer(x_temp) # (H, B*W, C)\n",
    "        # Reshape back to (B, C, H, W)\n",
    "        x_temp = x_temp.reshape(H, B, W, C).permute(1, 3, 0, 2) # (B, C, H, W)\n",
    "        x = x_temp\n",
    "\n",
    "\n",
    "        # --- Spatial (Geophone) Attention ---\n",
    "        # Input (B, C, H, W). Treat each (C, H) slice as a batch element, sequence length W.\n",
    "        # Reshape to (W, B*H, C) for Transformer\n",
    "        x_spat = x + pos_embed_w_4d.to(x.dtype) # Add PE for W dimension (broadcasts over B, H)\n",
    "        x_spat = x_spat.permute(3, 0, 2, 1).reshape(W, B * H, C) # (W, B*H, C)\n",
    "        x_spat = self.spatial_transformer(x_spat) # (W, B*H, C)\n",
    "        # Reshape back to (B, C, H, W)\n",
    "        x_spat = x_spat.reshape(W, B, H, C).permute(1, 3, 2, 0) # (B, C, H, W)\n",
    "        x = x_spat\n",
    "\n",
    "        # Add residual connection\n",
    "        x = x + identity\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: str,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder - use features_only=True to get multi-scale features\n",
    "        # We will manually modify the stem after loading the model.\n",
    "        self.backbone= timm.create_model(\n",
    "            backbone,\n",
    "            in_chans= 5,\n",
    "            pretrained= pretrained,\n",
    "            features_only= True, # Get list of features from stages\n",
    "            drop_path_rate=0.0,\n",
    "        )\n",
    "\n",
    "        # Get encoder channel counts from feature_info (deep to shallow)\n",
    "        # feature_info lists info for each output feature from `features_only=True`\n",
    "        # The order is typically from the deepest stage to the shallowest.\n",
    "        encoder_feature_info = self.backbone.feature_info\n",
    "        encoder_channels = [info['num_chs'] for info in encoder_feature_info] # [C_s3, C_s2, C_s1, C_s0] for a 4-stage model\n",
    "\n",
    "        # --- Modify the stem AFTER loading ---\n",
    "        # This requires inspecting the backbone's structure (usually has a 'stem' attribute)\n",
    "        # The timm ConvNeXt stem is a Sequential block: Conv2d -> LayerNorm2d\n",
    "        # Let's override the forward method of the backbone to use our custom stem\n",
    "        # Or, replace the stem layers directly. Replacing layers is cleaner.\n",
    "        self._update_stem(backbone)\n",
    "\n",
    "        # --- Determine the spatial size of the last feature map (input to Transformer) ---\n",
    "        # This depends on the backbone's downsampling after the modified stem.\n",
    "        # A dummy pass is the most reliable way to get the shapes *after* the custom stem.\n",
    "        # Or, calculate manually based on kernel/stride/padding for each stage.\n",
    "        # Let's calculate based on our assumed stem output (71, 72) and standard ConvNeXt downsampling (2x per stage).\n",
    "        # Stem output: (B, 128, 71, 72) - assuming _update_stem results in this\n",
    "        # Stages 0, 1, 2, 3 have downsampling before them (except stage 0).\n",
    "        # Stage 0: no spatial downsample (uses stride 1) -> (71, 72)\n",
    "        # Stage 1: 2x spatial downsample -> (71//2, 72//2) = (35, 36)\n",
    "        # Stage 2: 2x spatial downsample -> (35//2, 36//2) = (17, 18)\n",
    "        # Stage 3: 2x spatial downsample -> (17//2, 18//2) = (8, 9)\n",
    "        # So the last feature map (from stage 3) is approximately (8, 9).\n",
    "        # The channel count is encoder_channels[0] (e.g. 768 for convnext_small).\n",
    "\n",
    "        H_last_feat = max(1, round(71 / (2**3))) # Approx H after 3 stages of 2x downsampling\n",
    "        W_last_feat = max(1, round(72 / (2**3))) # Approx W after 3 stages of 2x downsampling\n",
    "        C_last_feat = encoder_channels[0] # Channel count of the deepest feature (from feature_info)\n",
    "\n",
    "        # Instantiate the Transformer module\n",
    "        self.transformer_module = TemporalSpatialTransformer(\n",
    "             embed_dim=C_last_feat,\n",
    "             seq_len_h=H_last_feat,\n",
    "             seq_len_w=W_last_feat,\n",
    "             num_layers=getattr(cfg, 'num_transformer_layers', 1),\n",
    "             dropout=getattr(cfg, 'transformer_dropout', 0.1),\n",
    "        )\n",
    "\n",
    "        # Decoder - expects encoder channels deep to shallow [C_s3, C_s2, C_s1, C_s0]\n",
    "        # Needs to match the number of decoder blocks to the number of encoder stages/features\n",
    "        # Let's use default decoder_channels or make it configurable.\n",
    "        # Default: (256, 128, 64, 32)\n",
    "        default_decoder_channels = (256, 128, 64, 32)\n",
    "        # Ensure number of decoder blocks matches number of encoder features\n",
    "        if len(default_decoder_channels) != len(encoder_channels):\n",
    "             print(f\"Warning: Adjusting number of decoder channels from {len(default_decoder_channels)} to match encoder stages {len(encoder_channels)}\")\n",
    "             # Simple strategy: Take the first N channels or repeat the last one\n",
    "             num_stages = len(encoder_channels)\n",
    "             if len(default_decoder_channels) >= num_stages:\n",
    "                 decoder_channels = default_decoder_channels[:num_stages]\n",
    "             else:\n",
    "                 # Repeat the last channel or interpolate? Simple repeat:\n",
    "                 decoder_channels = list(default_decoder_channels) + [default_decoder_channels[-1]] * (num_stages - len(default_decoder_channels))\n",
    "        else:\n",
    "             decoder_channels = default_decoder_channels\n",
    "\n",
    "        # Use InstanceNorm and GELU in the decoder blocks consistent with the encoder replacement\n",
    "        self.decoder= UnetDecoder2d(\n",
    "            encoder_channels= encoder_channels, # [C_s3, C_s2, C_s1, C_s0]\n",
    "            decoder_channels= list(decoder_channels), # [DC0, DC1, DC2, DC3] - explicit list\n",
    "            norm_layer= nn.InstanceNorm2d, # Use InstanceNorm\n",
    "            act_layer= nn.GELU, # Use GELU\n",
    "            attention_type= 'scse', # Keep SCSE attention if desired\n",
    "        )\n",
    "\n",
    "        # Segmentation Head - expects the output of the last decoder block\n",
    "        self.seg_head= SegmentationHead2d(\n",
    "            in_channels= decoder_channels[-1], # Output channel of the last decoder block (DC3)\n",
    "            out_channels= 1,\n",
    "            scale_factor= 1, # Output is already 70x70, no need to scale up further\n",
    "            norm_layer= nn.Identity, # No norm/act in head usually\n",
    "            act_layer= nn.Identity,\n",
    "        )\n",
    "\n",
    "        # Apply model modifications (activations, norms)\n",
    "        # Make sure LayerNorm within Transformer is NOT replaced by InstanceNorm\n",
    "        self.replace_activations(self.backbone, log=True)\n",
    "        self.replace_norms(self.backbone, log=True)\n",
    "        # No need to replace forwards if we handle the stem manually and use features_only=True output.\n",
    "        # self.replace_forwards(self.backbone, log=True) # REMOVE THIS CALL\n",
    "\n",
    "\n",
    "    # Helper to calculate stem output shape - only needed if we applied transformer here\n",
    "    # Or if the subsequent ConvNeXt stages rely on a specific spatial padding/size relationship\n",
    "    # relative to the stem output size. For now, we assume standard ConvNeXt downsampling applies\n",
    "    # after the modified stem, and the Transformer input size is calculated accordingly.\n",
    "    # If spatial sizes are critical, this function might be needed to adjust subsequent layers or padding.\n",
    "    # def _calculate_stem_output_shape(self, input_shape):\n",
    "    #     # This is complex due to padding and stride in the custom stem\n",
    "    #     # A dummy pass through the stem is the most reliable way\n",
    "    #     with torch.no_grad():\n",
    "    #         dummy_input = torch.randn(1, *input_shape)\n",
    "    #         stem_out = self.backbone.stem(dummy_input)\n",
    "    #         return stem_out.shape[1:] # Return (C, H, W)\n",
    "    #     pass # Not implemented for now as Transformer is applied later\n",
    "\n",
    "\n",
    "    def _update_stem(self, backbone):\n",
    "        \"\"\"\n",
    "        Modifies the initial layers (stem) of the ConvNeXt backbone\n",
    "        to handle the (C, T, W) -> (5, 1000, 70) input shape.\n",
    "        \"\"\"\n",
    "        if not hasattr(backbone, 'stem'):\n",
    "             print(f\"Warning: Backbone {backbone.__class__.__name__} does not have a 'stem' attribute. Stem modification skipped.\")\n",
    "             return\n",
    "\n",
    "        # Assuming the original stem is nn.Sequential(Conv2d, LayerNorm2d)\n",
    "        original_stem_conv = backbone.stem[0]\n",
    "        original_stem_norm = backbone.stem[1]\n",
    "\n",
    "        # Input shape: (B, 5, 1000, 70)\n",
    "        # Desired shape after stem (approx): (B, C_stem, H_stem, W_stem) where H_stem ~70, W_stem ~70\n",
    "        # The original code uses two conv layers in the stem modification. Let's replicate that.\n",
    "\n",
    "        # First conv: adapts channels (5 -> C_stem) and does initial downsampling\n",
    "        # Original: kernel=(16, 4), stride=(4, 1), padding=(0, 2)\n",
    "        # Input (1000, 70) -> (247, 71) spatial after this conv\n",
    "        # The padding and kernel size are unusual for a standard image stem.\n",
    "        # Let's replicate the structure from the user's original Net class.\n",
    "\n",
    "        # Note: The original _update_stem replaced the stem_0 (first layer of timm's Sequential stem)\n",
    "        # and added a second conv. This seems specific. Let's follow the *structure* from the original code.\n",
    "        # The original code defines stem_0 as nn.Sequential(ReflectionPad2d, original_stem_conv, new_conv)\n",
    "        # And the original LayerNorm is likely lost unless added back explicitly.\n",
    "        # Let's redefine the stem Sequential block entirely.\n",
    "\n",
    "        # Get original conv and norm parameters\n",
    "        in_chans = original_stem_conv.in_channels\n",
    "        out_chans = original_stem_conv.out_channels # C_stem\n",
    "\n",
    "        # Define the new stem sequential block\n",
    "        new_stem_layers = []\n",
    "\n",
    "        # Add padding used in original code\n",
    "        # The original code used ReflectionPad2d((1,1,80,80))\n",
    "        # This pads Width by 1 on each side (70->72) and Height by 80 on each side (1000->1160)\n",
    "        new_stem_layers.append(nn.ReflectionPad2d((1, 1, 80, 80)))\n",
    "\n",
    "        # Add the original Conv2d layer (potentially with modified stride/padding)\n",
    "        # Original code modified stride to (4, 1) and padding to (0, 2) on the original conv.\n",
    "        original_kernel = original_stem_conv.kernel_size\n",
    "        original_padding = original_stem_conv.padding\n",
    "        original_stride = original_stem_conv.stride\n",
    "\n",
    "        # Let's make sure the stride/padding are set *before* adding it to the sequence\n",
    "        stem_conv1 = nn.Conv2d(\n",
    "             in_channels=in_chans,\n",
    "             out_channels=out_chans,\n",
    "             kernel_size=original_kernel,\n",
    "             stride=(4, 1), # Modified stride from original code\n",
    "             padding=(0, 2), # Modified padding from original code\n",
    "             bias=original_stem_conv.bias is not None # Keep bias state\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "             stem_conv1.weight.copy_(original_stem_conv.weight)\n",
    "             if original_stem_conv.bias is not None:\n",
    "                  stem_conv1.bias.copy_(original_stem_conv.bias)\n",
    "\n",
    "        new_stem_layers.append(stem_conv1)\n",
    "\n",
    "        # Add the second custom conv layer from the original code\n",
    "        # kernel=(4, 4), stride=(4, 1), padding=(0, 1)\n",
    "        # Input channels = output channels of stem_conv1 (out_chans)\n",
    "        # Output channels = out_chans (seems to keep channels the same)\n",
    "        # This second conv further downsamples Height by 4x.\n",
    "        stem_conv2 = nn.Conv2d(\n",
    "             in_channels=out_chans,\n",
    "             out_channels=out_chans, # Assumed based on original code's new_conv initialization\n",
    "             kernel_size=(4, 4),\n",
    "             stride=(4, 1),\n",
    "             padding=(0, 1),\n",
    "             bias=False # Assumed bias=False for new conv before norm/activation\n",
    "        )\n",
    "        # Initialize stem_conv2 weights/bias if needed, original code copied/repeated weights\n",
    "        # Let's use default Kaiming init or similar, copying repeated weights is unusual.\n",
    "        nn.init.kaiming_normal_(stem_conv2.weight, mode='fan_out', nonlinearity='relu')\n",
    "        # The original code copied original stem weights *repeatedly*. This is highly specific and possibly incorrect.\n",
    "        # Let's use standard initialization for the new conv layer.\n",
    "        # If bias exists, initialize to zero.\n",
    "        if stem_conv2.bias is not None:\n",
    "             nn.init.constant_(stem_conv2.bias, 0)\n",
    "\n",
    "        new_stem_layers.append(stem_conv2)\n",
    "\n",
    "        # Where does the LayerNorm go? The original timm stem had Conv->Norm.\n",
    "        # The original modified stem in user code only showed convs and padding.\n",
    "        # For ConvNeXt, LayerNorm is crucial after the stem.\n",
    "        # Let's add a LayerNorm AFTER the two custom conv layers in the new stem.\n",
    "        # It should normalize over spatial dims (H, W) and channels.\n",
    "        # LayerNorm(normalized_shape) - should be (C, H, W) or just (C,) depending on usage.\n",
    "        # ConvNeXt LayerNorm is usually over channels only.\n",
    "        # LayerNorm(num_features) is equivalent to GroupNorm with group=1 and num_groups=num_features.\n",
    "        # timm uses LayerNorm(num_channels) with affine=True.\n",
    "        # The output shape before norm is (B, out_chans, H_final_stem, W_final_stem).\n",
    "        # Let's add a LayerNorm over the channel dimension.\n",
    "        # However, `replace_norms` will turn this into InstanceNorm2d.\n",
    "        # So, just add a standard LayerNorm here, and let `replace_norms` handle it.\n",
    "        # It should be LayerNorm(out_chans)\n",
    "\n",
    "        # Let's add the LayerNorm after the second conv\n",
    "        stem_norm = nn.LayerNorm(out_chans, eps=1e-6) # ConvNeXt uses eps=1e-6\n",
    "        new_stem_layers.append(stem_norm)\n",
    "\n",
    "\n",
    "        # Replace the backbone's original stem with the new sequential block\n",
    "        # timm ConvNeXt usually has backbone.stem as nn.Sequential\n",
    "        if hasattr(backbone, 'stem') and isinstance(backbone.stem, nn.Sequential):\n",
    "             backbone.stem = nn.Sequential(*new_stem_layers)\n",
    "             print(f\"Replaced backbone stem with custom sequential block.\")\n",
    "             # Also need to modify the first downsample layer, as the stem output\n",
    "             # spatial size is different from the original ConvNeXt expected input to stages[0].\n",
    "             # Standard ConvNeXt has a downsample layer *after* the stem and *before* stage 0.\n",
    "             # This downsample layer usually has stride 1 and changes channels.\n",
    "             # In features_only=True, stage 0 is the first element in `stages`.\n",
    "             # The input to stages[0] is the output of the stem.\n",
    "             # The first *spatial* downsampling (2x) happens *before* stage 1, via downsample_layers[0].\n",
    "             # The custom stem already aggressively downsamples H (1000 -> 71).\n",
    "             # Standard stages/downsampling might need adjustment.\n",
    "             # Let's assume the standard downsample_layers and stages can handle the (71, 72) spatial input from the stem.\n",
    "             # If not, more complex surgery on the backbone is needed, which violates 'lightweight'.\n",
    "             # Stick to modifying only the stem and applying Transformer after the last stage.\n",
    "        else:\n",
    "             print(f\"Warning: Backbone stem structure is not as expected for modification.\")\n",
    "\n",
    "\n",
    "    def replace_activations(self, module, log=False):\n",
    "        \"\"\" Recursively replaces specific activation functions with GELU. \"\"\"\n",
    "        if log:\n",
    "            print(f\"Replacing activations with GELU...\")\n",
    "\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, (\n",
    "                nn.ReLU, nn.LeakyReLU, nn.Mish, nn.Sigmoid,\n",
    "                nn.Tanh, nn.Softmax, nn.Hardtanh, nn.ELU,\n",
    "                nn.SELU, nn.PReLU, nn.CELU, nn.SiLU,\n",
    "            )):\n",
    "                # Replace activation instance\n",
    "                if log: print(f\"  Replacing {type(child).__name__} at {name} with GELU\")\n",
    "                setattr(module, name, nn.GELU())\n",
    "            else:\n",
    "                # Recurse into child modules\n",
    "                self.replace_activations(child, log=log)\n",
    "\n",
    "    def replace_norms(self, mod, log=False):\n",
    "        \"\"\" Recursively replaces specific normalization layers with InstanceNorm2d, skipping LayerNorm. \"\"\"\n",
    "        if log:\n",
    "            print(f\"Replacing norms with InstanceNorm2d (skipping LayerNorm)...\")\n",
    "\n",
    "        for name, c in mod.named_children():\n",
    "            # Skip LayerNorm as it's used in Transformers and ConvNeXt often uses it after stem\n",
    "            if isinstance(c, nn.LayerNorm):\n",
    "                if log: print(f\"  Skipping LayerNorm at {name}\")\n",
    "                continue\n",
    "\n",
    "            # Get feature size (handle different norm types)\n",
    "            n_feats= None\n",
    "            if isinstance(c, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n",
    "                n_feats= c.num_features\n",
    "            elif isinstance(c, (nn.GroupNorm,)):\n",
    "                n_feats= c.num_channels\n",
    "            # nn.LayerNorm is already skipped above\n",
    "\n",
    "            if n_feats is not None:\n",
    "                # Create new InstanceNorm2d layer\n",
    "                if log: print(f\"  Replacing {type(c).__name__} at {name} with InstanceNorm2d\")\n",
    "                new = nn.InstanceNorm2d(\n",
    "                    n_feats,\n",
    "                    affine=True, # Keep affine=True to match BatchNorm/LayerNorm behavior\n",
    "                    track_running_stats=False, # InstanceNorm typically doesn't track stats\n",
    "                )\n",
    "                setattr(mod, name, new)\n",
    "            else:\n",
    "                # Recurse into child modules\n",
    "                self.replace_norms(c, log=log)\n",
    "\n",
    "    # Removed replace_forwards as it's not needed with features_only=True approach\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = batch # Input (B, 5, 1000, 70)\n",
    "        B_ = x.shape[0]\n",
    "\n",
    "        # AMP context for mixed precision\n",
    "        # Use bfloat16 if available and requested, otherwise float16\n",
    "        amp_dtype = None\n",
    "        if hasattr(cfg, 'use_amp') and cfg.use_amp and cfg.device.type == 'cuda':\n",
    "             amp_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() and getattr(cfg, 'amp_dtype', 'bfloat16') == 'bfloat16' else torch.float16\n",
    "             if amp_dtype == torch.float16:\n",
    "                 print(\"Warning: Using float16 AMP without GradScaler might cause issues during training.\")\n",
    "             # Enable AMP only if dtype was successfully determined\n",
    "             amp_enabled = (amp_dtype is not None)\n",
    "        else:\n",
    "             amp_enabled = False\n",
    "             amp_dtype = None # Explicitly None if AMP is off\n",
    "\n",
    "\n",
    "        # The backbone was loaded with features_only=True, but we replaced the stem.\n",
    "        # The backbone forward *might* still call its internal original stem logic or expect a certain input format.\n",
    "        # It's safer to call the stages manually after our modified stem.\n",
    "\n",
    "        # Manual backbone forward\n",
    "        # Need access to backbone.stem, backbone.stages (ModuleList), backbone.downsample_layers (ModuleList)\n",
    "        # Check if the modified stem and these attributes exist\n",
    "        if not hasattr(self.backbone, 'stem') or not hasattr(self.backbone, 'stages') or not hasattr(self.backbone, 'downsample_layers'):\n",
    "            raise AttributeError(\"Backbone structure not as expected for manual forward pass.\")\n",
    "\n",
    "        with torch.autocast(device_type=self.cfg.device.type, dtype=amp_dtype, enabled=amp_enabled):\n",
    "             x_in = x # Keep original input for inference-time flip\n",
    "\n",
    "             # Pass through the custom stem\n",
    "             # The stem expects (B, C, H, W) where C=5, H=1000, W=70\n",
    "             stem_out = self.backbone.stem(x) # Output (B, C_stem, H_stem, W_stem) e.g. (B, 128, 71, 72)\n",
    "\n",
    "             # Pass through subsequent stages and downsamples\n",
    "             # Collect intermediate features for the decoder\n",
    "             features_for_decoder = []\n",
    "             # Stage 0 is the first stage after the stem. Add its output.\n",
    "             x = self.backbone.stages[0](stem_out)\n",
    "             features_for_decoder.append(x) # s0\n",
    "\n",
    "             # Pass through subsequent stages and downsamples\n",
    "             # Stages 1, 2, 3 are connected via downsample layers\n",
    "             # The i-th downsample_layer is applied before the i+1-th stage\n",
    "             for i in range(len(self.backbone.downsample_layers)): # Loop through downsample layers\n",
    "                 # downsample_layers[i] is applied to the output of stages[i]\n",
    "                 x = self.backbone.downsample_layers[i](x)\n",
    "                 # The output of downsample_layers[i] is input to stages[i+1]\n",
    "                 x = self.backbone.stages[i+1](x)\n",
    "                 features_for_decoder.append(x) # s1, s2, s3...\n",
    "\n",
    "             # features_for_decoder is now [s0, s1, s2, s3] (shallow to deep)\n",
    "             # Reverse for decoder input [s3, s2, s1, s0]\n",
    "             feats = features_for_decoder[::-1]\n",
    "\n",
    "             # Apply Transformer module to the LAST feature map (s3)\n",
    "             last_feat_map = feats[0] # This is s3\n",
    "\n",
    "             # Ensure the transformer input spatial dimensions match the expected seq_len_h/w\n",
    "             # The Transformer module handles padding/cropping internally based on its init values.\n",
    "             trans_feat = self.transformer_module(last_feat_map)\n",
    "\n",
    "             # Prepare features for the decoder: [transformed_s3, s2, s1, s0]\n",
    "             decoder_input_feats = [trans_feat] + feats[1:]\n",
    "\n",
    "             # Decoder forward - pass the main input (transformed s3) and the list of skips [s2, s1, s0]\n",
    "             decoder_outs = self.decoder(decoder_input_feats[0], decoder_input_feats[1:])\n",
    "\n",
    "             # Segmentation Head takes the output of the last decoder block\n",
    "             seg_head_input = decoder_outs[-1] # This is the output of the shallowest decoder block\n",
    "\n",
    "             # Segmentation Head\n",
    "             x_seg = self.seg_head(seg_head_input)\n",
    "\n",
    "             # Post-processing (cropping, scaling)\n",
    "             # The output is (B, 1, 72, 72) approximately due to padding, need to crop to 70x70\n",
    "             # Assumes the output size is 72x72 based on stem calculation and no further downsampling affecting this ratio.\n",
    "             # Need to calculate the actual output size reliably or make cropping dynamic.\n",
    "             # Let's assume it's 72x72 and crop 1 pixel from each side.\n",
    "             output_h, output_w = x_seg.shape[-2:]\n",
    "             target_h, target_w = 70, 70\n",
    "             if output_h != target_h or output_w != target_w:\n",
    "                 # Calculate cropping amounts\n",
    "                 crop_h = max(0, output_h - target_h)\n",
    "                 crop_w = max(0, output_w - target_w)\n",
    "                 # Apply center crop\n",
    "                 x_seg = x_seg[:, :, crop_h // 2 : crop_h // 2 + target_h, crop_w // 2 : crop_w // 2 + target_w]\n",
    "                 # print(f\"Cropped head output from {output_h}x{output_w} to {x_seg.shape[-2:]}\")\n",
    "\n",
    "\n",
    "             x_seg = x_seg * 1500 + 3000 # Apply scaling\n",
    "\n",
    "        # Inference-time flip augmentation\n",
    "        if not self.training:\n",
    "             # Apply spatial flip to the input data\n",
    "             x_in_flipped = torch.flip(x_in, dims=[-1]) # Flip geophone dimension\n",
    "             # Process the flipped input through the same model forward path\n",
    "             # This recursive call will also apply AMP internally\n",
    "             flipped_output = self.forward(x_in_flipped)\n",
    "             # Flip the output spatially back\n",
    "             flipped_output_spatial_flipped = torch.flip(flipped_output, dims=[-1]) # Flip map width dimension\n",
    "\n",
    "             # Average the original and flipped outputs\n",
    "             x_seg = torch.mean(torch.stack([x_seg, flipped_output_spatial_flipped]), dim=0)\n",
    "\n",
    "\n",
    "        return x_seg\n",
    "    \n",
    "\n",
    "def set_seed(seed=cfg.seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1006f7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files in the data base: 3\n",
      "Using 2 files for training and 1 for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train data (mmap): 100%|██████████| 2/2 [00:00<00:00, 1479.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CustomDataset - load data arrays ]:arr shape: (500, 5, 1000, 70) | lbl shape: (500, 1, 70, 70)\n",
      "[CustomDataset - load data arrays ]:lbl after squeeze shape: (500, 70, 70)\n",
      "[CustomDataset - load data arrays ]:arr shape: (500, 5, 1000, 70) | lbl shape: (500, 1, 70, 70)\n",
      "[CustomDataset - load data arrays ]:lbl after squeeze shape: (500, 70, 70)\n",
      "Finished loading 2 file pairs for train mode.\n",
      "[CustomDataset - init ]:self.data shape: (500, 5, 1000, 70) | self.labels shape (500, 70, 70)\n",
      "[CustomDataset - init ]:samples_per_file : 500\n",
      "[CustomDataset - init ]:file_idx : 0 \n",
      "[CustomDataset - init ]:file_idx : 1 \n",
      "[CustomDataset - init ]:total_samples : 1000\n",
      "[CustomDataset - init ]:index_map : [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (0, 20), (0, 21), (0, 22), (0, 23), (0, 24), (0, 25), (0, 26), (0, 27), (0, 28), (0, 29), (0, 30), (0, 31), (0, 32), (0, 33), (0, 34), (0, 35), (0, 36), (0, 37), (0, 38), (0, 39), (0, 40), (0, 41), (0, 42), (0, 43), (0, 44), (0, 45), (0, 46), (0, 47), (0, 48), (0, 49), (0, 50), (0, 51), (0, 52), (0, 53), (0, 54), (0, 55), (0, 56), (0, 57), (0, 58), (0, 59), (0, 60), (0, 61), (0, 62), (0, 63), (0, 64), (0, 65), (0, 66), (0, 67), (0, 68), (0, 69), (0, 70), (0, 71), (0, 72), (0, 73), (0, 74), (0, 75), (0, 76), (0, 77), (0, 78), (0, 79), (0, 80), (0, 81), (0, 82), (0, 83), (0, 84), (0, 85), (0, 86), (0, 87), (0, 88), (0, 89), (0, 90), (0, 91), (0, 92), (0, 93), (0, 94), (0, 95), (0, 96), (0, 97), (0, 98), (0, 99), (0, 100), (0, 101), (0, 102), (0, 103), (0, 104), (0, 105), (0, 106), (0, 107), (0, 108), (0, 109), (0, 110), (0, 111), (0, 112), (0, 113), (0, 114), (0, 115), (0, 116), (0, 117), (0, 118), (0, 119), (0, 120), (0, 121), (0, 122), (0, 123), (0, 124), (0, 125), (0, 126), (0, 127), (0, 128), (0, 129), (0, 130), (0, 131), (0, 132), (0, 133), (0, 134), (0, 135), (0, 136), (0, 137), (0, 138), (0, 139), (0, 140), (0, 141), (0, 142), (0, 143), (0, 144), (0, 145), (0, 146), (0, 147), (0, 148), (0, 149), (0, 150), (0, 151), (0, 152), (0, 153), (0, 154), (0, 155), (0, 156), (0, 157), (0, 158), (0, 159), (0, 160), (0, 161), (0, 162), (0, 163), (0, 164), (0, 165), (0, 166), (0, 167), (0, 168), (0, 169), (0, 170), (0, 171), (0, 172), (0, 173), (0, 174), (0, 175), (0, 176), (0, 177), (0, 178), (0, 179), (0, 180), (0, 181), (0, 182), (0, 183), (0, 184), (0, 185), (0, 186), (0, 187), (0, 188), (0, 189), (0, 190), (0, 191), (0, 192), (0, 193), (0, 194), (0, 195), (0, 196), (0, 197), (0, 198), (0, 199), (0, 200), (0, 201), (0, 202), (0, 203), (0, 204), (0, 205), (0, 206), (0, 207), (0, 208), (0, 209), (0, 210), (0, 211), (0, 212), (0, 213), (0, 214), (0, 215), (0, 216), (0, 217), (0, 218), (0, 219), (0, 220), (0, 221), (0, 222), (0, 223), (0, 224), (0, 225), (0, 226), (0, 227), (0, 228), (0, 229), (0, 230), (0, 231), (0, 232), (0, 233), (0, 234), (0, 235), (0, 236), (0, 237), (0, 238), (0, 239), (0, 240), (0, 241), (0, 242), (0, 243), (0, 244), (0, 245), (0, 246), (0, 247), (0, 248), (0, 249), (0, 250), (0, 251), (0, 252), (0, 253), (0, 254), (0, 255), (0, 256), (0, 257), (0, 258), (0, 259), (0, 260), (0, 261), (0, 262), (0, 263), (0, 264), (0, 265), (0, 266), (0, 267), (0, 268), (0, 269), (0, 270), (0, 271), (0, 272), (0, 273), (0, 274), (0, 275), (0, 276), (0, 277), (0, 278), (0, 279), (0, 280), (0, 281), (0, 282), (0, 283), (0, 284), (0, 285), (0, 286), (0, 287), (0, 288), (0, 289), (0, 290), (0, 291), (0, 292), (0, 293), (0, 294), (0, 295), (0, 296), (0, 297), (0, 298), (0, 299), (0, 300), (0, 301), (0, 302), (0, 303), (0, 304), (0, 305), (0, 306), (0, 307), (0, 308), (0, 309), (0, 310), (0, 311), (0, 312), (0, 313), (0, 314), (0, 315), (0, 316), (0, 317), (0, 318), (0, 319), (0, 320), (0, 321), (0, 322), (0, 323), (0, 324), (0, 325), (0, 326), (0, 327), (0, 328), (0, 329), (0, 330), (0, 331), (0, 332), (0, 333), (0, 334), (0, 335), (0, 336), (0, 337), (0, 338), (0, 339), (0, 340), (0, 341), (0, 342), (0, 343), (0, 344), (0, 345), (0, 346), (0, 347), (0, 348), (0, 349), (0, 350), (0, 351), (0, 352), (0, 353), (0, 354), (0, 355), (0, 356), (0, 357), (0, 358), (0, 359), (0, 360), (0, 361), (0, 362), (0, 363), (0, 364), (0, 365), (0, 366), (0, 367), (0, 368), (0, 369), (0, 370), (0, 371), (0, 372), (0, 373), (0, 374), (0, 375), (0, 376), (0, 377), (0, 378), (0, 379), (0, 380), (0, 381), (0, 382), (0, 383), (0, 384), (0, 385), (0, 386), (0, 387), (0, 388), (0, 389), (0, 390), (0, 391), (0, 392), (0, 393), (0, 394), (0, 395), (0, 396), (0, 397), (0, 398), (0, 399), (0, 400), (0, 401), (0, 402), (0, 403), (0, 404), (0, 405), (0, 406), (0, 407), (0, 408), (0, 409), (0, 410), (0, 411), (0, 412), (0, 413), (0, 414), (0, 415), (0, 416), (0, 417), (0, 418), (0, 419), (0, 420), (0, 421), (0, 422), (0, 423), (0, 424), (0, 425), (0, 426), (0, 427), (0, 428), (0, 429), (0, 430), (0, 431), (0, 432), (0, 433), (0, 434), (0, 435), (0, 436), (0, 437), (0, 438), (0, 439), (0, 440), (0, 441), (0, 442), (0, 443), (0, 444), (0, 445), (0, 446), (0, 447), (0, 448), (0, 449), (0, 450), (0, 451), (0, 452), (0, 453), (0, 454), (0, 455), (0, 456), (0, 457), (0, 458), (0, 459), (0, 460), (0, 461), (0, 462), (0, 463), (0, 464), (0, 465), (0, 466), (0, 467), (0, 468), (0, 469), (0, 470), (0, 471), (0, 472), (0, 473), (0, 474), (0, 475), (0, 476), (0, 477), (0, 478), (0, 479), (0, 480), (0, 481), (0, 482), (0, 483), (0, 484), (0, 485), (0, 486), (0, 487), (0, 488), (0, 489), (0, 490), (0, 491), (0, 492), (0, 493), (0, 494), (0, 495), (0, 496), (0, 497), (0, 498), (0, 499), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (1, 19), (1, 20), (1, 21), (1, 22), (1, 23), (1, 24), (1, 25), (1, 26), (1, 27), (1, 28), (1, 29), (1, 30), (1, 31), (1, 32), (1, 33), (1, 34), (1, 35), (1, 36), (1, 37), (1, 38), (1, 39), (1, 40), (1, 41), (1, 42), (1, 43), (1, 44), (1, 45), (1, 46), (1, 47), (1, 48), (1, 49), (1, 50), (1, 51), (1, 52), (1, 53), (1, 54), (1, 55), (1, 56), (1, 57), (1, 58), (1, 59), (1, 60), (1, 61), (1, 62), (1, 63), (1, 64), (1, 65), (1, 66), (1, 67), (1, 68), (1, 69), (1, 70), (1, 71), (1, 72), (1, 73), (1, 74), (1, 75), (1, 76), (1, 77), (1, 78), (1, 79), (1, 80), (1, 81), (1, 82), (1, 83), (1, 84), (1, 85), (1, 86), (1, 87), (1, 88), (1, 89), (1, 90), (1, 91), (1, 92), (1, 93), (1, 94), (1, 95), (1, 96), (1, 97), (1, 98), (1, 99), (1, 100), (1, 101), (1, 102), (1, 103), (1, 104), (1, 105), (1, 106), (1, 107), (1, 108), (1, 109), (1, 110), (1, 111), (1, 112), (1, 113), (1, 114), (1, 115), (1, 116), (1, 117), (1, 118), (1, 119), (1, 120), (1, 121), (1, 122), (1, 123), (1, 124), (1, 125), (1, 126), (1, 127), (1, 128), (1, 129), (1, 130), (1, 131), (1, 132), (1, 133), (1, 134), (1, 135), (1, 136), (1, 137), (1, 138), (1, 139), (1, 140), (1, 141), (1, 142), (1, 143), (1, 144), (1, 145), (1, 146), (1, 147), (1, 148), (1, 149), (1, 150), (1, 151), (1, 152), (1, 153), (1, 154), (1, 155), (1, 156), (1, 157), (1, 158), (1, 159), (1, 160), (1, 161), (1, 162), (1, 163), (1, 164), (1, 165), (1, 166), (1, 167), (1, 168), (1, 169), (1, 170), (1, 171), (1, 172), (1, 173), (1, 174), (1, 175), (1, 176), (1, 177), (1, 178), (1, 179), (1, 180), (1, 181), (1, 182), (1, 183), (1, 184), (1, 185), (1, 186), (1, 187), (1, 188), (1, 189), (1, 190), (1, 191), (1, 192), (1, 193), (1, 194), (1, 195), (1, 196), (1, 197), (1, 198), (1, 199), (1, 200), (1, 201), (1, 202), (1, 203), (1, 204), (1, 205), (1, 206), (1, 207), (1, 208), (1, 209), (1, 210), (1, 211), (1, 212), (1, 213), (1, 214), (1, 215), (1, 216), (1, 217), (1, 218), (1, 219), (1, 220), (1, 221), (1, 222), (1, 223), (1, 224), (1, 225), (1, 226), (1, 227), (1, 228), (1, 229), (1, 230), (1, 231), (1, 232), (1, 233), (1, 234), (1, 235), (1, 236), (1, 237), (1, 238), (1, 239), (1, 240), (1, 241), (1, 242), (1, 243), (1, 244), (1, 245), (1, 246), (1, 247), (1, 248), (1, 249), (1, 250), (1, 251), (1, 252), (1, 253), (1, 254), (1, 255), (1, 256), (1, 257), (1, 258), (1, 259), (1, 260), (1, 261), (1, 262), (1, 263), (1, 264), (1, 265), (1, 266), (1, 267), (1, 268), (1, 269), (1, 270), (1, 271), (1, 272), (1, 273), (1, 274), (1, 275), (1, 276), (1, 277), (1, 278), (1, 279), (1, 280), (1, 281), (1, 282), (1, 283), (1, 284), (1, 285), (1, 286), (1, 287), (1, 288), (1, 289), (1, 290), (1, 291), (1, 292), (1, 293), (1, 294), (1, 295), (1, 296), (1, 297), (1, 298), (1, 299), (1, 300), (1, 301), (1, 302), (1, 303), (1, 304), (1, 305), (1, 306), (1, 307), (1, 308), (1, 309), (1, 310), (1, 311), (1, 312), (1, 313), (1, 314), (1, 315), (1, 316), (1, 317), (1, 318), (1, 319), (1, 320), (1, 321), (1, 322), (1, 323), (1, 324), (1, 325), (1, 326), (1, 327), (1, 328), (1, 329), (1, 330), (1, 331), (1, 332), (1, 333), (1, 334), (1, 335), (1, 336), (1, 337), (1, 338), (1, 339), (1, 340), (1, 341), (1, 342), (1, 343), (1, 344), (1, 345), (1, 346), (1, 347), (1, 348), (1, 349), (1, 350), (1, 351), (1, 352), (1, 353), (1, 354), (1, 355), (1, 356), (1, 357), (1, 358), (1, 359), (1, 360), (1, 361), (1, 362), (1, 363), (1, 364), (1, 365), (1, 366), (1, 367), (1, 368), (1, 369), (1, 370), (1, 371), (1, 372), (1, 373), (1, 374), (1, 375), (1, 376), (1, 377), (1, 378), (1, 379), (1, 380), (1, 381), (1, 382), (1, 383), (1, 384), (1, 385), (1, 386), (1, 387), (1, 388), (1, 389), (1, 390), (1, 391), (1, 392), (1, 393), (1, 394), (1, 395), (1, 396), (1, 397), (1, 398), (1, 399), (1, 400), (1, 401), (1, 402), (1, 403), (1, 404), (1, 405), (1, 406), (1, 407), (1, 408), (1, 409), (1, 410), (1, 411), (1, 412), (1, 413), (1, 414), (1, 415), (1, 416), (1, 417), (1, 418), (1, 419), (1, 420), (1, 421), (1, 422), (1, 423), (1, 424), (1, 425), (1, 426), (1, 427), (1, 428), (1, 429), (1, 430), (1, 431), (1, 432), (1, 433), (1, 434), (1, 435), (1, 436), (1, 437), (1, 438), (1, 439), (1, 440), (1, 441), (1, 442), (1, 443), (1, 444), (1, 445), (1, 446), (1, 447), (1, 448), (1, 449), (1, 450), (1, 451), (1, 452), (1, 453), (1, 454), (1, 455), (1, 456), (1, 457), (1, 458), (1, 459), (1, 460), (1, 461), (1, 462), (1, 463), (1, 464), (1, 465), (1, 466), (1, 467), (1, 468), (1, 469), (1, 470), (1, 471), (1, 472), (1, 473), (1, 474), (1, 475), (1, 476), (1, 477), (1, 478), (1, 479), (1, 480), (1, 481), (1, 482), (1, 483), (1, 484), (1, 485), (1, 486), (1, 487), (1, 488), (1, 489), (1, 490), (1, 491), (1, 492), (1, 493), (1, 494), (1, 495), (1, 496), (1, 497), (1, 498), (1, 499)]\n",
      "[CustomDataset - __len__]: self.total_samples:  1000\n",
      "[CustomDataset - __len__]: self.total_samples:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading valid data (mmap): 100%|██████████| 1/1 [00:00<00:00, 1286.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CustomDataset - load data arrays ]:arr shape: (500, 5, 1000, 70) | lbl shape: (500, 1, 70, 70)\n",
      "[CustomDataset - load data arrays ]:lbl after squeeze shape: (500, 70, 70)\n",
      "Finished loading 1 file pairs for valid mode.\n",
      "[CustomDataset - init ]:self.data shape: (500, 5, 1000, 70) | self.labels shape (500, 70, 70)\n",
      "[CustomDataset - init ]:samples_per_file : 500\n",
      "[CustomDataset - init ]:file_idx : 0 \n",
      "[CustomDataset - init ]:total_samples : 500\n",
      "[CustomDataset - init ]:index_map : [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (0, 19), (0, 20), (0, 21), (0, 22), (0, 23), (0, 24), (0, 25), (0, 26), (0, 27), (0, 28), (0, 29), (0, 30), (0, 31), (0, 32), (0, 33), (0, 34), (0, 35), (0, 36), (0, 37), (0, 38), (0, 39), (0, 40), (0, 41), (0, 42), (0, 43), (0, 44), (0, 45), (0, 46), (0, 47), (0, 48), (0, 49), (0, 50), (0, 51), (0, 52), (0, 53), (0, 54), (0, 55), (0, 56), (0, 57), (0, 58), (0, 59), (0, 60), (0, 61), (0, 62), (0, 63), (0, 64), (0, 65), (0, 66), (0, 67), (0, 68), (0, 69), (0, 70), (0, 71), (0, 72), (0, 73), (0, 74), (0, 75), (0, 76), (0, 77), (0, 78), (0, 79), (0, 80), (0, 81), (0, 82), (0, 83), (0, 84), (0, 85), (0, 86), (0, 87), (0, 88), (0, 89), (0, 90), (0, 91), (0, 92), (0, 93), (0, 94), (0, 95), (0, 96), (0, 97), (0, 98), (0, 99), (0, 100), (0, 101), (0, 102), (0, 103), (0, 104), (0, 105), (0, 106), (0, 107), (0, 108), (0, 109), (0, 110), (0, 111), (0, 112), (0, 113), (0, 114), (0, 115), (0, 116), (0, 117), (0, 118), (0, 119), (0, 120), (0, 121), (0, 122), (0, 123), (0, 124), (0, 125), (0, 126), (0, 127), (0, 128), (0, 129), (0, 130), (0, 131), (0, 132), (0, 133), (0, 134), (0, 135), (0, 136), (0, 137), (0, 138), (0, 139), (0, 140), (0, 141), (0, 142), (0, 143), (0, 144), (0, 145), (0, 146), (0, 147), (0, 148), (0, 149), (0, 150), (0, 151), (0, 152), (0, 153), (0, 154), (0, 155), (0, 156), (0, 157), (0, 158), (0, 159), (0, 160), (0, 161), (0, 162), (0, 163), (0, 164), (0, 165), (0, 166), (0, 167), (0, 168), (0, 169), (0, 170), (0, 171), (0, 172), (0, 173), (0, 174), (0, 175), (0, 176), (0, 177), (0, 178), (0, 179), (0, 180), (0, 181), (0, 182), (0, 183), (0, 184), (0, 185), (0, 186), (0, 187), (0, 188), (0, 189), (0, 190), (0, 191), (0, 192), (0, 193), (0, 194), (0, 195), (0, 196), (0, 197), (0, 198), (0, 199), (0, 200), (0, 201), (0, 202), (0, 203), (0, 204), (0, 205), (0, 206), (0, 207), (0, 208), (0, 209), (0, 210), (0, 211), (0, 212), (0, 213), (0, 214), (0, 215), (0, 216), (0, 217), (0, 218), (0, 219), (0, 220), (0, 221), (0, 222), (0, 223), (0, 224), (0, 225), (0, 226), (0, 227), (0, 228), (0, 229), (0, 230), (0, 231), (0, 232), (0, 233), (0, 234), (0, 235), (0, 236), (0, 237), (0, 238), (0, 239), (0, 240), (0, 241), (0, 242), (0, 243), (0, 244), (0, 245), (0, 246), (0, 247), (0, 248), (0, 249), (0, 250), (0, 251), (0, 252), (0, 253), (0, 254), (0, 255), (0, 256), (0, 257), (0, 258), (0, 259), (0, 260), (0, 261), (0, 262), (0, 263), (0, 264), (0, 265), (0, 266), (0, 267), (0, 268), (0, 269), (0, 270), (0, 271), (0, 272), (0, 273), (0, 274), (0, 275), (0, 276), (0, 277), (0, 278), (0, 279), (0, 280), (0, 281), (0, 282), (0, 283), (0, 284), (0, 285), (0, 286), (0, 287), (0, 288), (0, 289), (0, 290), (0, 291), (0, 292), (0, 293), (0, 294), (0, 295), (0, 296), (0, 297), (0, 298), (0, 299), (0, 300), (0, 301), (0, 302), (0, 303), (0, 304), (0, 305), (0, 306), (0, 307), (0, 308), (0, 309), (0, 310), (0, 311), (0, 312), (0, 313), (0, 314), (0, 315), (0, 316), (0, 317), (0, 318), (0, 319), (0, 320), (0, 321), (0, 322), (0, 323), (0, 324), (0, 325), (0, 326), (0, 327), (0, 328), (0, 329), (0, 330), (0, 331), (0, 332), (0, 333), (0, 334), (0, 335), (0, 336), (0, 337), (0, 338), (0, 339), (0, 340), (0, 341), (0, 342), (0, 343), (0, 344), (0, 345), (0, 346), (0, 347), (0, 348), (0, 349), (0, 350), (0, 351), (0, 352), (0, 353), (0, 354), (0, 355), (0, 356), (0, 357), (0, 358), (0, 359), (0, 360), (0, 361), (0, 362), (0, 363), (0, 364), (0, 365), (0, 366), (0, 367), (0, 368), (0, 369), (0, 370), (0, 371), (0, 372), (0, 373), (0, 374), (0, 375), (0, 376), (0, 377), (0, 378), (0, 379), (0, 380), (0, 381), (0, 382), (0, 383), (0, 384), (0, 385), (0, 386), (0, 387), (0, 388), (0, 389), (0, 390), (0, 391), (0, 392), (0, 393), (0, 394), (0, 395), (0, 396), (0, 397), (0, 398), (0, 399), (0, 400), (0, 401), (0, 402), (0, 403), (0, 404), (0, 405), (0, 406), (0, 407), (0, 408), (0, 409), (0, 410), (0, 411), (0, 412), (0, 413), (0, 414), (0, 415), (0, 416), (0, 417), (0, 418), (0, 419), (0, 420), (0, 421), (0, 422), (0, 423), (0, 424), (0, 425), (0, 426), (0, 427), (0, 428), (0, 429), (0, 430), (0, 431), (0, 432), (0, 433), (0, 434), (0, 435), (0, 436), (0, 437), (0, 438), (0, 439), (0, 440), (0, 441), (0, 442), (0, 443), (0, 444), (0, 445), (0, 446), (0, 447), (0, 448), (0, 449), (0, 450), (0, 451), (0, 452), (0, 453), (0, 454), (0, 455), (0, 456), (0, 457), (0, 458), (0, 459), (0, 460), (0, 461), (0, 462), (0, 463), (0, 464), (0, 465), (0, 466), (0, 467), (0, 468), (0, 469), (0, 470), (0, 471), (0, 472), (0, 473), (0, 474), (0, 475), (0, 476), (0, 477), (0, 478), (0, 479), (0, 480), (0, 481), (0, 482), (0, 483), (0, 484), (0, 485), (0, 486), (0, 487), (0, 488), (0, 489), (0, 490), (0, 491), (0, 492), (0, 493), (0, 494), (0, 495), (0, 496), (0, 497), (0, 498), (0, 499)]\n",
      "[CustomDataset - __len__]: self.total_samples:  1000\n",
      "[CustomDataset - __len__]: self.total_samples:  1000\n",
      "[CustomDataset - __len__]: self.total_samples:  1000\n",
      "[CustomDataset - getitem ]:file_idx 0 | file_idx35\n",
      "[CustomDataset - getitem ]:x_full_file shape (5, 1000, 70) | y_full_file shape: (70, 70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CustomDataset - getitem ]:x_augmented shape (5, 1000, 70) | y_augmented shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_tensor shape torch.Size([5, 1000, 70]) | y_tensor shape: torch.Size([1, 70, 70])\n",
      "[CustomDataset - getitem ]:file_idx 1 | file_idx401\n",
      "[CustomDataset - getitem ]:x_full_file shape (5, 1000, 70) | y_full_file shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_augmented shape (5, 1000, 70) | y_augmented shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_tensor shape torch.Size([5, 1000, 70]) | y_tensor shape: torch.Size([1, 70, 70])\n",
      "[CustomDataset - getitem ]:file_idx 1 | file_idx33\n",
      "[CustomDataset - getitem ]:x_full_file shape (5, 1000, 70) | y_full_file shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_augmented shape (5, 1000, 70) | y_augmented shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_tensor shape torch.Size([5, 1000, 70]) | y_tensor shape: torch.Size([1, 70, 70])\n",
      "[CustomDataset - getitem ]:file_idx 0 | file_idx374\n",
      "[CustomDataset - getitem ]:x_full_file shape (5, 1000, 70) | y_full_file shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_augmented shape (5, 1000, 70) | y_augmented shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_tensor shape torch.Size([5, 1000, 70]) | y_tensor shape: torch.Size([1, 70, 70])\n",
      "[CustomDataset - getitem ]:file_idx 0 | file_idx55\n",
      "[CustomDataset - getitem ]:x_full_file shape (5, 1000, 70) | y_full_file shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_augmented shape (5, 1000, 70) | y_augmented shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_tensor shape torch.Size([5, 1000, 70]) | y_tensor shape: torch.Size([1, 70, 70])\n",
      "[CustomDataset - getitem ]:file_idx 0 | file_idx167\n",
      "[CustomDataset - getitem ]:x_full_file shape (5, 1000, 70) | y_full_file shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_augmented shape (5, 1000, 70) | y_augmented shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_tensor shape torch.Size([5, 1000, 70]) | y_tensor shape: torch.Size([1, 70, 70])\n",
      "[CustomDataset - getitem ]:file_idx 0 | file_idx138\n",
      "[CustomDataset - getitem ]:x_full_file shape (5, 1000, 70) | y_full_file shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_augmented shape (5, 1000, 70) | y_augmented shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_tensor shape torch.Size([5, 1000, 70]) | y_tensor shape: torch.Size([1, 70, 70])\n",
      "[CustomDataset - getitem ]:file_idx 1 | file_idx182\n",
      "[CustomDataset - getitem ]:x_full_file shape (5, 1000, 70) | y_full_file shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_augmented shape (5, 1000, 70) | y_augmented shape: (70, 70)\n",
      "[CustomDataset - getitem ]:x_tensor shape torch.Size([5, 1000, 70]) | y_tensor shape: torch.Size([1, 70, 70])\n",
      "\n",
      "DataLoader sample shapes:\n",
      "Input (x): torch.Size([8, 5, 1000, 70])\n",
      "Output (y): torch.Size([8, 1, 70, 70])\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loading ---\n",
    "# Make sure file_pairs list is not empty before creating datasets\n",
    "if not cfg.file_pairs:\n",
    "    print(\"No data file pairs found. Exiting.\")\n",
    "    # You would typically handle this case appropriately, e.g., exit, raise error, or load dummy data\n",
    "    # For this notebook structure, we'll just print and the subsequent code might fail\n",
    "    train_dl = [] # Empty lists to prevent errors later\n",
    "    valid_dl = []\n",
    "    print(\"Created empty dataloaders.\")\n",
    "else:\n",
    "    # Use a small subset for validation/test if subsample is used for train\n",
    "    # Or split files into train/val/test sets. For simplicity, use all files for both train/val here.\n",
    "    # In a real scenario, you'd split `cfg.file_pairs` into train_files, val_files, test_files\n",
    "    # and pass the appropriate list to each dataset instance.\n",
    "    # For demonstration, using all files for train and val as in the original code structure,\n",
    "    # but the subsample logic will limit the actual number of samples.\n",
    "\n",
    "    # Small split for demonstration purposes\n",
    "    total_files = len(cfg.file_pairs)\n",
    "    print(f\"number of files in the data base: {total_files}\")\n",
    "    if total_files > 1:\n",
    "        split_idx = max(1, int(total_files * 0.8)) # 80% train, 20% val\n",
    "        train_file_pairs = cfg.file_pairs[:split_idx]\n",
    "        valid_file_pairs = cfg.file_pairs[split_idx:]\n",
    "        # Optional: Further split valid_file_pairs for a separate test set\n",
    "        # test_file_pairs = ...\n",
    "    else:\n",
    "        # If only one file, use it for both (not ideal for training)\n",
    "        train_file_pairs = cfg.file_pairs\n",
    "        valid_file_pairs = cfg.file_pairs\n",
    "\n",
    "\n",
    "    print(f\"Using {len(train_file_pairs)} files for training and {len(valid_file_pairs)} for validation.\")\n",
    "\n",
    "    train_ds = CustomDataset(cfg=cfg, file_pairs=train_file_pairs, mode=\"train\")\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size= cfg.batch_size,\n",
    "        num_workers= 0, # Set to > 0 for faster loading if memory allows\n",
    "        shuffle=True,\n",
    "        pin_memory=True, # speeds up data transfer to GPU\n",
    "    )\n",
    "\n",
    "    valid_ds = CustomDataset(cfg=cfg, file_pairs=valid_file_pairs, mode=\"valid\")\n",
    "    valid_dl = torch.utils.data.DataLoader(\n",
    "        valid_ds,\n",
    "        batch_size= cfg.batch_size_val,\n",
    "        num_workers= 0, # Set to > 0 for faster loading\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "# Check dataset output shapes\n",
    "if train_dl:\n",
    "    x, y = next(iter(train_dl))\n",
    "    print(\"\\nDataLoader sample shapes:\")\n",
    "    print(\"Input (x):\", x.shape) # Expected: (B, C, T, W) -> (B, 5, 1000, 70)\n",
    "    print(\"Output (y):\", y.shape) # Expected: (B, 1, H', W') -> (B, 1, 70, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef1b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model / Optim ==========\n",
    "print(f\"\\nInitializing model on device: {cfg.device}\")\n",
    "model = Net(backbone=cfg.backbone).to(cfg.device)\n",
    "\n",
    "# Check initial model output shape with a dummy input on the correct device\n",
    "if train_dl:\n",
    "    try:\n",
    "        dummy_input = torch.randn(cfg.batch_size, 5, 1000, 70).to(cfg.device)\n",
    "        with torch.no_grad():\n",
    "             dummy_output = model(dummy_input)\n",
    "        print(f\"Dummy model output shape: {dummy_output.shape}\")\n",
    "        expected_output_shape = (cfg.batch_size, 1, 70, 70)\n",
    "        if dummy_output.shape != expected_output_shape:\n",
    "             print(f\"Warning: Model output shape {dummy_output.shape} does not match expected {expected_output_shape}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during dummy model forward pass: {e}\")\n",
    "\n",
    "\n",
    "if cfg.ema:\n",
    "    if cfg.local_rank == 0:\n",
    "        print(\"Initializing EMA model..\")\n",
    "    # Initialize EMA model on the same device as the main model\n",
    "    ema_model = ModelEMA(\n",
    "        model,\n",
    "        decay=cfg.ema_decay,\n",
    "        device=cfg.device,\n",
    "    )\n",
    "else:\n",
    "    ema_model = None\n",
    "\n",
    "# criterion = nn.L1Loss() # Original criterion\n",
    "# Common losses for regression maps: MSELoss, L1Loss, HuberLoss. L1 is less sensitive to outliers.\n",
    "# For map-like output, often MSE or smooth L1 are used.\n",
    "criterion = nn.MSELoss() # Let's use MSELoss as it's common for regression\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Removed GradScaler - Re-add if using float16 AMP during training for stability\n",
    "# If using bfloat16, a scaler is often not strictly necessary but can still help in some cases.\n",
    "# The current code does NOT use a scaler with AMP. This might be unstable with float16.\n",
    "# For bfloat16, it's usually okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b33ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "\n",
    "best_loss= float('inf') # Initialize with infinity\n",
    "val_loss= float('inf')  # Initialize val_loss for logging on epoch 0\n",
    "\n",
    "\n",
    "if RUN_TRAIN and train_dl and valid_dl: # Only run training if data is available\n",
    "    print(\"\\nStarting Training Loop...\")\n",
    "    for epoch in range(1, cfg.epochs+1): # Start from epoch 1\n",
    "\n",
    "        tstart = time.time()\n",
    "        model.train()\n",
    "        total_loss = []\n",
    "\n",
    "        # tqdm for train loop\n",
    "        train_loop = tqdm(train_dl, disable=cfg.local_rank != 0, desc=f\"Epoch {epoch} Training\")\n",
    "        for i, (x, y) in enumerate(train_loop):\n",
    "            x = x.to(cfg.device)\n",
    "            y = y.to(cfg.device)\n",
    "\n",
    "            # AMP context is handled inside the model's forward method now\n",
    "            logits = model(x)\n",
    "\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            # Standard backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            if ema_model is not None:\n",
    "                ema_model.update(model)\n",
    "\n",
    "            # Update tqdm description periodically\n",
    "            if i % cfg.logging_steps == 0:\n",
    "                 train_loop.set_postfix(loss=f\"{np.mean(total_loss):.4f}\")\n",
    "\n",
    "\n",
    "        avg_train_loss = np.mean(total_loss)\n",
    "        tend_train = time.time()\n",
    "        train_time = tend_train - tstart\n",
    "\n",
    "        # ========== Validation ==========\n",
    "        model.eval()\n",
    "        val_logits = []\n",
    "        val_targets = []\n",
    "        # tqdm for validation loop\n",
    "        valid_loop = tqdm(valid_dl, disable=cfg.local_rank != 0, desc=f\"Epoch {epoch} Validation\")\n",
    "        with torch.no_grad():\n",
    "            for x, y in valid_loop:\n",
    "                x = x.to(cfg.device)\n",
    "                y = y.to(cfg.device)\n",
    "\n",
    "                # Use EMA model if available for validation\n",
    "                current_model = ema_model.module if ema_model is not None else model\n",
    "\n",
    "                # AMP context is handled inside the model's forward method\n",
    "                out = current_model(x)\n",
    "\n",
    "                val_logits.append(out.cpu())\n",
    "                val_targets.append(y.cpu())\n",
    "\n",
    "                valid_loop.set_postfix(batch_loss=f\"{criterion(out, y).item():.4f}\")\n",
    "\n",
    "\n",
    "            val_logits= torch.cat(val_logits, dim=0)\n",
    "            val_targets= torch.cat(val_targets, dim=0)\n",
    "\n",
    "            val_loss = criterion(val_logits, val_targets).item()\n",
    "\n",
    "\n",
    "        tend_val = time.time()\n",
    "        val_time = tend_val - tend_train\n",
    "\n",
    "        # --- Logging ---\n",
    "        epoch_time = time.time() - tstart\n",
    "        print(f\"Epoch {epoch}/{cfg.epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Time: {format_time(epoch_time)}\")\n",
    "\n",
    "        # --- Early Stopping & Checkpointing ---\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            cfg.early_stopping[\"streak\"] = 0\n",
    "            print(f\"Validation loss improved. Saving best model...\")\n",
    "            # Save model state_dict\n",
    "            model_to_save = ema_model.module if ema_model is not None else model\n",
    "            torch.save(model_to_save.state_dict(), f\"best_model_epoch_{epoch:03d}.pth\")\n",
    "        else:\n",
    "            cfg.early_stopping[\"streak\"] += 1\n",
    "            print(f\"Validation loss did not improve. Early stopping streak: {cfg.early_stopping['streak']}/{cfg.early_stopping['patience']}\")\n",
    "\n",
    "        if cfg.early_stopping[\"streak\"] >= cfg.early_stopping[\"patience\"]:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nTraining finished.\")\n",
    "\n",
    "elif RUN_VALID and valid_dl: # Only run validation if requested and data is available\n",
    "    print(\"\\nStarting Validation Only...\")\n",
    "    model.eval()\n",
    "    val_logits = []\n",
    "    val_targets = []\n",
    "    valid_loop = tqdm(valid_dl, disable=cfg.local_rank != 0, desc=\"Validation Only\")\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loop:\n",
    "            x = x.to(cfg.device)\n",
    "            y = y.to(cfg.device)\n",
    "\n",
    "            current_model = ema_model.module if ema_model is not None else model\n",
    "            out = current_model(x)\n",
    "\n",
    "            val_logits.append(out.cpu())\n",
    "            val_targets.append(y.cpu())\n",
    "\n",
    "            valid_loop.set_postfix(batch_loss=f\"{criterion(out, y).item():.4f}\")\n",
    "\n",
    "        val_logits = torch.cat(val_logits, dim=0)\n",
    "        val_targets = torch.cat(val_targets, dim=0)\n",
    "        val_loss = criterion(val_logits, val_targets).item()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Example of how to load the best model for inference\n",
    "if not RUN_TRAIN and not RUN_VALID and RUN_TEST and valid_dl:\n",
    "    print(\"\\nLoading best model and running inference on validation set...\")\n",
    "    # Ensure a model file exists, load the one with the best loss from training\n",
    "    # Or specify a path to load a specific model\n",
    "    model_path = glob.glob(\"best_model_epoch_*.pth\")\n",
    "    if model_path:\n",
    "        # Find the file with the highest epoch number if multiple exist\n",
    "        latest_model_path = sorted(model_path)[-1]\n",
    "        print(f\"Loading model from: {latest_model_path}\")\n",
    "        model.load_state_dict(torch.load(latest_model_path, map_location=cfg.device))\n",
    "        model.eval()\n",
    "\n",
    "        test_logits = []\n",
    "        test_targets = []\n",
    "        test_loop = tqdm(valid_dl, disable=cfg.local_rank != 0, desc=\"Inference\")\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loop:\n",
    "                x = x.to(cfg.device)\n",
    "                y = y.to(cfg.device)\n",
    "                out = model(x) # Model's forward handles flip augmentation at inference time\n",
    "                test_logits.append(out.cpu())\n",
    "                test_targets.append(y.cpu())\n",
    "\n",
    "        test_logits = torch.cat(test_logits, dim=0)\n",
    "        test_targets = torch.cat(test_targets, dim=0)\n",
    "        test_loss = criterion(test_logits, test_targets).item()\n",
    "        print(f\"Inference Loss on Validation Set: {test_loss:.4f}\")\n",
    "\n",
    "        # Optional: Visualize some predictions\n",
    "        # num_visualize = min(5, len(test_logits))\n",
    "        # fig, axes = plt.subplots(num_visualize, 2, figsize=(10, num_visualize * 5))\n",
    "        # for i in range(num_visualize):\n",
    "        #      axes[i, 0].imshow(test_targets[i, 0].numpy(), cmap='viridis')\n",
    "        #      axes[i, 0].set_title(\"Ground Truth\")\n",
    "        #      axes[i, 0].axis('off')\n",
    "        #      axes[i, 1].imshow(test_logits[i, 0].numpy(), cmap='viridis')\n",
    "        #      axes[i, 1].set_title(\"Prediction\")\n",
    "        #      axes[i, 1].axis('off')\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"No saved model found for testing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
