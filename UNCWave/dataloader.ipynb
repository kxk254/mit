{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d49c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dev\\mit\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Add necessary imports at the top\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import timm\n",
    "from timm.models.convnext import ConvNeXtBlock\n",
    "from types import MethodType\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time, glob\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "import sys # Added for stderr\n",
    "from copy import deepcopy\n",
    "\n",
    "RUN_TRAIN = True # bfloat16 or float32 recommended\n",
    "RUN_VALID = False\n",
    "RUN_TEST  = False\n",
    "USE_DEVICE = 'GPU' #'CPU'  # 'GPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c93eabc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Original cfg setup - replace or update as needed\n",
    "# For this code, we add transformer specific config here\n",
    "cfg= SimpleNamespace()\n",
    "cfg.device = torch.device(\"cuda\" if torch.cuda.is_available() and torch.cuda.device_count() > 0 else \"cpu\")\n",
    "# cfg.local_rank = 0 # Assume single GPU/CPU for simplicity unless distributed training is set up\n",
    "cfg.seed = 123\n",
    "cfg.subsample = 100 #None # Set to None to use all available samples\n",
    "\n",
    "# Assuming file paths are set up correctly\n",
    "data_paths_str = \".\\\\datasetfiles\\\\FlatVel_A\\\\data\\\\*.npy\"\n",
    "label_paths_str = \".\\\\datasetfiles\\\\FlatVel_A\\\\model\\\\*.npy\"\n",
    "\n",
    "# Get all file pairs\n",
    "# cfg.file_pairs = list(zip(sorted(glob.glob(data_paths_str)), sorted(glob.glob(label_paths_str))))\n",
    "# Split file pairs for train/validation\n",
    "data_paths = sorted(glob.glob(data_paths_str))\n",
    "label_paths = sorted(glob.glob(label_paths_str))\n",
    "all_file_pairs = list(zip(data_paths, label_paths))\n",
    "# Simple split (e.g., 80% train, 20% validation)\n",
    "split_ratio = 0.8\n",
    "split_idx = int(len(all_file_pairs) * split_ratio)\n",
    "train_file_pairs = all_file_pairs[:split_idx]\n",
    "valid_file_pairs = all_file_pairs[split_idx:]\n",
    "\n",
    "\n",
    "cfg.backbone = \"convnext_small.fb_in22k_ft_in1k\"\n",
    "cfg.ema = True\n",
    "cfg.ema_decay = 0.99\n",
    "\n",
    "cfg.epochs = 4\n",
    "cfg.batch_size = 8\n",
    "cfg.batch_size_val = 8\n",
    "\n",
    "cfg.early_stopping = {\"patience\": 3, \"streak\": 0}\n",
    "cfg.logging_steps = 10\n",
    "# --- New Transformer/Dataset related config ---\n",
    "cfg.num_input_slices = 5 # Number of consecutive input samples (time slices) to stack as channels\n",
    "# Inferred input height (H_in) for a single sample based on original stem logic transforming T=1000\n",
    "# This is highly speculative and assumes the original dataset or stem implicitly maps 1000 -> 352\n",
    "cfg.inferred_input_height = 352\n",
    "cfg.input_width = 70 # Original waveform width and target output width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc94a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDatasetWithSlices(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg,\n",
    "        file_pairs,  # list of (data_path, label_path) tuples for this specific split\n",
    "        mode = \"train\",\n",
    "        num_input_slices: int = 3, # Number of consecutive samples to stack\n",
    "        # inferred_input_height: int = 352, # Assumed H_in from original data processing\n",
    "        # input_width: int = 70, # W_in from original data processing\n",
    "        # output_height: int = 70, # H_out for labels\n",
    "        # output_width: int = 70,  # W_out for labels\n",
    "    ):\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        print(\"[init]-self mode\", self.mode)\n",
    "        self.file_pairs = file_pairs\n",
    "        print(\"[init]-self file pairs\", self.file_pairs)\n",
    "        self.num_input_slices = num_input_slices\n",
    "        print(\"[init]-self num_input_slices\", self.num_input_slices)\n",
    "        # self.inferred_input_height = inferred_input_height # Use from cfg\n",
    "        # self.input_width = input_width # Use from cfg\n",
    "        # self.output_height = output_height # Use from cfg\n",
    "        # self.output_width = output_width # Use from cfg\n",
    "\n",
    "        # Load data.\n",
    "        # ASSUMPTION: Each file pair (data.npy, model.npy) loads arrays\n",
    "        #   - data.npy contains (N_samples_per_file, 5, H_in, 70)\n",
    "        #   - model.npy contains (N_samples_per_file, 70, 70)\n",
    "        # Where N_samples_per_file is the number of pre-processed samples in that file,\n",
    "        # 5 is the fixed channel count, H_in is the fixed processed height (inferred from original stem logic),\n",
    "        # and 70 is the width.\n",
    "        self.data_arrays, self.label_arrays = self._load_data_arrays()\n",
    "        print(f\"[init]-self data_arrays {self.data_arrays[0][0][0][0][:3]} shape {self.data_arrays[0].shape} | label_arrays: {self.label_arrays[0][0][0][0][:3]}  shape {self.label_arrays[0].shape} \")\n",
    "\n",
    "        # Validate and store shapes based on loaded data\n",
    "        if not self.data_arrays:\n",
    "             raise RuntimeError(f\"[init]-No data files loaded for mode '{self.mode}'. Check file paths and format.\")\n",
    "\n",
    "        # Use the shape of the first loaded array to define dataset dimensions\n",
    "        first_data_shape = self.data_arrays[0].shape\n",
    "        first_label_shape = self.label_arrays[0].shape\n",
    "\n",
    "        self.samples_per_file_ = first_data_shape[0] # Number of samples per file\n",
    "        self.channels_in_single = first_data_shape[1] # Should be 5\n",
    "        self.H_in = self.data_arrays[0].shape[2]        # Assumed fixed input height (matches cfg.inferred_input_height)\n",
    "        self.W_in = self.data_arrays[0].shape[3]        # Should be 70 (matches cfg.input_width)\n",
    "        self.H_out = self.label_arrays[0].shape[2]      # Should be 70 (matches cfg.output_height)\n",
    "        self.W_out = self.label_arrays[0].shape[3]      # Should be 70 (matches cfg.output_width)\n",
    "\n",
    "        # Validate against cfg expectations (optional but good)\n",
    "        if self.channels_in_single != 5:\n",
    "             print(f\"[init]-Warning: Loaded data has {self.channels_in_single} channels, expected 5.\", file=sys.stderr)\n",
    "        # Note: Cannot strictly validate H_in against cfg.inferred_input_height here\n",
    "        # as the cfg value was just an inference based on the original stem.\n",
    "        # The loaded data's shape dictates the true H_in for the model.\n",
    "        # self.cfg.inferred_input_height = self.H_in # Update cfg with actual loaded height\n",
    "        if self.W_in != 70:\n",
    "            print(f\"[init]-Warning: Loaded data has width {self.W_in}, expected 70.\", file=sys.stderr)\n",
    "            self.cfg.input_width = self.W_in # Update cfg\n",
    "        if self.H_out != 70 or self.W_out != 70:\n",
    "            print(f\"[init]-Warning: Loaded labels have shape ({self.H_out}, {self.W_out}), expected (70, 70).\", file=sys.stderr)\n",
    "            # Update cfg with actual dimensions if needed downstream\n",
    "            self.cfg.output_height = self.H_out\n",
    "            self.cfg.output_width = self.W_out\n",
    "        \n",
    "        # Update cfg with actual loaded dimensions for model compatibility\n",
    "        self.cfg.inferred_input_height = self.H_in\n",
    "        print(f\"[init]-self.cfg.inferred_input_height {self.cfg.inferred_input_height} \")\n",
    "        self.cfg.input_width = self.W_in # Matches W_in\n",
    "        print(f\"[init]-self.cfg.input_width {self.cfg.input_width} \")\n",
    "\n",
    "        total_files = len(self.data_arrays)\n",
    "        print(f\"[init]-total_files {total_files} \")\n",
    "        # Total number of *original* samples across all successfully loaded files\n",
    "        total_samples_available = total_files * self.samples_per_file_\n",
    "\n",
    "        subsample = getattr(self.cfg, \"subsample\", None)\n",
    "        print(f\"[init]-subsample {subsample} \")\n",
    "        # Determine the total number of *effective* samples based on subsampling,\n",
    "        # but the actual count in index_map might be slightly less due to padding\n",
    "        # requirements at file boundaries.\n",
    "        effective_subsample_limit = subsample if subsample and subsample > 0 else float('inf')\n",
    "        print(f\"[init]-effective_subsample_limit {effective_subsample_limit} \")\n",
    "\n",
    "        # Build list of (file_idx, sample_center_idx) pairs\n",
    "        # We select num_input_slices consecutive samples centered at sample_center_idx.\n",
    "        pad = (self.num_input_slices - 1) // 2\n",
    "        print(f\"[init]-pad {pad} \")\n",
    "\n",
    "        self.index_map = []\n",
    "        current_effective_samples = 0\n",
    "        \n",
    "        for file_idx in range(total_files):\n",
    "            print(f\"[init]-file_idx {file_idx} \")\n",
    "            N_samples_in_file = self.data_arrays[file_idx].shape[0] # Number of samples in this file's array\n",
    "\n",
    "            # Iterate through possible center sample indices such that the window\n",
    "            # [center - pad, center + pad] is entirely within [0, N_samples_in_file - 1].\n",
    "            valid_start_idx = pad\n",
    "            valid_end_idx = N_samples_in_file - pad - 1 # Inclusive end index for center\n",
    "\n",
    "            # Check if there are enough samples in the file to form *at least one* window\n",
    "            if valid_end_idx < valid_start_idx:\n",
    "                print(f\"[init]-Warning: File {file_idx} (with {N_samples_in_file} samples) is too short for window size {self.num_input_slices}. Skipping file for effective samples.\", file=sys.stderr)\n",
    "                continue # Skip this file if not enough samples for any window\n",
    "\n",
    "            for sample_center_idx in range(valid_start_idx, valid_end_idx + 1):\n",
    "                 self.index_map.append((file_idx, sample_center_idx))\n",
    "                 current_effective_samples += 1\n",
    "                 # Stop if subsample limit is reached for effective samples\n",
    "                 if current_effective_samples >= effective_subsample_limit:\n",
    "                     break\n",
    "            # Stop if subsample limit is reached for effective samples\n",
    "            if current_effective_samples >= effective_subsample_limit:\n",
    "                 break\n",
    "\n",
    "        self.total_effective_samples = len(self.index_map)\n",
    "\n",
    "        print(f\"[init]-Dataset initialized in {self.mode} mode.\")\n",
    "        print(f\"[init]-Loaded {total_files} file pairs containing a total of {total_samples_available} raw samples.\")\n",
    "        print(f\"[init]-Input shape per single slice: ({self.channels_in_single}, {self.H_in}, {self.W_in})\")\n",
    "        print(f\"[init]-Output label shape: ({self.H_out}, {self.W_out})\")\n",
    "        print(f\"[init]-Window size for stacking: {self.num_input_slices} slices (padding {pad} on each side).\")\n",
    "        print(f\"[init]-Generated {self.total_effective_samples} effective samples for training/validation after considering windowing and subsampling.\")\n",
    "\n",
    "\n",
    "    def _load_data_arrays(self):\n",
    "        \"\"\"\n",
    "        Loads data and label arrays from file pairs using mmap_mode for efficiency.\n",
    "        Includes validation for expected shapes.\n",
    "        \"\"\"\n",
    "        data_arrays_list = []\n",
    "        label_arrays_list = []\n",
    "        # Use 'r' mode always for memory efficiency with large datasets\n",
    "        mmap_mode = \"r\"\n",
    "\n",
    "        print(f\"[_load_data_arrays] - Loading {self.mode} data using mmap_mode='{mmap_mode}'...\")\n",
    "\n",
    "        # Use local_rank to ensure tqdm is only shown on the main process in DDP\n",
    "        disable_tqdm = getattr(self.cfg, 'local_rank', 0) != 0\n",
    "\n",
    "        successful_loads = 0\n",
    "        for data_fpath, label_fpath in tqdm(\n",
    "                        self.file_pairs, desc=f\"[_load_data_arrays / for loop] - Loading {self.mode} data (mmap)\",\n",
    "                        disable=disable_tqdm):\n",
    "            print(f\"[_load_data_arrays / for data_fpath] - data_fpath {data_fpath} | label_fpath={label_fpath} \")\n",
    "            try:\n",
    "                # Check if files exist before attempting to load\n",
    "                if not os.path.exists(data_fpath):\n",
    "                    print(f\"[_load_data_arrays / try] - Warning: Data file not found: {data_fpath}. Skipping pair.\", file=sys.stderr)\n",
    "                    continue\n",
    "                if not os.path.exists(label_fpath):\n",
    "                    print(f\"[_load_data_arrays / try] - Warning: Label file not found: {label_fpath}. Skipping pair.\", file=sys.stderr)\n",
    "                    continue\n",
    "\n",
    "                # Load data with expected shape (N_samples, Channels, H_in, W_in)\n",
    "                # For your data: (N, 5, 1000, 70)\n",
    "                arr = np.load(data_fpath, mmap_mode=mmap_mode)\n",
    "                print(f\"[_load_data_arrays / try] - arr {arr[0][0][0][:10]} | arr shape {arr.shape}\")\n",
    "                # Load labels with expected shape (N_samples, H_out, W_out)\n",
    "                # For your data: (N, 70, 70)\n",
    "                lbl = np.load(label_fpath, mmap_mode=mmap_mode)\n",
    "                print(f\"[_load_data_arrays / try] - lbl {lbl[0][0][0][:10]} | lbl shape {lbl.shape} | lbl ndim {lbl.ndim}\")\n",
    "\n",
    "                # --- Basic shape validation based on YOUR specified structure ---\n",
    "                expected_data_ndim = 4\n",
    "                expected_label_ndim = 4\n",
    "                expected_channels = 5 # Your specific channel count\n",
    "                expected_data_width = 70 # Your specific GeoPhones dimension\n",
    "                expected_label_height = 70 # Your specific output height\n",
    "                expected_label_width = 70  # Your specific output width\n",
    "                print(f\"[_load_data_arrays / expected_label_width] - expected_label_width {expected_label_width} \")\n",
    "\n",
    "                if arr.ndim != expected_data_ndim or \\\n",
    "                   arr.shape[1] != expected_channels or \\\n",
    "                   arr.shape[3] != expected_data_width:\n",
    "                     print(\"[_load_data_arrays / if 1] \")\n",
    "                     print(f\"[_load_data_arrays / try / if] - Warning: Data file {data_fpath} has unexpected shape {arr.shape}. \"\n",
    "                           f\"[_load_data_arrays / try / if] - Expected ndim={expected_data_ndim}, shape[1]={expected_channels} (channels), \"\n",
    "                           f\"[_load_data_arrays / try / if] - shape[3]={expected_data_width} (width/geophones). Skipping.\", file=sys.stderr)\n",
    "                     continue\n",
    "\n",
    "                if lbl.ndim != expected_label_ndim or \\\n",
    "                   lbl.shape[2] != expected_label_height or \\\n",
    "                   lbl.shape[3] != expected_label_width:\n",
    "                     print(\"[_load_data_arrays / if 2] \")\n",
    "                     print(f\"[_load_data_arrays / try / if2] - Warning: Label file {label_fpath} has unexpected shape {lbl.shape}. \"\n",
    "                           f\"[_load_data_arrays / try / if2] - Expected ndim={expected_label_ndim}, shape[1]={expected_label_height} (height), \"\n",
    "                           f\"[_load_data_arrays / try / if2] - shape[2]={expected_label_width} (width). Skipping.\", file=sys.stderr)\n",
    "                     continue\n",
    "\n",
    "                # Validate that the number of samples (batch dimension) matches\n",
    "                if arr.shape[0] != lbl.shape[0]:\n",
    "                     print(\"[_load_data_arrays / if 3] \")\n",
    "                     print(f\"[_load_data_arrays / try / if3] - Warning: Mismatch in number of samples (batch size) between data ({arr.shape[0]}) and label ({lbl.shape[0]}) \"\n",
    "                           f\"[_load_data_arrays / try / if3] - in file pair {data_fpath}, {label_fpath}. Skipping.\", file=sys.stderr)\n",
    "                     continue\n",
    "\n",
    "                # If it passes validation, add to lists\n",
    "                print(\"[_load_data_arrays / try / before --- data_arrays_list] \")\n",
    "                data_arrays_list.append(arr)\n",
    "                print(f\"[_load_data_arrays / try / data_arrays_list] - data_arrays_list {data_arrays_list[0][0][0][0][:3]} | data shape {arr.shape}\")\n",
    "                label_arrays_list.append(lbl)\n",
    "                print(f\"[_load_data_arrays / try / label_arrays_list] - label_arrays_list {label_arrays_list[0][0][0][0][:3]} | label shape {lbl.shape} \")\n",
    "                successful_loads += 1\n",
    "                print(f\"[_load_data_arrays / try / successful_loads] - successful_loads {successful_loads} \")\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                # This check is now redundant with the os.path.exists check above,\n",
    "                # but keeping it doesn't hurt as a fallback.\n",
    "                print(f\"[_load_data_arrays / except] - Error: File not found - {data_fpath} or {label_fpath}. Skipping pair.\", file=sys.stderr)\n",
    "            except Exception as e:\n",
    "                print(f\"[_load_data_arrays / except Exception] - Error loading or validating file pair: {data_fpath}, {label_fpath}\", file=sys.stderr)\n",
    "                print(f\"[_load_data_arrays / except Exception] - Error details: {e}\", file=sys.stderr)\n",
    "                # traceback.print_exc() # Uncomment for detailed error\n",
    "                continue\n",
    "\n",
    "        # if self.cfg.local_rank == 0: # Only print summary from main process\n",
    "        print(f\"[_load_data_arrays / try / if - end] -Finished loading {successful_loads} out of {len(self.file_pairs)} file pairs successfully for {self.mode} mode.\")\n",
    "\n",
    "        return data_arrays_list, label_arrays_list\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of effective samples available in the dataset.\n",
    "        \"\"\"\n",
    "        print(\"[__len__]\")\n",
    "        return self.total_effective_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves a single effective sample (input and label) based on the index.\n",
    "        An effective sample consists of num_input_slices data slices stacked,\n",
    "        and the corresponding label for the center slice.\n",
    "        \"\"\"\n",
    "        print(\"[__getitem__]\")\n",
    "        if index < 0 or index >= self.total_effective_samples:\n",
    "            raise IndexError(f\"[__getitem__] - Index {index} out of bounds for dataset of size {self.total_effective_samples}\")\n",
    "\n",
    "        # Get the file index and the center sample index within that file\n",
    "        file_idx, sample_center_idx = self.index_map[index]\n",
    "        print(f\"[__getitem__] - file_idx: {file_idx} | sample_center_idx: {sample_center_idx}\")\n",
    "\n",
    "        # Calculate the start and end indices for the window of slices\n",
    "        pad = (self.num_input_slices - 1) // 2\n",
    "        start_idx = sample_center_idx - pad\n",
    "        end_idx = sample_center_idx + pad # This is inclusive\n",
    "\n",
    "        # Retrieve the batch of consecutive data slices\n",
    "        # The shape will be (num_input_slices, Channels, H_in, W_in)\n",
    "        data_slices = self.data_arrays[file_idx][start_idx : end_idx + 1, ...]\n",
    "\n",
    "        # Retrieve the label for the *center* slice\n",
    "        # The shape will be (H_out, W_out)\n",
    "        label_slice = self.label_arrays[file_idx][sample_center_idx, ...]\n",
    "\n",
    "        # --- Stack the data slices ---\n",
    "        # The original shape is (num_input_slices, Channels, H_in, W_in)\n",
    "        # We want to combine the 'num_input_slices' and 'Channels' dimensions\n",
    "        # into a single channel dimension, resulting in (num_input_slices * Channels, H_in, W_in).\n",
    "        # This is a common way to represent stacked time series data as input channels for CNNs.\n",
    "        combined_channels = self.num_input_slices * self.channels_in_single\n",
    "        input_tensor = data_slices.reshape(combined_channels, self.H_in, self.W_in)\n",
    "\n",
    "        # Convert numpy arrays to PyTorch tensors\n",
    "        # Ensure correct data types (float32 is common for model inputs/outputs)\n",
    "        input_tensor = torch.from_numpy(input_tensor).float()\n",
    "        label_tensor = torch.from_numpy(label_slice).float() # Assuming regression output\n",
    "\n",
    "        return input_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "658c42c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init]-self mode train\n",
      "[init]-self file pairs [('.\\\\datasetfiles\\\\FlatVel_A\\\\data\\\\data1.npy', '.\\\\datasetfiles\\\\FlatVel_A\\\\model\\\\model1.npy')]\n",
      "[init]-self num_input_slices 5\n",
      "[_load_data_arrays] - Loading train data using mmap_mode='r'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[_load_data_arrays / for loop] - Loading train data (mmap): 100%|██████████| 1/1 [00:00<00:00, 667.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_load_data_arrays / for data_fpath] - data_fpath .\\datasetfiles\\FlatVel_A\\data\\data1.npy | label_fpath=.\\datasetfiles\\FlatVel_A\\model\\model1.npy \n",
      "[_load_data_arrays / try] - arr [-0.00038193  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ] | arr shape (500, 5, 1000, 70)\n",
      "[_load_data_arrays / try] - lbl [1524. 1524. 1524. 1524. 1524. 1524. 1524. 1524. 1524. 1524.] | lbl shape (500, 1, 70, 70) | lbl ndim 4\n",
      "[_load_data_arrays / expected_label_width] - expected_label_width 70 \n",
      "[_load_data_arrays / try / before --- data_arrays_list] \n",
      "[_load_data_arrays / try / data_arrays_list] - data_arrays_list [-0.00038193  0.          0.        ] | data shape (500, 5, 1000, 70)\n",
      "[_load_data_arrays / try / label_arrays_list] - label_arrays_list [1524. 1524. 1524.] | label shape (500, 1, 70, 70) \n",
      "[_load_data_arrays / try / successful_loads] - successful_loads 1 \n",
      "[_load_data_arrays / try / if - end] -Finished loading 1 out of 1 file pairs successfully for train mode.\n",
      "[init]-self data_arrays [-0.00038193  0.          0.        ] shape (500, 5, 1000, 70) | label_arrays: [1524. 1524. 1524.]  shape (500, 1, 70, 70) \n",
      "[init]-self.cfg.inferred_input_height 1000 \n",
      "[init]-self.cfg.input_width 70 \n",
      "[init]-total_files 1 \n",
      "[init]-subsample 100 \n",
      "[init]-effective_subsample_limit 100 \n",
      "[init]-pad 2 \n",
      "[init]-file_idx 0 \n",
      "[init]-Dataset initialized in train mode.\n",
      "[init]-Loaded 1 file pairs containing a total of 500 raw samples.\n",
      "[init]-Input shape per single slice: (5, 1000, 70)\n",
      "[init]-Output label shape: (70, 70)\n",
      "[init]-Window size for stacking: 5 slices (padding 2 on each side).\n",
      "[init]-Generated 100 effective samples for training/validation after considering windowing and subsampling.\n",
      "[__len__]\n",
      "[__len__]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_ds = CustomDatasetWithSlices(cfg=cfg, file_pairs=train_file_pairs, mode=\"train\", num_input_slices=cfg.num_input_slices)\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size= cfg.batch_size,\n",
    "    num_workers= 4 if cfg.device.type == 'cuda' else 0, # Use more workers on GPU\n",
    "    shuffle=True,\n",
    "    pin_memory=cfg.device.type == 'cuda', # Pin memory for faster GPU transfer\n",
    "    drop_last=True, # Drop last batch if batch size doesn't divide dataset size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
