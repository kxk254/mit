{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf93b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dev\\mit\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "RUN_TRAIN = True # bfloat16 or float32 recommended\n",
    "RUN_VALID = False\n",
    "RUN_TEST  = False\n",
    "USE_DEVICE = 'GPU' #'CPU'  # 'GPU'\n",
    "\n",
    "import random\n",
    "import os\n",
    "import time, glob\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset # Use standard DataLoader/Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from _cfg import cfg\n",
    "import torch\n",
    "\n",
    "try: \n",
    "    import monai\n",
    "except: \n",
    "    !pip install --no-deps monai -q\n",
    "\n",
    "data_paths_str = \"./datasetfiles/FlatVel_A/data/*.npy\"\n",
    "label_paths_str = \"./datasetfiles/FlatVel_A/model/*.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0f8eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting _cfg.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile _cfg.py\n",
    "\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "cfg= SimpleNamespace()\n",
    "cfg.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg.local_rank = 0\n",
    "cfg.seed = 123\n",
    "cfg.subsample = 10 #None\n",
    "\n",
    "cfg.file_pairs = list(zip(sorted(glob.glob(\"./datasetfiles/FlatVel_A/data/*.npy\")), sorted(glob.glob(\"./datasetfiles/FlatVel_A/model/*.npy\"))))\n",
    "# cfg.file_pairs = list(zip(data_paths, label_paths))\n",
    "data_paths = sorted(glob.glob(\"./datasetfiles/FlatVel_A/data/*.npy\"))\n",
    "label_paths = sorted(glob.glob(\"./datasetfiles/FlatVel_A/model/*.npy\"))\n",
    "cfg.backbone = \"convnext_small.fb_in22k_ft_in1k\"\n",
    "cfg.ema = True\n",
    "cfg.ema_decay = 0.99\n",
    "\n",
    "cfg.epochs = 1\n",
    "cfg.batch_size = 8  # 16\n",
    "cfg.batch_size_val = 8 # 16\n",
    "\n",
    "cfg.early_stopping = {\"patience\": 3, \"streak\": 0}\n",
    "cfg.logging_steps = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c262b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile _dataset.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from types import MethodType \n",
    "import collections\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        cfg,\n",
    "        file_pairs,  #list of (data_path, label_path) tuples for this specific split\n",
    "        mode = \"train\", \n",
    "    ):\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "        self.file_pairs = file_pairs\n",
    "        \n",
    "        self.data, self.labels = self._load_data_arrays()\n",
    "\n",
    "        self.samples_per_file = 500  # assuming each file has 500 time steps\n",
    "        total_samples_available = len(self.data) * self.samples_per_file\n",
    "\n",
    "        # Subsample logic\n",
    "        subsample = getattr(self.cfg, \"subsample\", None)\n",
    "        self.total_samples = min(subsample, total_samples_available) if subsample else total_samples_available\n",
    "\n",
    "        \n",
    "        # Build list of (file_idx, time_step_idx) pairs\n",
    "        self.index_map = []\n",
    "        for file_idx in range(len(self.data)):\n",
    "            for time_step_idx in range(self.samples_per_file):\n",
    "                self.index_map.append((file_idx, time_step_idx))\n",
    "                if len(self.index_map) >= self.total_samples:\n",
    "                    break\n",
    "            if len(self.index_map) >= self.total_samples:\n",
    "                break\n",
    "\n",
    "    def _load_data_arrays(self, ):\n",
    "               \n",
    "        data_arrays = []\n",
    "        label_arrays = []\n",
    "        mmap_mode = \"r\"\n",
    "\n",
    "        for data_fpath, label_fpath in tqdm(\n",
    "                        self.file_pairs, desc=f\"Loading {self.mode} data (mmap)\",\n",
    "                        disable=self.cfg.local_rank != 0):\n",
    "            try:\n",
    "                # Load the numpy arrays using memory mapping\n",
    "                arr = np.load(data_fpath, mmap_mode=mmap_mode)\n",
    "                lbl = np.load(label_fpath, mmap_mode=mmap_mode)\n",
    "                print(f\"Loaded {data_fpath}: {arr.shape}, {lbl.shape}\")\n",
    "                data_arrays.append(arr)\n",
    "                label_arrays.append(lbl)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: File not found - {data_fpath} r {label_fpath}\", file=sys.stderr)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file pari: {data_fpath}, {label_fpath}\", file=sys.stderr)\n",
    "                print(f\"Error: {e}\", file=sys.stderr)\n",
    "                continue\n",
    "\n",
    "            if self.cfg.local_rank == 0:\n",
    "                print(f\"Finished loading {len(data_arrays)} file pairs for {self.mode} mode.\")\n",
    "\n",
    "        return data_arrays, label_arrays\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # file_idx= idx // 500\n",
    "        # time_step_idx= idx % 500\n",
    "        # self.idx = idx\n",
    "\n",
    "        file_idx, time_step_idx = self.index_map[idx]\n",
    "        \n",
    "        x_full = self.data[file_idx]\n",
    "        y_full = self.labels[file_idx]\n",
    "\n",
    "        # --- Augmentations ---\n",
    "        # Apply augmentations to the full 3D blocks *before* slicing out the time step.\n",
    "        # Make copies after slicing and augmentation to ensure memory safety.\n",
    "        x_augmented = x_full\n",
    "        y_augmented = y_full\n",
    "\n",
    "        # Augs \n",
    "        if self.mode == \"train\":\n",
    "            \n",
    "            # Temporal flip\n",
    "            if np.random.random() < 0.5:\n",
    "                x_augmented = x_full[::-1, :, ::-1] # Time flip (dim 0), Spatial flip (dim 2)\n",
    "                y_augmented = y_full[..., ::-1]  # Spatial flip (dim 2) only\n",
    "\n",
    "        # --- Slicing and Copying ---\n",
    "        # Get the specific time step from the (potentially augmented) full array\n",
    "        # This reslts in a 2D array (Dim1, Dim2)\n",
    "        x_sample = x_augmented[time_step_idx, ...]\n",
    "        y_sample = y_augmented[time_step_idx, ...]\n",
    "\n",
    "        # make copies to return independent arrays/tensors.\n",
    "        # This is important especially with mmap and multiprocessing DataLoaders.\n",
    "        x_sample = x_sample.copy()\n",
    "        y_sample = y_sample.copy()\n",
    "\n",
    "        x_tensor = torch.from_numpy(x_sample).float()\n",
    "        y_tensor = torch.from_numpy(y_sample).float()\n",
    "        \n",
    "        return x_tensor, y_tensor\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return self.total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4cdf5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile _model.py\n",
    "\n",
    "from copy import deepcopy\n",
    "from types import MethodType\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "from timm.models.convnext import ConvNeXtBlock\n",
    "\n",
    "from monai.networks.blocks import UpSample, SubpixelUpsample\n",
    "\n",
    "####################\n",
    "## EMA + Ensemble ##\n",
    "####################\n",
    "\n",
    "class ModelEMA(nn.Module):\n",
    "    def __init__(self, model, decay=0.99, device=None):\n",
    "        super().__init__()\n",
    "        self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.device = device\n",
    "        if self.device is not None:\n",
    "            self.module.to(device=device)\n",
    "\n",
    "    def _update(self, model, update_fn):\n",
    "        with torch.no_grad():\n",
    "            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
    "                if self.device is not None:\n",
    "                    model_v = model_v.to(device=self.device)\n",
    "                ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)\n",
    "\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models).eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = None\n",
    "        \n",
    "        for m in self.models:\n",
    "            logits= m(x)\n",
    "            \n",
    "            if output is None:\n",
    "                output = logits\n",
    "            else:\n",
    "                output += logits\n",
    "                \n",
    "        output /= len(self.models)\n",
    "        return output\n",
    "        \n",
    "\n",
    "#############\n",
    "## Decoder ##\n",
    "#############\n",
    "\n",
    "class ConvBnAct2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding: int = 0,\n",
    "        stride: int = 1,\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        act_layer: nn.Module = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv= nn.Conv2d(\n",
    "            in_channels, \n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride, \n",
    "            padding=padding, \n",
    "            bias=False,\n",
    "        )\n",
    "        self.norm = norm_layer(out_channels) if norm_layer != nn.Identity else nn.Identity()\n",
    "        self.act= act_layer(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"[ConvBnAct2d] --forward | x shape {x.shape}\")\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SCSEModule2d(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sSE = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 1, 1), \n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"[SCSEModule2d] --forward | x shape {x.shape}\")\n",
    "        return x * self.cSE(x) + x * self.sSE(x)\n",
    "\n",
    "class Attention2d(nn.Module):\n",
    "    def __init__(self, name, **params):\n",
    "        super().__init__()\n",
    "        if name is None:\n",
    "            self.attention = nn.Identity(**params)\n",
    "        elif name == \"scse\":\n",
    "            self.attention = SCSEModule2d(**params)\n",
    "        else:\n",
    "            raise ValueError(\"Attention {} is not implemented\".format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"[Attention2d] --forward | x shape {x.shape}\")\n",
    "        return self.attention(x)\n",
    "\n",
    "class DecoderBlock2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels,\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = None,\n",
    "        intermediate_conv: bool = False,\n",
    "        upsample_mode: str = \"deconv\",\n",
    "        scale_factor: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Upsample block\n",
    "        if upsample_mode == \"pixelshuffle\":\n",
    "            self.upsample= SubpixelUpsample(\n",
    "                spatial_dims= 2,\n",
    "                in_channels= in_channels,\n",
    "                scale_factor= scale_factor,\n",
    "            )\n",
    "        else:\n",
    "            self.upsample = UpSample(\n",
    "                spatial_dims= 2,\n",
    "                in_channels= in_channels,\n",
    "                out_channels= in_channels,\n",
    "                scale_factor= scale_factor,\n",
    "                mode= upsample_mode,\n",
    "            )\n",
    "\n",
    "        if intermediate_conv:\n",
    "            k= 3\n",
    "            c= skip_channels if skip_channels != 0 else in_channels\n",
    "            self.intermediate_conv = nn.Sequential(\n",
    "                ConvBnAct2d(c, c, k, k//2),\n",
    "                ConvBnAct2d(c, c, k, k//2),\n",
    "                )\n",
    "        else:\n",
    "            self.intermediate_conv= None\n",
    "\n",
    "        self.attention1 = Attention2d(\n",
    "            name= attention_type, \n",
    "            in_channels= in_channels + skip_channels,\n",
    "            )\n",
    "\n",
    "        self.conv1 = ConvBnAct2d(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size= 3,\n",
    "            padding= 1,\n",
    "            norm_layer= norm_layer,\n",
    "        )\n",
    "\n",
    "        self.conv2 = ConvBnAct2d(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size= 3,\n",
    "            padding= 1,\n",
    "            norm_layer= norm_layer,\n",
    "        )\n",
    "        self.attention2 = Attention2d(\n",
    "            name= attention_type, \n",
    "            in_channels= out_channels,\n",
    "            )\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        print(f\"[DecoderBlock2d] -- forward x shape {x.shape} | skip shape {skip.shape}\")\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        if self.intermediate_conv is not None:\n",
    "            if skip is not None:\n",
    "                skip = self.intermediate_conv(skip)\n",
    "            else:\n",
    "                x = self.intermediate_conv(x)\n",
    "\n",
    "        if skip is not None:\n",
    "            # print(x.shape, skip.shape)\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention1(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetDecoder2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet decoder.\n",
    "    Source: https://arxiv.org/abs/1505.04597\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: tuple[int],\n",
    "        skip_channels: tuple[int] = None,\n",
    "        decoder_channels: tuple = (256, 128, 64, 32),\n",
    "        scale_factors: tuple = (2,2,2,2),\n",
    "        norm_layer: nn.Module = nn.Identity,\n",
    "        attention_type: str = None,\n",
    "        intermediate_conv: bool = False,\n",
    "        upsample_mode: str = \"deconv\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if len(encoder_channels) == 4:\n",
    "            decoder_channels= decoder_channels[1:]\n",
    "        self.decoder_channels= decoder_channels\n",
    "        \n",
    "        if skip_channels is None:\n",
    "            skip_channels= list(encoder_channels[1:]) + [0]\n",
    "\n",
    "        # Build decoder blocks\n",
    "        in_channels= [encoder_channels[0]] + list(decoder_channels[:-1])\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        for i, (ic, sc, dc) in enumerate(zip(in_channels, skip_channels, decoder_channels)):\n",
    "            # print(i, ic, sc, dc)\n",
    "            self.blocks.append(\n",
    "                DecoderBlock2d(\n",
    "                    ic, sc, dc, \n",
    "                    norm_layer= norm_layer,\n",
    "                    attention_type= attention_type,\n",
    "                    intermediate_conv= intermediate_conv,\n",
    "                    upsample_mode= upsample_mode,\n",
    "                    scale_factor= scale_factors[i],\n",
    "                    )\n",
    "            )\n",
    "\n",
    "    def forward(self, feats: list[torch.Tensor]):\n",
    "        res= [feats[0]]\n",
    "        feats= feats[1:]\n",
    "        print(f\"[UnetDecoder2d] - forward length of feats and shapes {len(feats)} | s0 shape {feats[0].shape} | s1 shape {feats[1].shape} |s2 shape {feats[2].shape} |\"  )\n",
    "\n",
    "        # Decoder blocks\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            skip= feats[i] if i < len(feats) else None\n",
    "            res.append(\n",
    "                b(res[-1], skip=skip),\n",
    "                )\n",
    "            \n",
    "        return res\n",
    "\n",
    "class SegmentationHead2d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        scale_factor: tuple[int] = (2,2),\n",
    "        kernel_size: int = 3,\n",
    "        mode: str = \"nontrainable\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv= nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size= kernel_size,\n",
    "            padding= kernel_size//2\n",
    "        )\n",
    "        self.upsample = UpSample(\n",
    "            spatial_dims= 2,\n",
    "            in_channels= out_channels,\n",
    "            out_channels= out_channels,\n",
    "            scale_factor= scale_factor,\n",
    "            mode= mode,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"[Segmentation Head 2d] forward--\")\n",
    "        x = self.conv(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "#############\n",
    "## Encoder ##\n",
    "#############\n",
    "\n",
    "def _convnext_block_forward(self, x):\n",
    "    shortcut = x\n",
    "    # print(\"[convnext block forward] :\", x.shape)\n",
    "    x = self.conv_dw(x)\n",
    "\n",
    "    if self.use_conv_mlp:\n",
    "        x = self.norm(x)\n",
    "        x = self.mlp(x)\n",
    "    else:\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "        x = self.mlp(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = x.contiguous()\n",
    "\n",
    "    if self.gamma is not None:\n",
    "        x = x * self.gamma.reshape(1, -1, 1, 1)\n",
    "\n",
    "    x = self.drop_path(x) + self.shortcut(shortcut)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: str,\n",
    "        pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "            \n",
    "        # Encoder\n",
    "        self.backbone= timm.create_model(\n",
    "            backbone,\n",
    "            in_chans= 5,\n",
    "            pretrained= pretrained,\n",
    "            features_only= True,\n",
    "            drop_path_rate=0.0,\n",
    "            )\n",
    "        ecs= [_[\"num_chs\"] for _ in self.backbone.feature_info][::-1]\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder= UnetDecoder2d(\n",
    "            encoder_channels= ecs,\n",
    "        )\n",
    "\n",
    "        self.seg_head= SegmentationHead2d(\n",
    "            in_channels= self.decoder.decoder_channels[-1],\n",
    "            out_channels= 1,\n",
    "            scale_factor= 1,\n",
    "        )\n",
    "        \n",
    "        self._update_stem(backbone)\n",
    "        \n",
    "        self.replace_activations(self.backbone, log=True)\n",
    "        self.replace_norms(self.backbone, log=True)\n",
    "        self.replace_forwards(self.backbone, log=True)\n",
    "\n",
    "    def _update_stem(self, backbone):\n",
    "        if backbone.startswith(\"convnext\"):\n",
    "\n",
    "            # Update stride\n",
    "            self.backbone.stem_0.stride = (4, 1)\n",
    "            self.backbone.stem_0.padding = (0, 2)\n",
    "\n",
    "            # Duplicate stem layer (to downsample height)\n",
    "            with torch.no_grad():\n",
    "                w = self.backbone.stem_0.weight\n",
    "                new_conv= nn.Conv2d(w.shape[0], w.shape[0], kernel_size=(4, 4), stride=(4, 1), padding=(0, 1))\n",
    "                new_conv.weight.copy_(w.repeat(1, (128//w.shape[1])+1, 1, 1)[:, :new_conv.weight.shape[1], :, :])\n",
    "                new_conv.bias.copy_(self.backbone.stem_0.bias)\n",
    "\n",
    "            self.backbone.stem_0= nn.Sequential(\n",
    "                nn.ReflectionPad2d((1,1,80,80)),\n",
    "                self.backbone.stem_0,\n",
    "                new_conv,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Custom striding not implemented.\")\n",
    "        pass\n",
    "\n",
    "    def replace_activations(self, module, log=False):\n",
    "        if log:\n",
    "            print(f\"Replacing all activations with GELU...\")\n",
    "        \n",
    "        # Apply activations\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, (\n",
    "                nn.ReLU, nn.LeakyReLU, nn.Mish, nn.Sigmoid, \n",
    "                nn.Tanh, nn.Softmax, nn.Hardtanh, nn.ELU, \n",
    "                nn.SELU, nn.PReLU, nn.CELU, nn.GELU, nn.SiLU,\n",
    "            )):\n",
    "                setattr(module, name, nn.GELU())\n",
    "            else:\n",
    "                self.replace_activations(child)\n",
    "\n",
    "    def replace_norms(self, mod, log=False):\n",
    "        if log:\n",
    "            print(f\"Replacing all norms with InstanceNorm...\")\n",
    "            \n",
    "        for name, c in mod.named_children():\n",
    "\n",
    "            # Get feature size\n",
    "            n_feats= None\n",
    "            if isinstance(c, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n",
    "                n_feats= c.num_features\n",
    "            elif isinstance(c, (nn.GroupNorm,)):\n",
    "                n_feats= c.num_channels\n",
    "            elif isinstance(c, (nn.LayerNorm,)):\n",
    "                n_feats= c.normalized_shape[0]\n",
    "\n",
    "            if n_feats is not None:\n",
    "                new = nn.InstanceNorm2d(\n",
    "                    n_feats,\n",
    "                    affine=True,\n",
    "                    )\n",
    "                setattr(mod, name, new)\n",
    "            else:\n",
    "                self.replace_norms(c)\n",
    "\n",
    "    def replace_forwards(self, mod, log=False):\n",
    "        if log:\n",
    "            print(f\"Replacing forward functions...\")\n",
    "            \n",
    "        for name, c in mod.named_children():\n",
    "            if isinstance(c, ConvNeXtBlock):\n",
    "                c.forward = MethodType(_convnext_block_forward, c)\n",
    "            else:\n",
    "                self.replace_forwards(c)\n",
    "\n",
    "        \n",
    "    def proc_flip(self, x_in):\n",
    "        x_in= torch.flip(x_in, dims=[-3, -1])\n",
    "        x= self.backbone(x_in)\n",
    "        x= x[::-1]\n",
    "\n",
    "        # Decoder\n",
    "        x= self.decoder(x)\n",
    "        x_seg= self.seg_head(x[-1])\n",
    "        x_seg= x_seg[..., 1:-1, 1:-1]\n",
    "        x_seg= torch.flip(x_seg, dims=[-1])\n",
    "        x_seg= x_seg * 1500 + 3000\n",
    "        return x_seg\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x= batch\n",
    "        print(\"[Net] - Stem x shape :\", x.shape)\n",
    "\n",
    "        # Encoder\n",
    "        x_in = x\n",
    "        x= self.backbone(x)\n",
    "        print(\"[Net] - Encoder x shape :\", [_.shape for _ in x])\n",
    "        x= x[::-1]\n",
    "\n",
    "        # Decoder\n",
    "        x= self.decoder(x)\n",
    "        print(\"[Net] - Decoder x shape :\", [_.shape for _ in x])\n",
    "        x_seg= self.seg_head(x[-1])\n",
    "        x_seg= x_seg[..., 1:-1, 1:-1]\n",
    "        print(\"[Net] - Decoder x_seg shape :\", x_seg.shape)\n",
    "        x_seg= x_seg * 1500 + 3000\n",
    "        print(\"[Net] - Decoder x_seg x1500 + 3000 shape :\", x_seg.shape)\n",
    "    \n",
    "        if self.training:\n",
    "            return x_seg\n",
    "        else:\n",
    "            p1 = self.proc_flip(x_in)\n",
    "            x_seg = torch.mean(torch.stack([x_seg, p1]), dim=0)\n",
    "            return x_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14240622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile _utils.py\n",
    "\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# hook activations and gradients\n",
    "activations = {}\n",
    "gradients = {}\n",
    "\n",
    "def save_activation(name):\n",
    "    def hook(module, input, output):\n",
    "        activations[name] = output.detach().cpu() \n",
    "    return hook\n",
    "\n",
    "def save_gradient(name):\n",
    "    def hook(module, grad_input, grad_output):\n",
    "        gradients[name] = grad_output[0].detach().cpu() \n",
    "    return hook\n",
    "\n",
    "def set_seed(seed=cfg.seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fed660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2610826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train data (mmap): 100%|██████████| 2/2 [00:00<00:00, 499.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./datasetfiles/FlatVel_A/data\\data1.npy: (500, 5, 1000, 70), (500, 1, 70, 70)\n",
      "Finished loading 1 file pairs for train mode.\n",
      "Loaded ./datasetfiles/FlatVel_A/data\\data2.npy: (500, 5, 1000, 70), (500, 1, 70, 70)\n",
      "Finished loading 2 file pairs for train mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading valid data (mmap): 100%|██████████| 2/2 [00:00<00:00, 499.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ./datasetfiles/FlatVel_A/data\\data1.npy: (500, 5, 1000, 70), (500, 1, 70, 70)\n",
      "Finished loading 1 file pairs for valid mode.\n",
      "Loaded ./datasetfiles/FlatVel_A/data\\data2.npy: (500, 5, 1000, 70), (500, 1, 70, 70)\n",
      "Finished loading 2 file pairs for valid mode.\n",
      "Sample shape: torch.Size([5, 1000, 70]) torch.Size([1, 70, 70])\n",
      "Replacing all activations with GELU...\n",
      "Replacing all norms with InstanceNorm...\n",
      "Replacing forward functions...\n",
      "Initializing EMA model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Net] - Stem x shape : torch.Size([8, 5, 1000, 70])\n",
      "[Net] - Encoder x shape : [torch.Size([8, 96, 72, 72]), torch.Size([8, 192, 36, 36]), torch.Size([8, 384, 18, 18]), torch.Size([8, 768, 9, 9])]\n",
      "[UnetDecoder2d] - forward length of feats and shapes 3 | s0 shape torch.Size([8, 384, 18, 18]) | s1 shape torch.Size([8, 192, 36, 36]) |s2 shape torch.Size([8, 96, 72, 72]) |\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 768, 9, 9]) | skip shape torch.Size([8, 384, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 128, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 128, 18, 18])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 128, 18, 18]) | skip shape torch.Size([8, 192, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 64, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 64, 36, 36])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 64, 36, 36]) | skip shape torch.Size([8, 96, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 32, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 32, 72, 72])\n",
      "[Net] - Decoder x shape : [torch.Size([8, 768, 9, 9]), torch.Size([8, 128, 18, 18]), torch.Size([8, 64, 36, 36]), torch.Size([8, 32, 72, 72])]\n",
      "[Segmentation Head 2d] forward--\n",
      "[Net] - Decoder x_seg shape : torch.Size([8, 1, 70, 70])\n",
      "[Net] - Decoder x_seg x1500 + 3000 shape : torch.Size([8, 1, 70, 70])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation:  50%|█████     | 1/2 [00:02<00:02,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnetDecoder2d] - forward length of feats and shapes 3 | s0 shape torch.Size([8, 384, 18, 18]) | s1 shape torch.Size([8, 192, 36, 36]) |s2 shape torch.Size([8, 96, 72, 72]) |\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 768, 9, 9]) | skip shape torch.Size([8, 384, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 128, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 128, 18, 18])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 128, 18, 18]) | skip shape torch.Size([8, 192, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 64, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 64, 36, 36])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 64, 36, 36]) | skip shape torch.Size([8, 96, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 32, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 32, 72, 72])\n",
      "[Segmentation Head 2d] forward--\n",
      "[Net] - Stem x shape : torch.Size([2, 5, 1000, 70])\n",
      "[Net] - Encoder x shape : [torch.Size([2, 96, 72, 72]), torch.Size([2, 192, 36, 36]), torch.Size([2, 384, 18, 18]), torch.Size([2, 768, 9, 9])]\n",
      "[UnetDecoder2d] - forward length of feats and shapes 3 | s0 shape torch.Size([2, 384, 18, 18]) | s1 shape torch.Size([2, 192, 36, 36]) |s2 shape torch.Size([2, 96, 72, 72]) |\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 768, 9, 9]) | skip shape torch.Size([2, 384, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 128, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 128, 18, 18])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 128, 18, 18]) | skip shape torch.Size([2, 192, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 64, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 64, 36, 36])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 64, 36, 36]) | skip shape torch.Size([2, 96, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 32, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 32, 72, 72])\n",
      "[Net] - Decoder x shape : [torch.Size([2, 768, 9, 9]), torch.Size([2, 128, 18, 18]), torch.Size([2, 64, 36, 36]), torch.Size([2, 32, 72, 72])]\n",
      "[Segmentation Head 2d] forward--\n",
      "[Net] - Decoder x_seg shape : torch.Size([2, 1, 70, 70])\n",
      "[Net] - Decoder x_seg x1500 + 3000 shape : torch.Size([2, 1, 70, 70])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation: 100%|██████████| 2/2 [00:03<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnetDecoder2d] - forward length of feats and shapes 3 | s0 shape torch.Size([2, 384, 18, 18]) | s1 shape torch.Size([2, 192, 36, 36]) |s2 shape torch.Size([2, 96, 72, 72]) |\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 768, 9, 9]) | skip shape torch.Size([2, 384, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 128, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 128, 18, 18])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 128, 18, 18]) | skip shape torch.Size([2, 192, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 64, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 64, 36, 36])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 64, 36, 36]) | skip shape torch.Size([2, 96, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 32, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 32, 72, 72])\n",
      "[Segmentation Head 2d] forward--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Net] - Stem x shape : torch.Size([8, 5, 1000, 70])\n",
      "[Net] - Encoder x shape : [torch.Size([8, 96, 72, 72]), torch.Size([8, 192, 36, 36]), torch.Size([8, 384, 18, 18]), torch.Size([8, 768, 9, 9])]\n",
      "[UnetDecoder2d] - forward length of feats and shapes 3 | s0 shape torch.Size([8, 384, 18, 18]) | s1 shape torch.Size([8, 192, 36, 36]) |s2 shape torch.Size([8, 96, 72, 72]) |\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 768, 9, 9]) | skip shape torch.Size([8, 384, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 128, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 128, 18, 18])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 128, 18, 18]) | skip shape torch.Size([8, 192, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 64, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 64, 36, 36])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 64, 36, 36]) | skip shape torch.Size([8, 96, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 32, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 32, 72, 72])\n",
      "[Net] - Decoder x shape : [torch.Size([8, 768, 9, 9]), torch.Size([8, 128, 18, 18]), torch.Size([8, 64, 36, 36]), torch.Size([8, 32, 72, 72])]\n",
      "[Segmentation Head 2d] forward--\n",
      "[Net] - Decoder x_seg shape : torch.Size([8, 1, 70, 70])\n",
      "[Net] - Decoder x_seg x1500 + 3000 shape : torch.Size([8, 1, 70, 70])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:  50%|█████     | 1/2 [00:05<00:05,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Net] - Stem x shape : torch.Size([2, 5, 1000, 70])\n",
      "[Net] - Encoder x shape : [torch.Size([2, 96, 72, 72]), torch.Size([2, 192, 36, 36]), torch.Size([2, 384, 18, 18]), torch.Size([2, 768, 9, 9])]\n",
      "[UnetDecoder2d] - forward length of feats and shapes 3 | s0 shape torch.Size([2, 384, 18, 18]) | s1 shape torch.Size([2, 192, 36, 36]) |s2 shape torch.Size([2, 96, 72, 72]) |\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 768, 9, 9]) | skip shape torch.Size([2, 384, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 128, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 128, 18, 18])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 128, 18, 18]) | skip shape torch.Size([2, 192, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 64, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 64, 36, 36])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 64, 36, 36]) | skip shape torch.Size([2, 96, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 32, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 32, 72, 72])\n",
      "[Net] - Decoder x shape : [torch.Size([2, 768, 9, 9]), torch.Size([2, 128, 18, 18]), torch.Size([2, 64, 36, 36]), torch.Size([2, 32, 72, 72])]\n",
      "[Segmentation Head 2d] forward--\n",
      "[Net] - Decoder x_seg shape : torch.Size([2, 1, 70, 70])\n",
      "[Net] - Decoder x_seg x1500 + 3000 shape : torch.Size([2, 1, 70, 70])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 2/2 [00:07<00:00,  3.51s/it]\n",
      "Epoch 1 Validation:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Net] - Stem x shape : torch.Size([8, 5, 1000, 70])\n",
      "[Net] - Encoder x shape : [torch.Size([8, 96, 72, 72]), torch.Size([8, 192, 36, 36]), torch.Size([8, 384, 18, 18]), torch.Size([8, 768, 9, 9])]\n",
      "[UnetDecoder2d] - forward length of feats and shapes 3 | s0 shape torch.Size([8, 384, 18, 18]) | s1 shape torch.Size([8, 192, 36, 36]) |s2 shape torch.Size([8, 96, 72, 72]) |\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 768, 9, 9]) | skip shape torch.Size([8, 384, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 128, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 128, 18, 18])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 128, 18, 18]) | skip shape torch.Size([8, 192, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 64, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 64, 36, 36])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 64, 36, 36]) | skip shape torch.Size([8, 96, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 32, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 32, 72, 72])\n",
      "[Net] - Decoder x shape : [torch.Size([8, 768, 9, 9]), torch.Size([8, 128, 18, 18]), torch.Size([8, 64, 36, 36]), torch.Size([8, 32, 72, 72])]\n",
      "[Segmentation Head 2d] forward--\n",
      "[Net] - Decoder x_seg shape : torch.Size([8, 1, 70, 70])\n",
      "[Net] - Decoder x_seg x1500 + 3000 shape : torch.Size([8, 1, 70, 70])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation:  50%|█████     | 1/2 [00:02<00:02,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnetDecoder2d] - forward length of feats and shapes 3 | s0 shape torch.Size([8, 384, 18, 18]) | s1 shape torch.Size([8, 192, 36, 36]) |s2 shape torch.Size([8, 96, 72, 72]) |\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 768, 9, 9]) | skip shape torch.Size([8, 384, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 128, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 128, 18, 18])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 128, 18, 18]) | skip shape torch.Size([8, 192, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 64, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 64, 36, 36])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([8, 64, 36, 36]) | skip shape torch.Size([8, 96, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([8, 32, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([8, 32, 72, 72])\n",
      "[Segmentation Head 2d] forward--\n",
      "[Net] - Stem x shape : torch.Size([2, 5, 1000, 70])\n",
      "[Net] - Encoder x shape : [torch.Size([2, 96, 72, 72]), torch.Size([2, 192, 36, 36]), torch.Size([2, 384, 18, 18]), torch.Size([2, 768, 9, 9])]\n",
      "[UnetDecoder2d] - forward length of feats and shapes 3 | s0 shape torch.Size([2, 384, 18, 18]) | s1 shape torch.Size([2, 192, 36, 36]) |s2 shape torch.Size([2, 96, 72, 72]) |\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 768, 9, 9]) | skip shape torch.Size([2, 384, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 128, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 128, 18, 18])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 128, 18, 18]) | skip shape torch.Size([2, 192, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 64, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 64, 36, 36])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 64, 36, 36]) | skip shape torch.Size([2, 96, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 32, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 32, 72, 72])\n",
      "[Net] - Decoder x shape : [torch.Size([2, 768, 9, 9]), torch.Size([2, 128, 18, 18]), torch.Size([2, 64, 36, 36]), torch.Size([2, 32, 72, 72])]\n",
      "[Segmentation Head 2d] forward--\n",
      "[Net] - Decoder x_seg shape : torch.Size([2, 1, 70, 70])\n",
      "[Net] - Decoder x_seg x1500 + 3000 shape : torch.Size([2, 1, 70, 70])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnetDecoder2d] - forward length of feats and shapes 3 | s0 shape torch.Size([2, 384, 18, 18]) | s1 shape torch.Size([2, 192, 36, 36]) |s2 shape torch.Size([2, 96, 72, 72]) |\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 768, 9, 9]) | skip shape torch.Size([2, 384, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 1152, 18, 18])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 128, 18, 18])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 128, 18, 18])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 128, 18, 18]) | skip shape torch.Size([2, 192, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 320, 36, 36])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 64, 36, 36])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 64, 36, 36])\n",
      "[DecoderBlock2d] -- forward x shape torch.Size([2, 64, 36, 36]) | skip shape torch.Size([2, 96, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 160, 72, 72])\n",
      "[ConvBnAct2d] --forward | x shape torch.Size([2, 32, 72, 72])\n",
      "[Attention2d] --forward | x shape torch.Size([2, 32, 72, 72])\n",
      "[Segmentation Head 2d] forward--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training code for CPU\n",
    "\n",
    "data_paths = sorted(glob.glob(data_paths_str))\n",
    "label_paths = sorted(glob.glob(label_paths_str))\n",
    "file_pairs = list(zip(data_paths, label_paths))\n",
    "\n",
    "\n",
    "train_ds = CustomDataset(cfg=cfg, file_pairs=file_pairs, mode=\"train\")\n",
    "# Replaced DistributedSampler with standard DataLoader and shuffle\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_ds,\n",
    "    batch_size= cfg.batch_size,\n",
    "    num_workers= 0, #4,\n",
    "    shuffle=True, # Add shuffle for training\n",
    ")\n",
    "\n",
    "valid_ds = CustomDataset(cfg=cfg, file_pairs=file_pairs, mode=\"valid\")\n",
    "# Replaced DistributedSampler with standard DataLoader\n",
    "valid_dl = torch.utils.data.DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size= cfg.batch_size_val,\n",
    "    num_workers= 0, #4,\n",
    "    shuffle=False, # No shuffle for validation\n",
    ")\n",
    "\n",
    "\n",
    "x, y = train_ds[0]\n",
    "print(\"Sample shape:\", x.shape, y.shape)\n",
    "# ========== Model / Optim ==========\n",
    "model = Net(backbone=cfg.backbone).to(cfg.device)\n",
    "# Removed .to(cfg.local_rank) - models are on CPU by default\n",
    "\n",
    "#-----\n",
    "\"\"\" \n",
    "HOOK activation and gradient \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if cfg.ema:\n",
    "    if cfg.local_rank == 0:\n",
    "        print(\"Initializing EMA model..\")\n",
    "    # Set device explicitly to 'cpu' for EMA\n",
    "    ema_model = ModelEMA(\n",
    "        model,\n",
    "        decay=cfg.ema_decay,\n",
    "        device=cfg.device,\n",
    "    )\n",
    "else:\n",
    "    ema_model = None\n",
    "\n",
    "# Removed DistributedDataParallel wrap - use the base model directly\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# Removed GradScaler - not needed for CPU or non-mixed precision\n",
    "\n",
    "\n",
    "parameters_conv = []\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Conv2d) and m.weight.requires_grad:\n",
    "        p = m.weight\n",
    "        if p.ndim == 4:\n",
    "            # print(f\"p as param with shape {p.shape} from module {m}\")\n",
    "            parameters_conv.append(p)\n",
    "ud = []\n",
    "eps = 1e-8\n",
    "\n",
    "\n",
    "best_loss= 1_000_000\n",
    "val_loss= 1_000_000 # Initialize val_loss for logging on epoch 0\n",
    "\n",
    "for epoch in range(0, cfg.epochs+1):\n",
    "    if epoch != 0: # Skip epoch 0 training loop\n",
    "        tstart= time.time()\n",
    "        # Removed sampler.set_epoch - not needed for standard DataLoader\n",
    "\n",
    "        # Train loop\n",
    "        model.train()\n",
    "        total_loss = []\n",
    "        # tqdm for train loop only on rank 0\n",
    "        train_loop = tqdm(train_dl, disable=cfg.local_rank != 0, desc=f\"Epoch {epoch} Training\")\n",
    "        for i, (x, y) in enumerate(train_loop):\n",
    "            # Removed .to(cfg.local_rank) - data is already on CPU\n",
    "            # Removed autocast context\n",
    "            x = x.to(cfg.device)\n",
    "            y = y.to(cfg.device)\n",
    "\n",
    "            logits = model(x)\n",
    "\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            # Replaced scaler steps with standard backprop and optimizer step\n",
    "            loss.backward()\n",
    "            # Removed scaler.unscale_\n",
    "\n",
    "            ### Trach UD\n",
    "\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            with torch.no_grad():\n",
    "                ud.append([\n",
    "                        (lr * (p.grad.std() + eps) / (p.data.std() + eps)).log10().item()\n",
    "                        if p.grad is not None else float('-inf')\n",
    "                        for p in parameters_conv\n",
    "                ])\n",
    "\n",
    "            optimizer.step()\n",
    "            # Removed scaler.update\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            if ema_model is not None:\n",
    "                ema_model.update(model)\n",
    "\n",
    "\n",
    "    # ========== Valid ==========\n",
    "    model.eval()\n",
    "    val_logits = []\n",
    "    val_targets = []\n",
    "    # tqdm for validation loop on rank 0\n",
    "    valid_loop = tqdm(valid_dl, disable=cfg.local_rank != 0, desc=f\"Epoch {epoch} Validation\")\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loop:\n",
    "            # Removed .to(cfg.local_rank) - data is already on CPU\n",
    "            x = x.to(cfg.device)\n",
    "            y = y.to(cfg.device)\n",
    "            \n",
    "            # Removed autocast context\n",
    "            if ema_model is not None:\n",
    "                # Access the underlying model from EMA wrapper\n",
    "                # out = ema_model.ema_model(x) # Or ema_model.model(x) depending on EMA implementation\n",
    "                out = ema_model.module(x) # Or ema_model.model(x) depending on EMA implementation\n",
    "            else:\n",
    "                out = model(x)\n",
    "\n",
    "            val_logits.append(out.cpu())\n",
    "            val_targets.append(y.cpu())\n",
    "\n",
    "        val_logits= torch.cat(val_logits, dim=0)\n",
    "        val_targets= torch.cat(val_targets, dim=0)\n",
    "\n",
    "        # Calculate loss directly (no all_reduce needed)\n",
    "        val_loss = criterion(val_logits, val_targets).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4f7dea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (backbone): FeatureListNet(\n",
       "    (stem_0): Sequential(\n",
       "      (0): ReflectionPad2d((1, 1, 80, 80))\n",
       "      (1): Conv2d(5, 96, kernel_size=(4, 4), stride=(4, 1), padding=(0, 2))\n",
       "      (2): Conv2d(96, 96, kernel_size=(4, 4), stride=(4, 1), padding=(0, 1))\n",
       "    )\n",
       "    (stem_1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (stages_0): ConvNeXtStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stages_1): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): InstanceNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): InstanceNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): InstanceNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stages_2): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): InstanceNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (6): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (7): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (8): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (9): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (10): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (11): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (12): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (13): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (14): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (15): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (16): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (17): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (18): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (19): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (20): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (21): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (22): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (23): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (24): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (25): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (26): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (stages_3): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): InstanceNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): InstanceNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): InstanceNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): InstanceNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder2d(\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock2d(\n",
       "        (upsample): UpSample(\n",
       "          (deconv): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (attention1): Attention2d(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv1): ConvBnAct2d(\n",
       "          (conv): Conv2d(1152, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBnAct2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention2d(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock2d(\n",
       "        (upsample): UpSample(\n",
       "          (deconv): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (attention1): Attention2d(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv1): ConvBnAct2d(\n",
       "          (conv): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBnAct2d(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention2d(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock2d(\n",
       "        (upsample): UpSample(\n",
       "          (deconv): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (attention1): Attention2d(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv1): ConvBnAct2d(\n",
       "          (conv): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (conv2): ConvBnAct2d(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (norm): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention2d(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (seg_head): SegmentationHead2d(\n",
       "    (conv): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (upsample): UpSample(\n",
       "      (upsample_non_trainable): Upsample(scale_factor=(1.0, 1.0), mode='bilinear')\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a395e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_conv[0].size()\n",
    "\n",
    "all_weights = []\n",
    "\n",
    "# Collect flattened weights from all Conv2D layers\n",
    "for p in parameters_conv:\n",
    "    weights = p.detach().cpu().flatten()\n",
    "    all_weights.append(weights)\n",
    "\n",
    "# Concatenate into one long tensor\n",
    "all_weights_flat = torch.cat(all_weights)\n",
    "\n",
    "# Plot in a single chart\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(all_weights_flat.numpy())\n",
    "plt.title(\"All Conv2D Layer Weights (Flattened)\")\n",
    "plt.xlabel(\"Weight Index\")\n",
    "plt.ylabel(\"Weight Value\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c895bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "offset = 0\n",
    "for i, p in enumerate(parameters_conv):\n",
    "    weights = p.detach().cpu().flatten()\n",
    "    x = torch.arange(offset, offset + len(weights))\n",
    "    plt.plot(x.numpy(), weights.numpy(), label=f\"Layer {i}\")\n",
    "    offset += len(weights)\n",
    "\n",
    "plt.title(\"Conv2D Layer Weights by Layer\")\n",
    "plt.xlabel(\"Weight Index\")\n",
    "plt.ylabel(\"Weight Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.backbone.stages_0.parameters(), 'lr': 1e-5},      # Pretrained backbone\n",
    "    {'params': model.backbone.stages_1.parameters(), 'lr': 1e-3} # New classification head\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8179061",
   "metadata": {},
   "source": [
    "|Typical Learning Rate Assignments in Deep Networks|||\n",
    "|--|--|--|\n",
    "|- Model Part|\tLearning Rate Size|\tReason / Explanation|\n",
    "|- Early Encoder| Layers\tSmall LR|\tThese layers extract generic, low-level features (edges, textures). Usually pretrained and well-optimized, so small LR avoids destroying learned features (stability).|\n",
    "|- Later Encoder Layers\t|Medium LR|\tMore task-specific features start to emerge. Sometimes a slightly higher LR than early layers helps adapt them better to the new task.|\n",
    "|- Decoder / Head Layers|\tLarge LR\t|Often randomly initialized or task-specific layers (classification head, segmentation decoder). Need to learn quickly from scratch, so use higher LR.|\n",
    "|- Newly Added Layers\t|Large LR\t|Same as decoder — new layers typically require higher LR to learn fast.|\n",
    "|- Normalization Layers (BatchNorm, LayerNorm)|Often small or zero LR|\tUsually, you freeze or apply small LR to avoid disturbing learned statistics or parameters.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#################################\n",
    "#################################\n",
    "### COMPARING ADAM and ADAMW loss\n",
    "#################################\n",
    "#################################\n",
    "#################################\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sample neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Instantiate model and loss\n",
    "model = SimpleNet()\n",
    "loss_fn = nn.MSELoss()\n",
    "data = torch.randn(64, 10)\n",
    "target = torch.randn(64, 1)\n",
    "\n",
    "# Adam Optimizer\n",
    "adam_optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "adam_loss = []\n",
    "for _ in range(100):\n",
    "    adam_optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    adam_optimizer.step()\n",
    "    adam_loss.append(loss.item())\n",
    "\n",
    "# AdamW Optimizer\n",
    "model = SimpleNet()  # Reset model weights\n",
    "adamw_optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "adamw_loss = []\n",
    "for _ in range(100):\n",
    "    adamw_optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    adamw_optimizer.step()\n",
    "    adamw_loss.append(loss.item())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(adam_loss, label=\"Adam Loss\")\n",
    "plt.plot(adamw_loss, label=\"AdamW Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Adam vs AdamW Loss Convergence\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201b861",
   "metadata": {},
   "source": [
    "When to Choose Adam: You can use Adam for quick prototyping or simpler tasks where regularization is not crucial. It may converge faster initially but can suffer from poor generalization due to interference from weight decay.\n",
    "When to Choose AdamW: In case you have larger models or when training on complex, high-dimensional data, it’s better to choose AdamW, because the decoupled weight decay helps achieve better generalization and stable convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea02112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5898b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
