{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cd47cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1acd1096410>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from subfunc import data_loader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(1337)\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "016dc2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "x, y, x_val, y_val, x_test, y_test = data_loader()\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "x_val = torch.tensor(x_val)\n",
    "y_val = torch.tensor(y_val)\n",
    "x_test = torch.tensor(x_test)\n",
    "y_test = torch.tensor(y_test)\n",
    "print(\"y shape\", y.shape)\n",
    "steps = 100 #400\n",
    "sample, yoko, tate = x.shape\n",
    "h1 = 80\n",
    "h2 = 30\n",
    "in_shape = yoko*tate\n",
    "l1 = nn.Linear(in_shape,h1)\n",
    "# nl1 = nn.LeakyReLU(0.2)\n",
    "bn1 = nn.BatchNorm1d(h1)\n",
    "nl1 = nn.ReLU()\n",
    "# nl1 = nn.Softmax(dim=1)\n",
    "l2 = nn.Linear(h1, h2)\n",
    "bn2 = nn.BatchNorm1d(h2)\n",
    "nl2 = nn.ReLU()\n",
    "# nl2 = nn.Softmax(dim=1)\n",
    "outl = nn.Linear(h2, 10)\n",
    "input_data = torch.reshape(x, (sample, yoko*tate))\n",
    "sample_val, yoko, tate = x_val.shape\n",
    "x_val = torch.reshape(x_val, (sample_val, yoko*tate))\n",
    "\n",
    "# optimizer and loss\n",
    "params = list(l1.parameters()) + list(l2.parameters()) + list(outl.parameters())\n",
    "lr=0.0002\n",
    "optimizer = torch.optim.Adam(params, lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83a349c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 iter -> loss output :2.3855 | lr: 0.000200 | Accuracy: 11.40%| Val Loss: 2.3908 | Val Acccuracy: 11.98%\n",
      "10 iter -> loss output :1.8977 | lr: 0.000200 | Accuracy: 37.89%| Val Loss: 1.9324 | Val Acccuracy: 35.78%\n",
      "20 iter -> loss output :1.6871 | lr: 0.000200 | Accuracy: 55.86%| Val Loss: 1.7201 | Val Acccuracy: 54.84%\n",
      "30 iter -> loss output :1.6260 | lr: 0.000080 | Accuracy: 60.46%| Val Loss: 1.6598 | Val Acccuracy: 59.84%\n",
      "40 iter -> loss output :1.5936 | lr: 0.000016 | Accuracy: 62.97%| Val Loss: 1.6283 | Val Acccuracy: 62.06%\n",
      "50 iter -> loss output :1.5808 | lr: 0.000016 | Accuracy: 63.98%| Val Loss: 1.6159 | Val Acccuracy: 62.90%\n",
      "60 iter -> loss output :1.5664 | lr: 0.000016 | Accuracy: 65.05%| Val Loss: 1.6019 | Val Acccuracy: 63.86%\n",
      "70 iter -> loss output :1.5511 | lr: 0.000016 | Accuracy: 66.14%| Val Loss: 1.5870 | Val Acccuracy: 64.94%\n",
      "80 iter -> loss output :1.5354 | lr: 0.000016 | Accuracy: 67.27%| Val Loss: 1.5716 | Val Acccuracy: 65.76%\n",
      "90 iter -> loss output :1.5195 | lr: 0.000016 | Accuracy: 68.31%| Val Loss: 1.5561 | Val Acccuracy: 67.02%\n"
     ]
    }
   ],
   "source": [
    "for step in range(steps):\n",
    "\n",
    "    output1 = l1(input_data)\n",
    "    output1b = bn1(output1)\n",
    "    output2 = nl1(output1b)  #non lenear 1\n",
    "    output3 = l2(output2)\n",
    "    output3b = bn2(output3)\n",
    "    output4 = nl2(output3b) #non lenear 2\n",
    "    output = outl(output4)\n",
    "\n",
    "    if step == 21:\n",
    "         lr = lr * 0.4\n",
    "         for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    if step == 35:\n",
    "         lr = lr * 0.2\n",
    "         for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    if step == 200:\n",
    "         lr = lr * 0.3\n",
    "         for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    if step == 300:\n",
    "         lr = lr * 0.3\n",
    "         for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "    loss_output = loss_fn(output, y)\n",
    "    if step % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # accuracy \n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            correct = (preds == y).sum().item()\n",
    "            accuracy = correct / y.size(0)\n",
    "\n",
    "            # validation\n",
    "            output_val = l1(x_val)\n",
    "            output_val = bn1(output_val)\n",
    "            output_val = nl1(output_val)\n",
    "            output_val = l2(output_val)\n",
    "            output_val = bn2(output_val)\n",
    "            output_val = nl2(output_val)\n",
    "            output_val = outl(output_val)\n",
    "\n",
    "            # validation loss\n",
    "            val_loss = loss_fn(output_val, y_val)\n",
    "\n",
    "            # calculate validation accuracy\n",
    "            preds_val = torch.argmax(output_val, dim=1)\n",
    "            correct_val = (preds_val == y_val).sum().item()\n",
    "            val_accuracy = correct_val / y_val.size(0)\n",
    "\n",
    "        print(f\"{step:2d} iter -> loss output :{loss_output.item():.4f} | lr: {lr:7f} | Accuracy: {accuracy*100:.2f}%| Val Loss: {val_loss.item():.4f} | Val Acccuracy: {val_accuracy*100:.2f}%\")\n",
    "    \n",
    "    loss_output.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f35378a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming output3 and output3b are tensors, you need to flatten them and convert to a list\n",
    "# output1_flat = output1.view(-1).tolist()\n",
    "# output1b_flat = output1b.view(-1).tolist()\n",
    "output3_flat = output3.view(-1).tolist()\n",
    "output3b_flat = output3b.view(-1).tolist()\n",
    "\n",
    "# Create the histogram\n",
    "# plt.hist(output1_flat, bins=20, alpha=0.5, label='output1')\n",
    "# plt.hist(output1b_flat, bins=20, alpha=0.5, label='output1b')\n",
    "plt.hist(output3_flat, bins=20, alpha=0.5, label='output3')\n",
    "plt.hist(output3b_flat, bins=20, alpha=0.5, label='output3b')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Comparison of output3 and output3b')\n",
    "\n",
    "# Add a legend to differentiate the two histograms\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec3b53",
   "metadata": {},
   "source": [
    "### CNN MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56d2e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from subfunc import data_loader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "class KMIST_CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, H, W, in_channels, layer_1_channels, layer_2_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, layer_1_channels, kernel_size=3, padding=1)  #output 32 x 28 x 28\n",
    "        self.bn1 = nn.BatchNorm2d(layer_1_channels)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  #output 32 x 14 x 14\n",
    "\n",
    "        self.conv2 = nn.Conv2d(layer_1_channels, layer_2_channels, kernel_size=3, padding=1)  #output 64 x 14 x 14\n",
    "        self.bn2 = nn.BatchNorm2d(layer_2_channels)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # output 64 x 7 x 7\n",
    "\n",
    "        self.fc1 = nn.Linear(layer_2_channels * H//4 * W//4, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 classes for KMINST\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)  #flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66f6f4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([5000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "### PARAMETERS\n",
    "in_channels = 1\n",
    "layer_1_channels = 32\n",
    "layer_2_channels = 64\n",
    "max_step = 400\n",
    "b_size = 80\n",
    "\n",
    "x, y, x_val, y_val, x_test, y_test = data_loader()\n",
    "# batch, height, width  B = 600000, height = 28, width = 28\n",
    "B, H, W = x.shape\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "x_val = torch.tensor(x_val)\n",
    "y_val = torch.tensor(y_val)\n",
    "x_test = torch.tensor(x_test)\n",
    "y_test = torch.tensor(y_test)\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(in_channels)\n",
    "x_val = x_val.unsqueeze(in_channels)\n",
    "print(x.shape)\n",
    "print(x_val.shape)\n",
    "# batch, channel, height, width  B = 600000, c = 1, height = 28, width = 28\n",
    "# channel is the depth of the data, RGB or just gray\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93e152f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training \n",
    "\n",
    "model = KMIST_CNN(H, W, in_channels, layer_1_channels, layer_2_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[40, 60, 200, 300], gamma=0.5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "824311cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 x-> loss:2.4084| lr: 0.0010000 | Accuracy :3.75% | Val-loss: 2.7739| Val Accuracy: 9.15%\n",
      "20 x-> loss:0.8100| lr: 0.0010000 | Accuracy :75.00% | Val-loss: 0.6961| Val Accuracy: 81.10%\n",
      "40 x-> loss:0.6115| lr: 0.0005000 | Accuracy :77.50% | Val-loss: 0.4134| Val Accuracy: 88.55%\n",
      "60 x-> loss:0.4904| lr: 0.0002500 | Accuracy :83.75% | Val-loss: 0.3135| Val Accuracy: 92.10%\n",
      "80 x-> loss:0.5062| lr: 0.0002500 | Accuracy :87.50% | Val-loss: 0.2984| Val Accuracy: 91.75%\n",
      "100 x-> loss:0.2786| lr: 0.0002500 | Accuracy :93.75% | Val-loss: 0.2728| Val Accuracy: 92.45%\n",
      "120 x-> loss:0.3946| lr: 0.0002500 | Accuracy :87.50% | Val-loss: 0.2569| Val Accuracy: 93.10%\n",
      "140 x-> loss:0.3385| lr: 0.0002500 | Accuracy :95.00% | Val-loss: 0.2515| Val Accuracy: 92.80%\n",
      "160 x-> loss:0.2282| lr: 0.0002500 | Accuracy :95.00% | Val-loss: 0.2241| Val Accuracy: 94.00%\n",
      "180 x-> loss:0.2142| lr: 0.0002500 | Accuracy :96.25% | Val-loss: 0.2056| Val Accuracy: 94.20%\n",
      "200 x-> loss:0.3974| lr: 0.0001250 | Accuracy :87.50% | Val-loss: 0.1871| Val Accuracy: 94.45%\n",
      "220 x-> loss:0.2392| lr: 0.0001250 | Accuracy :96.25% | Val-loss: 0.1797| Val Accuracy: 94.90%\n",
      "240 x-> loss:0.2770| lr: 0.0001250 | Accuracy :90.00% | Val-loss: 0.1704| Val Accuracy: 95.00%\n",
      "260 x-> loss:0.2544| lr: 0.0001250 | Accuracy :95.00% | Val-loss: 0.1673| Val Accuracy: 94.85%\n",
      "280 x-> loss:0.1413| lr: 0.0001250 | Accuracy :96.25% | Val-loss: 0.1645| Val Accuracy: 95.15%\n",
      "300 x-> loss:0.2286| lr: 0.0000625 | Accuracy :96.25% | Val-loss: 0.1587| Val Accuracy: 95.35%\n",
      "320 x-> loss:0.2182| lr: 0.0000625 | Accuracy :93.75% | Val-loss: 0.1510| Val Accuracy: 95.50%\n",
      "340 x-> loss:0.2807| lr: 0.0000625 | Accuracy :87.50% | Val-loss: 0.1451| Val Accuracy: 95.70%\n",
      "360 x-> loss:0.1438| lr: 0.0000625 | Accuracy :98.75% | Val-loss: 0.1416| Val Accuracy: 95.55%\n",
      "380 x-> loss:0.1548| lr: 0.0000625 | Accuracy :95.00% | Val-loss: 0.1401| Val Accuracy: 95.55%\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_step):\n",
    "\n",
    "    # print(\"step\", step)\n",
    "\n",
    "    if step == 0:\n",
    "        batch_start = 0\n",
    "        batch_end = b_size\n",
    "    else:\n",
    "        batch_start = step * b_size\n",
    "        batch_end = (step + 1) * b_size\n",
    "    \n",
    "    batch_end = min(batch_end, len(x))\n",
    "    # print(\"batch start\", batch_start)\n",
    "    # print(\"batch_end\", batch_end)\n",
    "    \n",
    "    x_batch = x[batch_start:batch_end]\n",
    "    y_batch = y[batch_start:batch_end]\n",
    "\n",
    "    x_val_batch = x_val[:2000]\n",
    "    y_val_batch = y_val[:2000]\n",
    "\n",
    "    # Training step\n",
    "    model.train()\n",
    "    logits = model(x_batch)\n",
    "    loss = loss_fn(logits, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # accuracy \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct = (preds == y_batch).sum().item()\n",
    "        accuracy = correct / y_batch.size(0)\n",
    "\n",
    "        # validation\n",
    "        val_logits = model(x_val_batch)\n",
    "        loss_val = loss_fn(val_logits, y_val_batch)\n",
    "\n",
    "        # val accuracy\n",
    "        preds_val = torch.argmax(val_logits, dim=1)\n",
    "        correct_val = (preds_val == y_val_batch).sum().item()\n",
    "        accuracy_Val = correct_val / y_val_batch.size(0)\n",
    "\n",
    "\n",
    "    if step % 20 == 0:\n",
    "        print(f\"{step:2d} x-> loss:{loss.item():.4f}| lr: {current_lr:.7f} | Accuracy :{accuracy*100:.2f}% | Val-loss: {loss_val.item():.4f}| Val Accuracy: {accuracy_Val*100:.2f}%\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192eec85",
   "metadata": {},
   "source": [
    "### GENERAL NEURALNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "266963f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26bf33023b0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from subfunc import data_loader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "random.seed(1337)\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0c3666d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "x, y, x_val, y_val, x_test, y_test = data_loader()\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "x_val = torch.tensor(x_val)\n",
    "y_val = torch.tensor(y_val)\n",
    "x_test = torch.tensor(x_test)\n",
    "y_test = torch.tensor(y_test)\n",
    "print(\"y shape\", y.shape)\n",
    "steps = 300\n",
    "\n",
    "B, W, H = x.shape\n",
    "input_data = torch.reshape(x, (B, W*H))\n",
    "b_val, W, H = x_val.shape\n",
    "input_x_val = torch.reshape(x_val, (b_val, W*H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "db8b975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MNIST_MDL(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(784, 30)\n",
    "        self.bn1 = nn.BatchNorm1d(30)\n",
    "        self.nl1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(30, 20)\n",
    "        self.bn2 = nn.BatchNorm1d(20)\n",
    "        self.nl2 = nn.ReLU()\n",
    "        self.outln = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.nl1(self.bn1(self.linear1(x)))\n",
    "        x = self.nl2(self.bn2(self.linear2(x)))\n",
    "        x = self.outln(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3d04fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_MDL()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "73e8aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 iter -> loss output :2.3846 | lr: 0.010000 | Accuracy: 9.34%| Val Loss: 1.9091 | Val Acccuracy: 35.50%\n",
      " 1 iter -> loss output :1.9214 | lr: 0.010000 | Accuracy: 41.39%| Val Loss: 1.6055 | Val Acccuracy: 50.50%\n",
      " 2 iter -> loss output :1.7594 | lr: 0.010000 | Accuracy: 58.04%| Val Loss: 1.5683 | Val Acccuracy: 56.68%\n",
      " 3 iter -> loss output :1.6553 | lr: 0.010000 | Accuracy: 66.01%| Val Loss: 1.5324 | Val Acccuracy: 61.90%\n",
      " 4 iter -> loss output :1.5679 | lr: 0.010000 | Accuracy: 70.91%| Val Loss: 1.4842 | Val Acccuracy: 65.82%\n",
      " 5 iter -> loss output :1.4874 | lr: 0.010000 | Accuracy: 73.82%| Val Loss: 1.4275 | Val Acccuracy: 68.52%\n",
      " 6 iter -> loss output :1.4104 | lr: 0.010000 | Accuracy: 75.69%| Val Loss: 1.3662 | Val Acccuracy: 70.04%\n",
      " 7 iter -> loss output :1.3361 | lr: 0.010000 | Accuracy: 76.98%| Val Loss: 1.3035 | Val Acccuracy: 71.34%\n",
      " 8 iter -> loss output :1.2645 | lr: 0.010000 | Accuracy: 78.00%| Val Loss: 1.2410 | Val Acccuracy: 72.56%\n",
      " 9 iter -> loss output :1.1959 | lr: 0.010000 | Accuracy: 78.80%| Val Loss: 1.1795 | Val Acccuracy: 73.18%\n",
      "10 iter -> loss output :1.1303 | lr: 0.010000 | Accuracy: 79.58%| Val Loss: 1.1194 | Val Acccuracy: 73.76%\n",
      "11 iter -> loss output :1.0677 | lr: 0.010000 | Accuracy: 80.36%| Val Loss: 1.0613 | Val Acccuracy: 74.50%\n",
      "12 iter -> loss output :1.0079 | lr: 0.010000 | Accuracy: 81.26%| Val Loss: 1.0057 | Val Acccuracy: 75.30%\n",
      "13 iter -> loss output :0.9510 | lr: 0.010000 | Accuracy: 82.12%| Val Loss: 0.9528 | Val Acccuracy: 76.18%\n",
      "14 iter -> loss output :0.8967 | lr: 0.010000 | Accuracy: 83.04%| Val Loss: 0.9030 | Val Acccuracy: 77.36%\n",
      "15 iter -> loss output :0.8448 | lr: 0.010000 | Accuracy: 84.11%| Val Loss: 0.8568 | Val Acccuracy: 78.54%\n",
      "16 iter -> loss output :0.7954 | lr: 0.010000 | Accuracy: 85.21%| Val Loss: 0.8141 | Val Acccuracy: 79.34%\n",
      "17 iter -> loss output :0.7485 | lr: 0.010000 | Accuracy: 86.24%| Val Loss: 0.7747 | Val Acccuracy: 80.58%\n",
      "18 iter -> loss output :0.7043 | lr: 0.010000 | Accuracy: 87.10%| Val Loss: 0.7388 | Val Acccuracy: 81.64%\n",
      "19 iter -> loss output :0.6629 | lr: 0.010000 | Accuracy: 87.73%| Val Loss: 0.7062 | Val Acccuracy: 82.48%\n",
      "20 iter -> loss output :0.6244 | lr: 0.010000 | Accuracy: 88.27%| Val Loss: 0.6766 | Val Acccuracy: 83.28%\n",
      "21 iter -> loss output :0.5888 | lr: 0.010000 | Accuracy: 88.72%| Val Loss: 0.6495 | Val Acccuracy: 84.14%\n",
      "22 iter -> loss output :0.5559 | lr: 0.010000 | Accuracy: 89.15%| Val Loss: 0.6239 | Val Acccuracy: 84.76%\n",
      "23 iter -> loss output :0.5258 | lr: 0.010000 | Accuracy: 89.46%| Val Loss: 0.5998 | Val Acccuracy: 85.32%\n",
      "24 iter -> loss output :0.4984 | lr: 0.010000 | Accuracy: 89.71%| Val Loss: 0.5768 | Val Acccuracy: 86.04%\n",
      "25 iter -> loss output :0.4733 | lr: 0.010000 | Accuracy: 89.95%| Val Loss: 0.5550 | Val Acccuracy: 86.38%\n",
      "26 iter -> loss output :0.4505 | lr: 0.010000 | Accuracy: 90.17%| Val Loss: 0.5344 | Val Acccuracy: 86.84%\n",
      "27 iter -> loss output :0.4295 | lr: 0.010000 | Accuracy: 90.44%| Val Loss: 0.5150 | Val Acccuracy: 87.10%\n",
      "28 iter -> loss output :0.4103 | lr: 0.010000 | Accuracy: 90.64%| Val Loss: 0.4970 | Val Acccuracy: 87.62%\n",
      "29 iter -> loss output :0.3926 | lr: 0.010000 | Accuracy: 90.85%| Val Loss: 0.4804 | Val Acccuracy: 87.96%\n",
      "30 iter -> loss output :0.3762 | lr: 0.010000 | Accuracy: 91.06%| Val Loss: 0.4651 | Val Acccuracy: 88.22%\n",
      "31 iter -> loss output :0.3610 | lr: 0.010000 | Accuracy: 91.29%| Val Loss: 0.4512 | Val Acccuracy: 88.48%\n",
      "32 iter -> loss output :0.3470 | lr: 0.010000 | Accuracy: 91.53%| Val Loss: 0.4383 | Val Acccuracy: 88.78%\n",
      "33 iter -> loss output :0.3341 | lr: 0.010000 | Accuracy: 91.71%| Val Loss: 0.4263 | Val Acccuracy: 88.92%\n",
      "34 iter -> loss output :0.3223 | lr: 0.010000 | Accuracy: 91.93%| Val Loss: 0.4151 | Val Acccuracy: 89.16%\n",
      "35 iter -> loss output :0.3114 | lr: 0.010000 | Accuracy: 92.08%| Val Loss: 0.4046 | Val Acccuracy: 89.34%\n",
      "36 iter -> loss output :0.3015 | lr: 0.010000 | Accuracy: 92.22%| Val Loss: 0.3948 | Val Acccuracy: 89.44%\n",
      "37 iter -> loss output :0.2923 | lr: 0.010000 | Accuracy: 92.40%| Val Loss: 0.3855 | Val Acccuracy: 89.74%\n",
      "38 iter -> loss output :0.2837 | lr: 0.010000 | Accuracy: 92.56%| Val Loss: 0.3769 | Val Acccuracy: 89.98%\n",
      "39 iter -> loss output :0.2756 | lr: 0.010000 | Accuracy: 92.76%| Val Loss: 0.3688 | Val Acccuracy: 89.96%\n",
      "40 iter -> loss output :0.2679 | lr: 0.010000 | Accuracy: 92.92%| Val Loss: 0.3614 | Val Acccuracy: 90.14%\n",
      "41 iter -> loss output :0.2607 | lr: 0.010000 | Accuracy: 93.10%| Val Loss: 0.3547 | Val Acccuracy: 90.22%\n",
      "42 iter -> loss output :0.2539 | lr: 0.010000 | Accuracy: 93.27%| Val Loss: 0.3484 | Val Acccuracy: 90.40%\n",
      "43 iter -> loss output :0.2475 | lr: 0.010000 | Accuracy: 93.39%| Val Loss: 0.3424 | Val Acccuracy: 90.58%\n",
      "44 iter -> loss output :0.2415 | lr: 0.010000 | Accuracy: 93.50%| Val Loss: 0.3365 | Val Acccuracy: 90.80%\n",
      "45 iter -> loss output :0.2357 | lr: 0.010000 | Accuracy: 93.64%| Val Loss: 0.3307 | Val Acccuracy: 91.00%\n",
      "46 iter -> loss output :0.2303 | lr: 0.010000 | Accuracy: 93.75%| Val Loss: 0.3248 | Val Acccuracy: 91.14%\n",
      "47 iter -> loss output :0.2251 | lr: 0.010000 | Accuracy: 93.88%| Val Loss: 0.3188 | Val Acccuracy: 91.26%\n",
      "48 iter -> loss output :0.2202 | lr: 0.010000 | Accuracy: 94.01%| Val Loss: 0.3127 | Val Acccuracy: 91.30%\n",
      "49 iter -> loss output :0.2155 | lr: 0.010000 | Accuracy: 94.13%| Val Loss: 0.3066 | Val Acccuracy: 91.48%\n",
      "50 iter -> loss output :0.2109 | lr: 0.010000 | Accuracy: 94.23%| Val Loss: 0.3005 | Val Acccuracy: 91.54%\n",
      "51 iter -> loss output :0.2066 | lr: 0.010000 | Accuracy: 94.35%| Val Loss: 0.2945 | Val Acccuracy: 91.76%\n",
      "52 iter -> loss output :0.2024 | lr: 0.010000 | Accuracy: 94.49%| Val Loss: 0.2887 | Val Acccuracy: 91.90%\n",
      "53 iter -> loss output :0.1984 | lr: 0.010000 | Accuracy: 94.60%| Val Loss: 0.2832 | Val Acccuracy: 92.06%\n",
      "54 iter -> loss output :0.1946 | lr: 0.010000 | Accuracy: 94.72%| Val Loss: 0.2778 | Val Acccuracy: 92.14%\n",
      "55 iter -> loss output :0.1909 | lr: 0.010000 | Accuracy: 94.81%| Val Loss: 0.2727 | Val Acccuracy: 92.42%\n",
      "56 iter -> loss output :0.1874 | lr: 0.010000 | Accuracy: 94.88%| Val Loss: 0.2677 | Val Acccuracy: 92.46%\n",
      "57 iter -> loss output :0.1840 | lr: 0.010000 | Accuracy: 94.96%| Val Loss: 0.2629 | Val Acccuracy: 92.52%\n",
      "58 iter -> loss output :0.1807 | lr: 0.010000 | Accuracy: 95.03%| Val Loss: 0.2581 | Val Acccuracy: 92.72%\n",
      "59 iter -> loss output :0.1775 | lr: 0.010000 | Accuracy: 95.11%| Val Loss: 0.2534 | Val Acccuracy: 92.70%\n",
      "60 iter -> loss output :0.1745 | lr: 0.010000 | Accuracy: 95.20%| Val Loss: 0.2487 | Val Acccuracy: 92.86%\n",
      "61 iter -> loss output :0.1716 | lr: 0.010000 | Accuracy: 95.27%| Val Loss: 0.2441 | Val Acccuracy: 93.02%\n",
      "62 iter -> loss output :0.1687 | lr: 0.010000 | Accuracy: 95.34%| Val Loss: 0.2395 | Val Acccuracy: 93.16%\n",
      "63 iter -> loss output :0.1660 | lr: 0.010000 | Accuracy: 95.40%| Val Loss: 0.2352 | Val Acccuracy: 93.32%\n",
      "64 iter -> loss output :0.1633 | lr: 0.010000 | Accuracy: 95.49%| Val Loss: 0.2311 | Val Acccuracy: 93.40%\n",
      "65 iter -> loss output :0.1607 | lr: 0.010000 | Accuracy: 95.56%| Val Loss: 0.2274 | Val Acccuracy: 93.54%\n",
      "66 iter -> loss output :0.1582 | lr: 0.010000 | Accuracy: 95.61%| Val Loss: 0.2241 | Val Acccuracy: 93.58%\n",
      "67 iter -> loss output :0.1558 | lr: 0.010000 | Accuracy: 95.70%| Val Loss: 0.2210 | Val Acccuracy: 93.62%\n",
      "68 iter -> loss output :0.1535 | lr: 0.010000 | Accuracy: 95.79%| Val Loss: 0.2182 | Val Acccuracy: 93.70%\n",
      "69 iter -> loss output :0.1512 | lr: 0.010000 | Accuracy: 95.86%| Val Loss: 0.2155 | Val Acccuracy: 93.74%\n",
      "70 iter -> loss output :0.1490 | lr: 0.010000 | Accuracy: 95.93%| Val Loss: 0.2130 | Val Acccuracy: 93.84%\n",
      "71 iter -> loss output :0.1468 | lr: 0.010000 | Accuracy: 96.00%| Val Loss: 0.2105 | Val Acccuracy: 93.88%\n",
      "72 iter -> loss output :0.1446 | lr: 0.010000 | Accuracy: 96.06%| Val Loss: 0.2080 | Val Acccuracy: 93.98%\n",
      "73 iter -> loss output :0.1426 | lr: 0.010000 | Accuracy: 96.09%| Val Loss: 0.2055 | Val Acccuracy: 94.12%\n",
      "74 iter -> loss output :0.1405 | lr: 0.010000 | Accuracy: 96.14%| Val Loss: 0.2029 | Val Acccuracy: 94.12%\n",
      "75 iter -> loss output :0.1386 | lr: 0.010000 | Accuracy: 96.22%| Val Loss: 0.2003 | Val Acccuracy: 94.24%\n",
      "76 iter -> loss output :0.1366 | lr: 0.010000 | Accuracy: 96.27%| Val Loss: 0.1978 | Val Acccuracy: 94.36%\n",
      "77 iter -> loss output :0.1348 | lr: 0.010000 | Accuracy: 96.31%| Val Loss: 0.1954 | Val Acccuracy: 94.46%\n",
      "78 iter -> loss output :0.1330 | lr: 0.010000 | Accuracy: 96.36%| Val Loss: 0.1932 | Val Acccuracy: 94.44%\n",
      "79 iter -> loss output :0.1312 | lr: 0.010000 | Accuracy: 96.39%| Val Loss: 0.1911 | Val Acccuracy: 94.44%\n",
      "80 iter -> loss output :0.1295 | lr: 0.010000 | Accuracy: 96.44%| Val Loss: 0.1892 | Val Acccuracy: 94.50%\n",
      "81 iter -> loss output :0.1278 | lr: 0.010000 | Accuracy: 96.51%| Val Loss: 0.1875 | Val Acccuracy: 94.56%\n",
      "82 iter -> loss output :0.1261 | lr: 0.010000 | Accuracy: 96.54%| Val Loss: 0.1859 | Val Acccuracy: 94.64%\n",
      "83 iter -> loss output :0.1245 | lr: 0.010000 | Accuracy: 96.57%| Val Loss: 0.1844 | Val Acccuracy: 94.66%\n",
      "84 iter -> loss output :0.1229 | lr: 0.010000 | Accuracy: 96.60%| Val Loss: 0.1830 | Val Acccuracy: 94.72%\n",
      "85 iter -> loss output :0.1214 | lr: 0.010000 | Accuracy: 96.65%| Val Loss: 0.1817 | Val Acccuracy: 94.70%\n",
      "86 iter -> loss output :0.1199 | lr: 0.010000 | Accuracy: 96.69%| Val Loss: 0.1804 | Val Acccuracy: 94.70%\n",
      "87 iter -> loss output :0.1184 | lr: 0.010000 | Accuracy: 96.75%| Val Loss: 0.1792 | Val Acccuracy: 94.72%\n",
      "88 iter -> loss output :0.1169 | lr: 0.010000 | Accuracy: 96.76%| Val Loss: 0.1780 | Val Acccuracy: 94.74%\n",
      "89 iter -> loss output :0.1155 | lr: 0.010000 | Accuracy: 96.80%| Val Loss: 0.1769 | Val Acccuracy: 94.74%\n",
      "90 iter -> loss output :0.1141 | lr: 0.010000 | Accuracy: 96.84%| Val Loss: 0.1758 | Val Acccuracy: 94.72%\n",
      "91 iter -> loss output :0.1128 | lr: 0.010000 | Accuracy: 96.89%| Val Loss: 0.1747 | Val Acccuracy: 94.78%\n",
      "92 iter -> loss output :0.1114 | lr: 0.010000 | Accuracy: 96.93%| Val Loss: 0.1738 | Val Acccuracy: 94.80%\n",
      "93 iter -> loss output :0.1101 | lr: 0.010000 | Accuracy: 96.96%| Val Loss: 0.1728 | Val Acccuracy: 94.82%\n",
      "94 iter -> loss output :0.1089 | lr: 0.010000 | Accuracy: 96.99%| Val Loss: 0.1720 | Val Acccuracy: 94.80%\n",
      "95 iter -> loss output :0.1076 | lr: 0.010000 | Accuracy: 97.02%| Val Loss: 0.1712 | Val Acccuracy: 94.88%\n",
      "96 iter -> loss output :0.1064 | lr: 0.010000 | Accuracy: 97.09%| Val Loss: 0.1704 | Val Acccuracy: 94.94%\n",
      "97 iter -> loss output :0.1052 | lr: 0.010000 | Accuracy: 97.12%| Val Loss: 0.1695 | Val Acccuracy: 94.96%\n",
      "98 iter -> loss output :0.1040 | lr: 0.010000 | Accuracy: 97.15%| Val Loss: 0.1687 | Val Acccuracy: 94.98%\n",
      "99 iter -> loss output :0.1028 | lr: 0.010000 | Accuracy: 97.17%| Val Loss: 0.1679 | Val Acccuracy: 95.02%\n",
      "100 iter -> loss output :0.1017 | lr: 0.010000 | Accuracy: 97.21%| Val Loss: 0.1671 | Val Acccuracy: 95.06%\n",
      "101 iter -> loss output :0.1006 | lr: 0.010000 | Accuracy: 97.23%| Val Loss: 0.1664 | Val Acccuracy: 95.06%\n",
      "102 iter -> loss output :0.0994 | lr: 0.010000 | Accuracy: 97.27%| Val Loss: 0.1657 | Val Acccuracy: 95.10%\n",
      "103 iter -> loss output :0.0983 | lr: 0.010000 | Accuracy: 97.30%| Val Loss: 0.1651 | Val Acccuracy: 95.18%\n",
      "104 iter -> loss output :0.0973 | lr: 0.010000 | Accuracy: 97.33%| Val Loss: 0.1644 | Val Acccuracy: 95.18%\n",
      "105 iter -> loss output :0.0962 | lr: 0.010000 | Accuracy: 97.37%| Val Loss: 0.1638 | Val Acccuracy: 95.22%\n",
      "106 iter -> loss output :0.0951 | lr: 0.010000 | Accuracy: 97.41%| Val Loss: 0.1631 | Val Acccuracy: 95.20%\n",
      "107 iter -> loss output :0.0941 | lr: 0.010000 | Accuracy: 97.45%| Val Loss: 0.1624 | Val Acccuracy: 95.24%\n",
      "108 iter -> loss output :0.0931 | lr: 0.010000 | Accuracy: 97.49%| Val Loss: 0.1617 | Val Acccuracy: 95.30%\n",
      "109 iter -> loss output :0.0921 | lr: 0.010000 | Accuracy: 97.52%| Val Loss: 0.1611 | Val Acccuracy: 95.28%\n",
      "110 iter -> loss output :0.0911 | lr: 0.010000 | Accuracy: 97.56%| Val Loss: 0.1605 | Val Acccuracy: 95.28%\n",
      "111 iter -> loss output :0.0901 | lr: 0.010000 | Accuracy: 97.59%| Val Loss: 0.1600 | Val Acccuracy: 95.28%\n",
      "112 iter -> loss output :0.0891 | lr: 0.010000 | Accuracy: 97.61%| Val Loss: 0.1594 | Val Acccuracy: 95.36%\n",
      "113 iter -> loss output :0.0882 | lr: 0.010000 | Accuracy: 97.65%| Val Loss: 0.1589 | Val Acccuracy: 95.36%\n",
      "114 iter -> loss output :0.0872 | lr: 0.010000 | Accuracy: 97.67%| Val Loss: 0.1585 | Val Acccuracy: 95.38%\n",
      "115 iter -> loss output :0.0863 | lr: 0.010000 | Accuracy: 97.69%| Val Loss: 0.1580 | Val Acccuracy: 95.40%\n",
      "116 iter -> loss output :0.0854 | lr: 0.010000 | Accuracy: 97.72%| Val Loss: 0.1576 | Val Acccuracy: 95.44%\n",
      "117 iter -> loss output :0.0845 | lr: 0.010000 | Accuracy: 97.75%| Val Loss: 0.1572 | Val Acccuracy: 95.48%\n",
      "118 iter -> loss output :0.0836 | lr: 0.010000 | Accuracy: 97.77%| Val Loss: 0.1568 | Val Acccuracy: 95.42%\n",
      "119 iter -> loss output :0.0827 | lr: 0.010000 | Accuracy: 97.79%| Val Loss: 0.1565 | Val Acccuracy: 95.40%\n",
      "120 iter -> loss output :0.0818 | lr: 0.010000 | Accuracy: 97.82%| Val Loss: 0.1561 | Val Acccuracy: 95.44%\n",
      "121 iter -> loss output :0.0810 | lr: 0.010000 | Accuracy: 97.84%| Val Loss: 0.1558 | Val Acccuracy: 95.40%\n",
      "122 iter -> loss output :0.0801 | lr: 0.010000 | Accuracy: 97.86%| Val Loss: 0.1555 | Val Acccuracy: 95.38%\n",
      "123 iter -> loss output :0.0793 | lr: 0.010000 | Accuracy: 97.90%| Val Loss: 0.1552 | Val Acccuracy: 95.34%\n",
      "124 iter -> loss output :0.0785 | lr: 0.010000 | Accuracy: 97.92%| Val Loss: 0.1549 | Val Acccuracy: 95.32%\n",
      "125 iter -> loss output :0.0777 | lr: 0.010000 | Accuracy: 97.95%| Val Loss: 0.1545 | Val Acccuracy: 95.34%\n",
      "126 iter -> loss output :0.0769 | lr: 0.010000 | Accuracy: 97.98%| Val Loss: 0.1542 | Val Acccuracy: 95.34%\n",
      "127 iter -> loss output :0.0761 | lr: 0.010000 | Accuracy: 98.01%| Val Loss: 0.1538 | Val Acccuracy: 95.38%\n",
      "128 iter -> loss output :0.0753 | lr: 0.010000 | Accuracy: 98.02%| Val Loss: 0.1534 | Val Acccuracy: 95.38%\n",
      "129 iter -> loss output :0.0746 | lr: 0.010000 | Accuracy: 98.04%| Val Loss: 0.1531 | Val Acccuracy: 95.34%\n",
      "130 iter -> loss output :0.0738 | lr: 0.010000 | Accuracy: 98.05%| Val Loss: 0.1527 | Val Acccuracy: 95.32%\n",
      "131 iter -> loss output :0.0731 | lr: 0.010000 | Accuracy: 98.08%| Val Loss: 0.1524 | Val Acccuracy: 95.36%\n",
      "132 iter -> loss output :0.0723 | lr: 0.010000 | Accuracy: 98.10%| Val Loss: 0.1522 | Val Acccuracy: 95.36%\n",
      "133 iter -> loss output :0.0716 | lr: 0.010000 | Accuracy: 98.13%| Val Loss: 0.1520 | Val Acccuracy: 95.42%\n",
      "134 iter -> loss output :0.0709 | lr: 0.010000 | Accuracy: 98.15%| Val Loss: 0.1519 | Val Acccuracy: 95.44%\n",
      "135 iter -> loss output :0.0702 | lr: 0.010000 | Accuracy: 98.18%| Val Loss: 0.1518 | Val Acccuracy: 95.46%\n",
      "136 iter -> loss output :0.0695 | lr: 0.010000 | Accuracy: 98.20%| Val Loss: 0.1517 | Val Acccuracy: 95.52%\n",
      "137 iter -> loss output :0.0688 | lr: 0.010000 | Accuracy: 98.22%| Val Loss: 0.1515 | Val Acccuracy: 95.54%\n",
      "138 iter -> loss output :0.0681 | lr: 0.010000 | Accuracy: 98.25%| Val Loss: 0.1514 | Val Acccuracy: 95.54%\n",
      "139 iter -> loss output :0.0674 | lr: 0.010000 | Accuracy: 98.27%| Val Loss: 0.1512 | Val Acccuracy: 95.60%\n",
      "140 iter -> loss output :0.0667 | lr: 0.010000 | Accuracy: 98.30%| Val Loss: 0.1510 | Val Acccuracy: 95.60%\n",
      "141 iter -> loss output :0.0661 | lr: 0.010000 | Accuracy: 98.33%| Val Loss: 0.1508 | Val Acccuracy: 95.60%\n",
      "142 iter -> loss output :0.0654 | lr: 0.010000 | Accuracy: 98.34%| Val Loss: 0.1507 | Val Acccuracy: 95.64%\n",
      "143 iter -> loss output :0.0648 | lr: 0.010000 | Accuracy: 98.37%| Val Loss: 0.1506 | Val Acccuracy: 95.62%\n",
      "144 iter -> loss output :0.0641 | lr: 0.010000 | Accuracy: 98.38%| Val Loss: 0.1505 | Val Acccuracy: 95.64%\n",
      "145 iter -> loss output :0.0635 | lr: 0.010000 | Accuracy: 98.41%| Val Loss: 0.1504 | Val Acccuracy: 95.70%\n",
      "146 iter -> loss output :0.0629 | lr: 0.010000 | Accuracy: 98.44%| Val Loss: 0.1503 | Val Acccuracy: 95.70%\n",
      "147 iter -> loss output :0.0623 | lr: 0.010000 | Accuracy: 98.46%| Val Loss: 0.1502 | Val Acccuracy: 95.70%\n",
      "148 iter -> loss output :0.0617 | lr: 0.010000 | Accuracy: 98.48%| Val Loss: 0.1501 | Val Acccuracy: 95.70%\n",
      "149 iter -> loss output :0.0611 | lr: 0.010000 | Accuracy: 98.50%| Val Loss: 0.1501 | Val Acccuracy: 95.70%\n",
      "150 iter -> loss output :0.0605 | lr: 0.010000 | Accuracy: 98.52%| Val Loss: 0.1501 | Val Acccuracy: 95.74%\n",
      "151 iter -> loss output :0.0599 | lr: 0.010000 | Accuracy: 98.54%| Val Loss: 0.1501 | Val Acccuracy: 95.76%\n",
      "152 iter -> loss output :0.0594 | lr: 0.010000 | Accuracy: 98.56%| Val Loss: 0.1501 | Val Acccuracy: 95.78%\n",
      "153 iter -> loss output :0.0588 | lr: 0.010000 | Accuracy: 98.57%| Val Loss: 0.1500 | Val Acccuracy: 95.82%\n",
      "154 iter -> loss output :0.0582 | lr: 0.010000 | Accuracy: 98.60%| Val Loss: 0.1500 | Val Acccuracy: 95.82%\n",
      "155 iter -> loss output :0.0577 | lr: 0.010000 | Accuracy: 98.62%| Val Loss: 0.1500 | Val Acccuracy: 95.80%\n",
      "156 iter -> loss output :0.0571 | lr: 0.010000 | Accuracy: 98.64%| Val Loss: 0.1501 | Val Acccuracy: 95.78%\n",
      "157 iter -> loss output :0.0565 | lr: 0.010000 | Accuracy: 98.66%| Val Loss: 0.1502 | Val Acccuracy: 95.80%\n",
      "158 iter -> loss output :0.0560 | lr: 0.010000 | Accuracy: 98.69%| Val Loss: 0.1503 | Val Acccuracy: 95.82%\n",
      "159 iter -> loss output :0.0555 | lr: 0.010000 | Accuracy: 98.70%| Val Loss: 0.1504 | Val Acccuracy: 95.90%\n",
      "160 iter -> loss output :0.0549 | lr: 0.010000 | Accuracy: 98.72%| Val Loss: 0.1505 | Val Acccuracy: 95.88%\n",
      "161 iter -> loss output :0.0544 | lr: 0.010000 | Accuracy: 98.74%| Val Loss: 0.1506 | Val Acccuracy: 95.84%\n",
      "162 iter -> loss output :0.0539 | lr: 0.010000 | Accuracy: 98.75%| Val Loss: 0.1506 | Val Acccuracy: 95.84%\n",
      "163 iter -> loss output :0.0534 | lr: 0.010000 | Accuracy: 98.77%| Val Loss: 0.1508 | Val Acccuracy: 95.84%\n",
      "164 iter -> loss output :0.0529 | lr: 0.010000 | Accuracy: 98.78%| Val Loss: 0.1509 | Val Acccuracy: 95.84%\n",
      "165 iter -> loss output :0.0523 | lr: 0.010000 | Accuracy: 98.81%| Val Loss: 0.1510 | Val Acccuracy: 95.82%\n",
      "166 iter -> loss output :0.0518 | lr: 0.010000 | Accuracy: 98.82%| Val Loss: 0.1510 | Val Acccuracy: 95.82%\n",
      "167 iter -> loss output :0.0513 | lr: 0.010000 | Accuracy: 98.83%| Val Loss: 0.1509 | Val Acccuracy: 95.80%\n",
      "168 iter -> loss output :0.0509 | lr: 0.010000 | Accuracy: 98.84%| Val Loss: 0.1508 | Val Acccuracy: 95.78%\n",
      "169 iter -> loss output :0.0504 | lr: 0.010000 | Accuracy: 98.86%| Val Loss: 0.1508 | Val Acccuracy: 95.76%\n",
      "170 iter -> loss output :0.0499 | lr: 0.010000 | Accuracy: 98.88%| Val Loss: 0.1509 | Val Acccuracy: 95.80%\n",
      "171 iter -> loss output :0.0494 | lr: 0.010000 | Accuracy: 98.90%| Val Loss: 0.1510 | Val Acccuracy: 95.80%\n",
      "172 iter -> loss output :0.0489 | lr: 0.010000 | Accuracy: 98.91%| Val Loss: 0.1512 | Val Acccuracy: 95.80%\n",
      "173 iter -> loss output :0.0484 | lr: 0.010000 | Accuracy: 98.92%| Val Loss: 0.1512 | Val Acccuracy: 95.80%\n",
      "174 iter -> loss output :0.0480 | lr: 0.010000 | Accuracy: 98.94%| Val Loss: 0.1513 | Val Acccuracy: 95.82%\n",
      "175 iter -> loss output :0.0475 | lr: 0.010000 | Accuracy: 98.96%| Val Loss: 0.1513 | Val Acccuracy: 95.82%\n",
      "176 iter -> loss output :0.0471 | lr: 0.010000 | Accuracy: 98.97%| Val Loss: 0.1514 | Val Acccuracy: 95.80%\n",
      "177 iter -> loss output :0.0466 | lr: 0.010000 | Accuracy: 98.98%| Val Loss: 0.1514 | Val Acccuracy: 95.76%\n",
      "178 iter -> loss output :0.0462 | lr: 0.010000 | Accuracy: 98.98%| Val Loss: 0.1515 | Val Acccuracy: 95.74%\n",
      "179 iter -> loss output :0.0457 | lr: 0.010000 | Accuracy: 99.00%| Val Loss: 0.1516 | Val Acccuracy: 95.74%\n",
      "180 iter -> loss output :0.0453 | lr: 0.010000 | Accuracy: 99.03%| Val Loss: 0.1517 | Val Acccuracy: 95.74%\n",
      "181 iter -> loss output :0.0448 | lr: 0.010000 | Accuracy: 99.05%| Val Loss: 0.1518 | Val Acccuracy: 95.74%\n",
      "182 iter -> loss output :0.0444 | lr: 0.010000 | Accuracy: 99.08%| Val Loss: 0.1519 | Val Acccuracy: 95.72%\n",
      "183 iter -> loss output :0.0440 | lr: 0.010000 | Accuracy: 99.08%| Val Loss: 0.1520 | Val Acccuracy: 95.70%\n",
      "184 iter -> loss output :0.0435 | lr: 0.010000 | Accuracy: 99.09%| Val Loss: 0.1522 | Val Acccuracy: 95.66%\n",
      "185 iter -> loss output :0.0431 | lr: 0.010000 | Accuracy: 99.11%| Val Loss: 0.1524 | Val Acccuracy: 95.64%\n",
      "186 iter -> loss output :0.0427 | lr: 0.010000 | Accuracy: 99.12%| Val Loss: 0.1527 | Val Acccuracy: 95.66%\n",
      "187 iter -> loss output :0.0423 | lr: 0.010000 | Accuracy: 99.12%| Val Loss: 0.1530 | Val Acccuracy: 95.72%\n",
      "188 iter -> loss output :0.0419 | lr: 0.010000 | Accuracy: 99.13%| Val Loss: 0.1533 | Val Acccuracy: 95.74%\n",
      "189 iter -> loss output :0.0415 | lr: 0.010000 | Accuracy: 99.15%| Val Loss: 0.1536 | Val Acccuracy: 95.72%\n",
      "190 iter -> loss output :0.0410 | lr: 0.010000 | Accuracy: 99.16%| Val Loss: 0.1538 | Val Acccuracy: 95.66%\n",
      "191 iter -> loss output :0.0406 | lr: 0.010000 | Accuracy: 99.18%| Val Loss: 0.1541 | Val Acccuracy: 95.66%\n",
      "192 iter -> loss output :0.0402 | lr: 0.010000 | Accuracy: 99.19%| Val Loss: 0.1545 | Val Acccuracy: 95.58%\n",
      "193 iter -> loss output :0.0398 | lr: 0.010000 | Accuracy: 99.20%| Val Loss: 0.1549 | Val Acccuracy: 95.56%\n",
      "194 iter -> loss output :0.0395 | lr: 0.010000 | Accuracy: 99.21%| Val Loss: 0.1552 | Val Acccuracy: 95.56%\n",
      "195 iter -> loss output :0.0391 | lr: 0.010000 | Accuracy: 99.23%| Val Loss: 0.1555 | Val Acccuracy: 95.56%\n",
      "196 iter -> loss output :0.0387 | lr: 0.010000 | Accuracy: 99.25%| Val Loss: 0.1559 | Val Acccuracy: 95.56%\n",
      "197 iter -> loss output :0.0383 | lr: 0.010000 | Accuracy: 99.26%| Val Loss: 0.1564 | Val Acccuracy: 95.58%\n",
      "198 iter -> loss output :0.0379 | lr: 0.010000 | Accuracy: 99.28%| Val Loss: 0.1568 | Val Acccuracy: 95.54%\n",
      "199 iter -> loss output :0.0376 | lr: 0.010000 | Accuracy: 99.28%| Val Loss: 0.1572 | Val Acccuracy: 95.56%\n",
      "200 iter -> loss output :0.0372 | lr: 0.010000 | Accuracy: 99.29%| Val Loss: 0.1576 | Val Acccuracy: 95.56%\n",
      "201 iter -> loss output :0.0368 | lr: 0.010000 | Accuracy: 99.30%| Val Loss: 0.1581 | Val Acccuracy: 95.52%\n",
      "202 iter -> loss output :0.0365 | lr: 0.010000 | Accuracy: 99.31%| Val Loss: 0.1585 | Val Acccuracy: 95.52%\n",
      "203 iter -> loss output :0.0361 | lr: 0.010000 | Accuracy: 99.33%| Val Loss: 0.1588 | Val Acccuracy: 95.46%\n",
      "204 iter -> loss output :0.0357 | lr: 0.010000 | Accuracy: 99.34%| Val Loss: 0.1590 | Val Acccuracy: 95.46%\n",
      "205 iter -> loss output :0.0354 | lr: 0.010000 | Accuracy: 99.34%| Val Loss: 0.1595 | Val Acccuracy: 95.46%\n",
      "206 iter -> loss output :0.0350 | lr: 0.010000 | Accuracy: 99.35%| Val Loss: 0.1601 | Val Acccuracy: 95.48%\n",
      "207 iter -> loss output :0.0347 | lr: 0.010000 | Accuracy: 99.36%| Val Loss: 0.1604 | Val Acccuracy: 95.50%\n",
      "208 iter -> loss output :0.0343 | lr: 0.010000 | Accuracy: 99.37%| Val Loss: 0.1607 | Val Acccuracy: 95.50%\n",
      "209 iter -> loss output :0.0340 | lr: 0.010000 | Accuracy: 99.38%| Val Loss: 0.1611 | Val Acccuracy: 95.46%\n",
      "210 iter -> loss output :0.0337 | lr: 0.010000 | Accuracy: 99.39%| Val Loss: 0.1615 | Val Acccuracy: 95.42%\n",
      "211 iter -> loss output :0.0333 | lr: 0.010000 | Accuracy: 99.41%| Val Loss: 0.1618 | Val Acccuracy: 95.44%\n",
      "212 iter -> loss output :0.0330 | lr: 0.010000 | Accuracy: 99.42%| Val Loss: 0.1623 | Val Acccuracy: 95.48%\n",
      "213 iter -> loss output :0.0327 | lr: 0.010000 | Accuracy: 99.43%| Val Loss: 0.1627 | Val Acccuracy: 95.46%\n",
      "214 iter -> loss output :0.0324 | lr: 0.010000 | Accuracy: 99.43%| Val Loss: 0.1631 | Val Acccuracy: 95.44%\n",
      "215 iter -> loss output :0.0320 | lr: 0.010000 | Accuracy: 99.44%| Val Loss: 0.1634 | Val Acccuracy: 95.42%\n",
      "216 iter -> loss output :0.0317 | lr: 0.010000 | Accuracy: 99.45%| Val Loss: 0.1636 | Val Acccuracy: 95.42%\n",
      "217 iter -> loss output :0.0314 | lr: 0.010000 | Accuracy: 99.46%| Val Loss: 0.1640 | Val Acccuracy: 95.40%\n",
      "218 iter -> loss output :0.0311 | lr: 0.010000 | Accuracy: 99.47%| Val Loss: 0.1644 | Val Acccuracy: 95.36%\n",
      "219 iter -> loss output :0.0308 | lr: 0.010000 | Accuracy: 99.49%| Val Loss: 0.1647 | Val Acccuracy: 95.40%\n",
      "220 iter -> loss output :0.0305 | lr: 0.010000 | Accuracy: 99.49%| Val Loss: 0.1650 | Val Acccuracy: 95.36%\n",
      "221 iter -> loss output :0.0302 | lr: 0.010000 | Accuracy: 99.50%| Val Loss: 0.1654 | Val Acccuracy: 95.36%\n",
      "222 iter -> loss output :0.0299 | lr: 0.010000 | Accuracy: 99.50%| Val Loss: 0.1656 | Val Acccuracy: 95.34%\n",
      "223 iter -> loss output :0.0296 | lr: 0.010000 | Accuracy: 99.50%| Val Loss: 0.1658 | Val Acccuracy: 95.32%\n",
      "224 iter -> loss output :0.0293 | lr: 0.010000 | Accuracy: 99.51%| Val Loss: 0.1662 | Val Acccuracy: 95.32%\n",
      "225 iter -> loss output :0.0290 | lr: 0.010000 | Accuracy: 99.52%| Val Loss: 0.1667 | Val Acccuracy: 95.34%\n",
      "226 iter -> loss output :0.0288 | lr: 0.010000 | Accuracy: 99.53%| Val Loss: 0.1670 | Val Acccuracy: 95.32%\n",
      "227 iter -> loss output :0.0285 | lr: 0.010000 | Accuracy: 99.54%| Val Loss: 0.1673 | Val Acccuracy: 95.34%\n",
      "228 iter -> loss output :0.0282 | lr: 0.010000 | Accuracy: 99.55%| Val Loss: 0.1677 | Val Acccuracy: 95.30%\n",
      "229 iter -> loss output :0.0279 | lr: 0.010000 | Accuracy: 99.56%| Val Loss: 0.1681 | Val Acccuracy: 95.28%\n",
      "230 iter -> loss output :0.0277 | lr: 0.010000 | Accuracy: 99.56%| Val Loss: 0.1684 | Val Acccuracy: 95.30%\n",
      "231 iter -> loss output :0.0274 | lr: 0.010000 | Accuracy: 99.58%| Val Loss: 0.1687 | Val Acccuracy: 95.30%\n",
      "232 iter -> loss output :0.0271 | lr: 0.010000 | Accuracy: 99.59%| Val Loss: 0.1692 | Val Acccuracy: 95.22%\n",
      "233 iter -> loss output :0.0269 | lr: 0.010000 | Accuracy: 99.60%| Val Loss: 0.1696 | Val Acccuracy: 95.22%\n",
      "234 iter -> loss output :0.0266 | lr: 0.010000 | Accuracy: 99.61%| Val Loss: 0.1697 | Val Acccuracy: 95.18%\n",
      "235 iter -> loss output :0.0264 | lr: 0.010000 | Accuracy: 99.61%| Val Loss: 0.1700 | Val Acccuracy: 95.18%\n",
      "236 iter -> loss output :0.0261 | lr: 0.010000 | Accuracy: 99.62%| Val Loss: 0.1704 | Val Acccuracy: 95.18%\n",
      "237 iter -> loss output :0.0259 | lr: 0.010000 | Accuracy: 99.63%| Val Loss: 0.1710 | Val Acccuracy: 95.20%\n",
      "238 iter -> loss output :0.0256 | lr: 0.010000 | Accuracy: 99.64%| Val Loss: 0.1714 | Val Acccuracy: 95.18%\n",
      "239 iter -> loss output :0.0254 | lr: 0.010000 | Accuracy: 99.64%| Val Loss: 0.1716 | Val Acccuracy: 95.14%\n",
      "240 iter -> loss output :0.0251 | lr: 0.010000 | Accuracy: 99.66%| Val Loss: 0.1718 | Val Acccuracy: 95.14%\n",
      "241 iter -> loss output :0.0249 | lr: 0.010000 | Accuracy: 99.66%| Val Loss: 0.1722 | Val Acccuracy: 95.14%\n",
      "242 iter -> loss output :0.0246 | lr: 0.010000 | Accuracy: 99.67%| Val Loss: 0.1726 | Val Acccuracy: 95.12%\n",
      "243 iter -> loss output :0.0244 | lr: 0.010000 | Accuracy: 99.67%| Val Loss: 0.1731 | Val Acccuracy: 95.10%\n",
      "244 iter -> loss output :0.0242 | lr: 0.010000 | Accuracy: 99.68%| Val Loss: 0.1736 | Val Acccuracy: 95.12%\n",
      "245 iter -> loss output :0.0239 | lr: 0.010000 | Accuracy: 99.69%| Val Loss: 0.1740 | Val Acccuracy: 95.10%\n",
      "246 iter -> loss output :0.0237 | lr: 0.010000 | Accuracy: 99.69%| Val Loss: 0.1743 | Val Acccuracy: 95.16%\n",
      "247 iter -> loss output :0.0235 | lr: 0.010000 | Accuracy: 99.70%| Val Loss: 0.1745 | Val Acccuracy: 95.14%\n",
      "248 iter -> loss output :0.0233 | lr: 0.010000 | Accuracy: 99.70%| Val Loss: 0.1747 | Val Acccuracy: 95.14%\n",
      "249 iter -> loss output :0.0230 | lr: 0.010000 | Accuracy: 99.71%| Val Loss: 0.1754 | Val Acccuracy: 95.14%\n",
      "250 iter -> loss output :0.0228 | lr: 0.010000 | Accuracy: 99.71%| Val Loss: 0.1761 | Val Acccuracy: 95.14%\n",
      "251 iter -> loss output :0.0226 | lr: 0.010000 | Accuracy: 99.71%| Val Loss: 0.1766 | Val Acccuracy: 95.12%\n",
      "252 iter -> loss output :0.0224 | lr: 0.010000 | Accuracy: 99.72%| Val Loss: 0.1768 | Val Acccuracy: 95.12%\n",
      "253 iter -> loss output :0.0222 | lr: 0.010000 | Accuracy: 99.73%| Val Loss: 0.1771 | Val Acccuracy: 95.10%\n",
      "254 iter -> loss output :0.0220 | lr: 0.010000 | Accuracy: 99.72%| Val Loss: 0.1773 | Val Acccuracy: 95.12%\n",
      "255 iter -> loss output :0.0218 | lr: 0.010000 | Accuracy: 99.73%| Val Loss: 0.1778 | Val Acccuracy: 95.06%\n",
      "256 iter -> loss output :0.0216 | lr: 0.010000 | Accuracy: 99.74%| Val Loss: 0.1785 | Val Acccuracy: 95.04%\n",
      "257 iter -> loss output :0.0214 | lr: 0.010000 | Accuracy: 99.75%| Val Loss: 0.1785 | Val Acccuracy: 95.02%\n",
      "258 iter -> loss output :0.0212 | lr: 0.010000 | Accuracy: 99.75%| Val Loss: 0.1788 | Val Acccuracy: 95.02%\n",
      "259 iter -> loss output :0.0210 | lr: 0.010000 | Accuracy: 99.76%| Val Loss: 0.1792 | Val Acccuracy: 95.00%\n",
      "260 iter -> loss output :0.0208 | lr: 0.010000 | Accuracy: 99.77%| Val Loss: 0.1796 | Val Acccuracy: 95.02%\n",
      "261 iter -> loss output :0.0206 | lr: 0.010000 | Accuracy: 99.77%| Val Loss: 0.1796 | Val Acccuracy: 95.02%\n",
      "262 iter -> loss output :0.0204 | lr: 0.010000 | Accuracy: 99.78%| Val Loss: 0.1800 | Val Acccuracy: 95.02%\n",
      "263 iter -> loss output :0.0202 | lr: 0.010000 | Accuracy: 99.78%| Val Loss: 0.1802 | Val Acccuracy: 95.02%\n",
      "264 iter -> loss output :0.0200 | lr: 0.010000 | Accuracy: 99.78%| Val Loss: 0.1805 | Val Acccuracy: 95.04%\n",
      "265 iter -> loss output :0.0198 | lr: 0.010000 | Accuracy: 99.79%| Val Loss: 0.1808 | Val Acccuracy: 95.06%\n",
      "266 iter -> loss output :0.0196 | lr: 0.010000 | Accuracy: 99.79%| Val Loss: 0.1811 | Val Acccuracy: 95.04%\n",
      "267 iter -> loss output :0.0194 | lr: 0.010000 | Accuracy: 99.80%| Val Loss: 0.1812 | Val Acccuracy: 95.04%\n",
      "268 iter -> loss output :0.0192 | lr: 0.010000 | Accuracy: 99.80%| Val Loss: 0.1816 | Val Acccuracy: 95.06%\n",
      "269 iter -> loss output :0.0191 | lr: 0.010000 | Accuracy: 99.80%| Val Loss: 0.1819 | Val Acccuracy: 95.08%\n",
      "270 iter -> loss output :0.0189 | lr: 0.010000 | Accuracy: 99.81%| Val Loss: 0.1823 | Val Acccuracy: 95.04%\n",
      "271 iter -> loss output :0.0187 | lr: 0.010000 | Accuracy: 99.81%| Val Loss: 0.1822 | Val Acccuracy: 95.02%\n",
      "272 iter -> loss output :0.0185 | lr: 0.010000 | Accuracy: 99.82%| Val Loss: 0.1828 | Val Acccuracy: 95.00%\n",
      "273 iter -> loss output :0.0184 | lr: 0.010000 | Accuracy: 99.82%| Val Loss: 0.1829 | Val Acccuracy: 95.02%\n",
      "274 iter -> loss output :0.0182 | lr: 0.010000 | Accuracy: 99.83%| Val Loss: 0.1836 | Val Acccuracy: 95.04%\n",
      "275 iter -> loss output :0.0181 | lr: 0.010000 | Accuracy: 99.83%| Val Loss: 0.1834 | Val Acccuracy: 95.12%\n",
      "276 iter -> loss output :0.0181 | lr: 0.010000 | Accuracy: 99.84%| Val Loss: 0.1845 | Val Acccuracy: 95.08%\n",
      "277 iter -> loss output :0.0180 | lr: 0.010000 | Accuracy: 99.84%| Val Loss: 0.1841 | Val Acccuracy: 95.14%\n",
      "278 iter -> loss output :0.0180 | lr: 0.010000 | Accuracy: 99.82%| Val Loss: 0.1854 | Val Acccuracy: 95.16%\n",
      "279 iter -> loss output :0.0179 | lr: 0.010000 | Accuracy: 99.83%| Val Loss: 0.1845 | Val Acccuracy: 95.10%\n",
      "280 iter -> loss output :0.0177 | lr: 0.010000 | Accuracy: 99.82%| Val Loss: 0.1854 | Val Acccuracy: 95.10%\n",
      "281 iter -> loss output :0.0173 | lr: 0.010000 | Accuracy: 99.85%| Val Loss: 0.1848 | Val Acccuracy: 95.06%\n",
      "282 iter -> loss output :0.0169 | lr: 0.010000 | Accuracy: 99.85%| Val Loss: 0.1851 | Val Acccuracy: 95.10%\n",
      "283 iter -> loss output :0.0168 | lr: 0.010000 | Accuracy: 99.86%| Val Loss: 0.1863 | Val Acccuracy: 95.10%\n",
      "284 iter -> loss output :0.0169 | lr: 0.010000 | Accuracy: 99.85%| Val Loss: 0.1860 | Val Acccuracy: 95.08%\n",
      "285 iter -> loss output :0.0168 | lr: 0.010000 | Accuracy: 99.84%| Val Loss: 0.1869 | Val Acccuracy: 95.04%\n",
      "286 iter -> loss output :0.0165 | lr: 0.010000 | Accuracy: 99.86%| Val Loss: 0.1861 | Val Acccuracy: 95.04%\n",
      "287 iter -> loss output :0.0162 | lr: 0.010000 | Accuracy: 99.86%| Val Loss: 0.1867 | Val Acccuracy: 95.08%\n",
      "288 iter -> loss output :0.0160 | lr: 0.010000 | Accuracy: 99.87%| Val Loss: 0.1874 | Val Acccuracy: 95.08%\n",
      "289 iter -> loss output :0.0160 | lr: 0.010000 | Accuracy: 99.87%| Val Loss: 0.1874 | Val Acccuracy: 95.06%\n",
      "290 iter -> loss output :0.0159 | lr: 0.010000 | Accuracy: 99.87%| Val Loss: 0.1887 | Val Acccuracy: 95.00%\n",
      "291 iter -> loss output :0.0158 | lr: 0.010000 | Accuracy: 99.88%| Val Loss: 0.1878 | Val Acccuracy: 95.00%\n",
      "292 iter -> loss output :0.0156 | lr: 0.010000 | Accuracy: 99.88%| Val Loss: 0.1884 | Val Acccuracy: 95.04%\n",
      "293 iter -> loss output :0.0153 | lr: 0.010000 | Accuracy: 99.88%| Val Loss: 0.1884 | Val Acccuracy: 95.04%\n",
      "294 iter -> loss output :0.0152 | lr: 0.010000 | Accuracy: 99.89%| Val Loss: 0.1887 | Val Acccuracy: 95.02%\n",
      "295 iter -> loss output :0.0151 | lr: 0.010000 | Accuracy: 99.89%| Val Loss: 0.1896 | Val Acccuracy: 95.10%\n",
      "296 iter -> loss output :0.0150 | lr: 0.010000 | Accuracy: 99.89%| Val Loss: 0.1892 | Val Acccuracy: 95.02%\n",
      "297 iter -> loss output :0.0149 | lr: 0.010000 | Accuracy: 99.89%| Val Loss: 0.1902 | Val Acccuracy: 95.08%\n",
      "298 iter -> loss output :0.0147 | lr: 0.010000 | Accuracy: 99.90%| Val Loss: 0.1896 | Val Acccuracy: 95.02%\n",
      "299 iter -> loss output :0.0146 | lr: 0.010000 | Accuracy: 99.90%| Val Loss: 0.1900 | Val Acccuracy: 95.00%\n"
     ]
    }
   ],
   "source": [
    "for step in range(steps):\n",
    "\n",
    "    model.train()\n",
    "    logits = model(input_data)\n",
    "    loss_output = loss_fn(logits, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss_output.backward()\n",
    "    optimizer.step()\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct = (preds == y).sum().item()\n",
    "        accuracy = correct / y.size(0)\n",
    "\n",
    "        # validation\n",
    "        logits_val = model(input_x_val)\n",
    "        val_loss = loss_fn(logits_val, y_val)\n",
    "        preds_val = torch.argmax(logits_val, dim=1)\n",
    "        correct_val = (preds_val == y_val).sum().item()\n",
    "        val_accuracy = correct_val / y_val.size(0)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"{step:2d} iter -> loss output :{loss_output.item():.4f} | lr: {lr:7f} | Accuracy: {accuracy*100:.2f}%| Val Loss: {val_loss.item():.4f} | Val Acccuracy: {val_accuracy*100:.2f}%\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74b838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([1, 5, 2, 8, 3])\n",
    "max_index = np.argmax(arr)\n",
    "print(max_index)  # Output: 3\n",
    "max_value = arr[max_index]\n",
    "print(max_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f44c082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2]\n",
      "[[7 8 9]\n",
      " [7 8 9]\n",
      " [7 8 9]]\n",
      "[1 0 2]\n",
      "[[6 4 2]\n",
      " [1 5 3]\n",
      " [0 1 7]]\n"
     ]
    }
   ],
   "source": [
    "arr_2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "max_index_axis0 = np.argmax(arr_2d, axis=0)\n",
    "print(max_index_axis0)  # Output: [2 2 2]\n",
    "arr_value = arr_2d[max_index_axis0]\n",
    "print(arr_value)\n",
    "\n",
    "arr_2d2 = np.array([\n",
    "    [1, 5, 3],\n",
    "    [6, 4, 2],\n",
    "    [0, 1, 7]\n",
    "])\n",
    "\n",
    "\n",
    "max_index_axis1 = np.argmax(arr_2d2, axis=1)\n",
    "print(max_index_axis1)  # Output: [2 2 2]\n",
    "arr_value2 = arr_2d2[max_index_axis1]\n",
    "print(arr_value2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
