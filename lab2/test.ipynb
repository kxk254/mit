{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069bba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comet ML\n",
    "# !pip install comet_ml --quiet\n",
    "import comet_ml\n",
    "\n",
    "# TODO: ENTER YOUR API KEY HERE!! instructions above\n",
    "COMET_API_KEY = \"zKfBRErLnMKDmo32pYIJFQdhg\"\n",
    "assert COMET_API_KEY != \"\", \"Please insert your Comet API Key\"\n",
    "\n",
    "# MIT introduction to deep learning package\n",
    "# !pip install mitdeeplearning --quiet\n",
    "import mitdeeplearning as mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1199f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import IPython\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04539b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = Path.home() / \".cache\" / \"mitdeeplearning\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Get the training data: both images from CelebA and ImageNet\n",
    "path_to_training_data = CACHE_DIR.joinpath(\"train_face.h5\")\n",
    "\n",
    "# Create a simple check to avoid re-downloading\n",
    "if path_to_training_data.is_file():\n",
    "    print(f\"Using cached training data from {path_to_training_data}\")\n",
    "else:\n",
    "    print(f\"Downloading training data to {path_to_training_data}\")\n",
    "    url = \"https://www.dropbox.com/s/hlz8atheyozp1yx/train_face.h5?dl=1\"\n",
    "    torch.hub.download_url_to_file(url, path_to_training_data)\n",
    "\n",
    "# Instantiate a TrainingDatasetLoader using the downloaded dataset\n",
    "channels_last = False\n",
    "loader = mdl.lab2.TrainingDatasetLoader(\n",
    "    path_to_training_data, channels_last=channels_last\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553deeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_training_examples = loader.get_train_size()\n",
    "(images, labels) = loader.get_batch(100)\n",
    "print(\"images shape\", images.shape)\n",
    "print(\"labels\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_images = images[np.where(labels == 1)[0]].transpose(0,2,3,1)\n",
    "not_face_images = images[np.where(labels == 0)[0]].transpose(0,2,3,1)\n",
    "\n",
    "idx_face = 21\n",
    "idx_not_face = 5\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(face_images[idx_face])\n",
    "plt.title(\"Face\")\n",
    "plt.grid(False)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(not_face_images[idx_not_face])\n",
    "plt.title(\"Not Face\")\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910aaf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_filters = 12  #base number of convolutional filters\n",
    "in_channels = images.shape[1]\n",
    "\n",
    "def make_standard_classifier(n_outputs):\n",
    "    \"\"\"Create a standard CNN classifier.\"\"\"\n",
    "\n",
    "    class ConvBlock(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.bn(x)\n",
    "            return x\n",
    "        \n",
    "        # now use the block to define the classifier\n",
    "    model = nn.Sequential(\n",
    "        ConvBlock(in_channels, n_filters, kernel_size=5, stride=2, padding=2),\n",
    "        ConvBlock(n_filters, 2*n_filters, kernel_size=5, stride=2, padding=2),\n",
    "        ConvBlock(2*n_filters, 4*n_filters, kernel_size=3, stride=2, padding=1),\n",
    "        ConvBlock(4*n_filters, 6*n_filters, kernel_size=3, stride=2, padding=1),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(H // 16 * W // 16*6*n_filters, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(512, n_outputs),\n",
    "    )\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "# call the function to instantiate a classifier model\n",
    "standard_classifier = make_standard_classifier(n_outputs=1)\n",
    "print(standard_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1af387",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a Comet experiment to track our training run ###\n",
    "def create_experiment(project_name, params):\n",
    "    # end any prior experiments\n",
    "    if \"experiment\" in locals():\n",
    "        experiment.end()\n",
    "    \n",
    "    #initiate the coment experiment for tracking\n",
    "    experiment = comet_ml.Experiment(api_key=COMET_API_KEY, project_name=project_name)\n",
    "    # log out hyperparameters, defined above, to the experiment\n",
    "    for param, value in params.items():\n",
    "        experiment.log_parameter(param, value)\n",
    "    experiment.flush()\n",
    "\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fadb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train the standard CNN ###\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training hyperparameters\n",
    "params = dict(\n",
    "    batch_size=32,\n",
    "    num_epochs=2,\n",
    "    learning_rate=5e-4,\n",
    ")\n",
    "\n",
    "experiment = create_experiment(\"LAB2\", params)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    standard_classifier.parameters(), lr=params[\"learning_rate\"]\n",
    ")\n",
    "loss_history = mdl.util.LossHistory(smoothing_factor=0.99)\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, scale=\"semilogy\")\n",
    "if hasattr(tqdm, \"_instances\"):\n",
    "    tqdm._instances.clear()\n",
    "\n",
    "standard_classifier.train()\n",
    "\n",
    "def standard_train_step(x, y):\n",
    "    x = torch.from_numpy(x).float().to(device)\n",
    "    y = torch.from_numpy(y).float().to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #feed the images into the model\n",
    "    logits = standard_classifier(x)\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(logits, y)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "# The training loop!\n",
    "step = 0\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "    for idx in tqdm(range(loader.get_train_size() // params[\"batch_size\"])):\n",
    "        # Grab a batch of training data and propagate through the network\n",
    "        x, y = loader.get_batch(params[\"batch_size\"])\n",
    "        loss = standard_train_step(x, y)\n",
    "        loss_value = loss.detach().cpu().numpy()\n",
    "\n",
    "        # Record the loss and plot the evolution of the loss as a function of training\n",
    "        loss_history.append(loss_value)\n",
    "        plotter.plot(loss_history.get())\n",
    "\n",
    "        experiment.log_metric(\"loss\", loss_value, step=step)\n",
    "        step += 1\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation of standard CNN ###\n",
    "\n",
    "# Set the model to eval mode\n",
    "standard_classifier.eval()\n",
    "\n",
    "# Training data\n",
    "# evaluate on a subset of CelebA+Imagenet\n",
    "(batch_x, batch_y) = loader.get_batch(5000)\n",
    "batch_x = torch.from_numpy(batch_x).float().to(device)\n",
    "batch_y = torch.from_numpy(batch_y).float().to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_pred_logits = standard_classifier(batch_x)\n",
    "    y_pred_standard = torch.round(torch.sigmoid(y_pred_logits))\n",
    "    acc_standard = torch.mean((batch_y == y_pred_standard).float())\n",
    "\n",
    "print(\n",
    "    \"Standard CNN annuracy on (potentially biased) training set: {:.4f}\".format(\n",
    "        acc_standard.item()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73c2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load test dataset and plot expamples ###\n",
    "\n",
    "test_faces = mdl.lab2.get_test_faces(channels_last=channels_last)\n",
    "keys = [\"Light Female\", \"Light Male\", \"Dark Female\", \"Dark Male\"]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(keys), figsize=(7.5, 7.5))\n",
    "for i, (group, key) in enumerate(zip(test_faces, keys)):\n",
    "    axs[i].imshow(np.hstack(group).transpose(1, 2, 0))\n",
    "    axs[i].set_title(key, fontsize=10)\n",
    "    axs[i].axis(\"on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0463dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate the standard CNN on the test data ###\n",
    "\n",
    "standard_classifier_probs_list = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for x in test_faces:\n",
    "        x = torch.from_numpy(np.array(x, dtype=np.float32)).to(device)\n",
    "        logits = standard_classifier(x)  #[B, 1]\n",
    "        probs = torch.sigmoid(logits) #[B, 1]\n",
    "        probs = torch.squeeze(probs, dim=-1) # shape [B]\n",
    "        standard_classifier_probs_list.append(probs.cpu().numpy())\n",
    "\n",
    "standard_classifier_probs = np.stack(standard_classifier_probs_list, axis=0)\n",
    "\n",
    "print(standard_classifier_probs.mean(axis=1))\n",
    "\n",
    "# Plot the prediction accuracies per demographic\n",
    "xx = range(len(keys))\n",
    "yy = standard_classifier_probs.mean(axis=1) # shape [D]\n",
    "plt.bar(xx, yy)\n",
    "plt.xticks(xx, keys)\n",
    "plt.ylim(max(0, yy.min() - np.ptp(yy) / 2.0), yy.max() + np.ptp(yy) / 2.0)\n",
    "plt.title(\"Standard classifier predictions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22774dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the VAE loss function\n",
    "\n",
    "def vae_loss_function(x, x_recon, mu, logsigma, kl_weight=0.0005):\n",
    "    latent_loss = 0.5 * torch.sum(torch.exp(logsigma) + mu * mu - 1 - logsigma, dim=1) #TODO\n",
    "\n",
    "    reconstruction_loss = torch.mean(torch.abs(x - x_recon), dim=(1, 2, 3))   # TODO\n",
    "\n",
    "    vae_loss = kl_weight * latent_loss + reconstruction_loss # TODO\n",
    "\n",
    "    return vae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c293513",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VAE Reparametraization ###\n",
    "\n",
    "def sampling(z_mean, z_logsigma):\n",
    "\n",
    "    # Generate rondom noise with the same shape as z_mean, sampled from a standard normal distribution\n",
    "    # (mean = 0, std = 1)\n",
    "    eps = torch.randn_like(z_mean)\n",
    "\n",
    "    z = z_mean + torch.exp(z_logsigma) * eps\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss function for DB-VAE ###\n",
    "def debiasing_loss_function(x, x_pred, y, y_logit, mu, logsigma):\n",
    "    vae_loss = vae_loss_function(x, x_pred, mu, logsigma)\n",
    "\n",
    "    classification_loss = F.binary_cross_entropy_with_logits(y_logit, y, reduction=\"none\")\n",
    "\n",
    "    y = y.float()\n",
    "    face_indicator = (y == 1.0).float()\n",
    "\n",
    "    total_loss = torch.mean(classification_loss * face_indicator + vae_loss)\n",
    "\n",
    "    return total_loss, classification_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the decorder portion of the DB-VAE ##E\n",
    "\n",
    "n_filters = 12\n",
    "latent_dim = 100  # number of latent variables\n",
    "\n",
    "def make_face_decoder_network(latent_dim=100, n_filters=12):\n",
    "\n",
    "    class FaceDecoder(nn.Module):\n",
    "        def __init__(self, latent_dim, n_filters):\n",
    "            super(FaceDecoder, self).__init__()\n",
    "            \n",
    "            self.latent_dim = latent_dim\n",
    "            self.n_filters = n_filters\n",
    "\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(latent_dim, 4 * 4 * 6 * n_filters), nn.ReLU()\n",
    "            )\n",
    "\n",
    "            self.deconv = nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=6 * n_filters,\n",
    "                    out_channels=4 * n_filters,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=4 * n_filters,\n",
    "                    out_channels=2 * n_filters,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=2 * n_filters,\n",
    "                    out_channels=n_filters,\n",
    "                    kernel_size=5,\n",
    "                    stride=2,\n",
    "                    padding=2,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.ReLU(),\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=n_filters,\n",
    "                    out_channels=3,\n",
    "                    kernel_size=5,\n",
    "                    stride=2,\n",
    "                    padding=2,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "            )\n",
    "        \n",
    "        def forward(self, z):\n",
    "            x = self.linear(z)  #[B, 4*4*6*n_filters]\n",
    "            x = x.view(-1, 6 * self.n_filters, 4, 4)  #[B, 6 n_filters, 4, 4]\n",
    "            x = self.deconv(x)  #[B, 3, 64, 64]\n",
    "            return x\n",
    "    \n",
    "    return FaceDecoder(latent_dim, n_filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining and creating the DB-VAE ###\n",
    "\n",
    "\n",
    "class DB_VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(DB_VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = make_standard_classifier(n_outputs=2 * latent_dim + 1)\n",
    "        self.decoder = make_face_decoder_network()\n",
    "    \n",
    "    def encode(self, x):\n",
    "        encoder_output = self.encoder(x)\n",
    "\n",
    "        y_logit = encoder_output[:, 0].unsqueeze(-1)\n",
    "        z_mean = encoder_output[:, 1 : self.latent_dim + 1]\n",
    "        z_logsigma = encoder_output[:, self.latent_dim + 1 :]\n",
    "\n",
    "        return y_logit, z_mean, z_logsigma\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_logsigma):\n",
    "        z = sampling(z_mean, z_logsigma)\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_logit, z_mean, z_logsigma = self.encode(x)\n",
    "\n",
    "        z = self.reparameterize(z_mean, z_logsigma)\n",
    "\n",
    "        recon = self.decode(z)\n",
    "\n",
    "        return y_logit, z_mean, z_logsigma, recon\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_logit, z_mean, z_logsigma = self.encode(x)\n",
    "        return y_logit\n",
    "    \n",
    "dbvae = DB_VAE(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64076f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_mu(images, dbvae, batch_size=64):\n",
    "    dbvae.eval()\n",
    "    all_z_mean = []\n",
    "\n",
    "    images_t = torch.from_numpy(images).float()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for start in range(0, len(images_t), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch = images_t[start:end]\n",
    "            batch = batch.to(device).permute(0, 3, 1, 2)\n",
    "            _, z_mean, _, _ = dbvae(batch)\n",
    "            all_z_mean.append(z_mean.cpu())\n",
    "\n",
    "    z_mean_full = torch.cat(all_z_mean, dim=0)\n",
    "    mu = z_mean_full.numpy()\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Resampling algorithm for DB-VAE ###\n",
    "\n",
    "def get_training_sample_probabilities(images, dbvae, bins=10, smoothing_fac=0.001):\n",
    "    print(\"Recomputing the sampling probabilities\")\n",
    "\n",
    "    mu = get_latent_mu(images, dbvae)\n",
    "\n",
    "    training_sample_p = np.zeros(mu.shape[0], dtype=np.float64)\n",
    "\n",
    "    for i in range(latent_dim):\n",
    "        latent_distribution = mu[:, i]\n",
    "        hist_density, bin_edges = np.histogram(\n",
    "            latent_distribution, density=True, bins=bins\n",
    "        )\n",
    "\n",
    "        bin_edges[0] = -float(\"inf\")\n",
    "        bin_edges[-1] = float(\"inf\")\n",
    "\n",
    "        bin_idx = np.digitize(latent_distribution, bin_edges)\n",
    "\n",
    "        hist_smoothed_density = hist_density + smoothing_fac\n",
    "        hist_smoothed_density = hist_smoothed_density / np.sum(hist_smoothed_density)\n",
    "\n",
    "        p = 1.0 / (hist_smoothed_density[bin_idx - 1])\n",
    "\n",
    "        p = p / np.sum(p)\n",
    "\n",
    "        training_sample_p = np.maximum(training_sample_p, p)\n",
    "    \n",
    "    training_sample_p /= np.sum(training_sample_p)\n",
    "\n",
    "    return training_sample_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b0d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the DB-VAE ###\n",
    "\n",
    "params = dict(\n",
    "    batch_size=32,\n",
    "    learning_rate=5e-4,\n",
    "    latent_dim=100,\n",
    "    num_epochs=2,\n",
    ")\n",
    "\n",
    "experiment = create_experiment(\"CNN2\", params)\n",
    "\n",
    "dbvae = DB_VAE(params[\"latent_dim\"]).to(device)\n",
    "optimizer = optim.Adam(dbvae.parameters(), lr=params[\"learning_rate\"])\n",
    "\n",
    "def debiasing_train_step(x, y):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_logit, z_mean, z_logsigma, x_recon = dbvae(x)\n",
    "\n",
    "    loss, class_loss = debiasing_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "all_faces = loader.get_all_train_faces()\n",
    "\n",
    "step = 0\n",
    "for i in range(params[\"num_epochs\"]):\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    print(\"Starting epoch {}/{}\".format(i+1, params[\"num_epochs\"]))\n",
    "\n",
    "    p_faces = get_training_sample_probabilities(all_faces, dbvae)\n",
    "\n",
    "    for j in tqdm(range(loader.get_train_size() // params[\"batch_size\"])):\n",
    "        (x, y) = loader.get_batch(params[\"batch_size\"], p_pos=p_faces)\n",
    "        x = torch.from_numpy(x).float().to(device)\n",
    "        y = torch.from_numpy(y).float().to(device)\n",
    "\n",
    "        loss = debiasing_train_step(x, y)\n",
    "        loss_value = loss.detach().cpu().numpy()\n",
    "        experiment.log_metric(\"loss\", loss_value, step=step)\n",
    "\n",
    "        if j % 500 == 0:\n",
    "            mdl.util.plot_sample(x, y, dbvae, backend=\"pt\")\n",
    "\n",
    "        step += 1\n",
    "\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe129e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dbvae.to(device)\n",
    "dbvae_logits_list = []\n",
    "for face in test_faces:\n",
    "    face = np.asarray(face, dtype=np.float32)\n",
    "    face = torch.from_numpy(face).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        logit = dbvae.predict(face)\n",
    "    \n",
    "    dbvae_logits_list.append(logit.detach().cpu().numpy())\n",
    "\n",
    "dbvae_logits_array = np.concatenate(dbvae_logits_list, axis=0)\n",
    "dbvae_logits_tensor = torch.from_numpy(dbvae_logits_array)\n",
    "dbvae_probs_tensor = torch.sigmoid(dbvae_logits_tensor)\n",
    "dbvae_probs_array = dbvae_probs_tensor.squeeze(dim=-1).numpy()\n",
    "\n",
    "\n",
    "\n",
    "xx = np.arange(len(keys))\n",
    "\n",
    "std_probs_mean = standard_classifier_probs.mean(axis=1)\n",
    "dbvae_probs_mean = dbvae_probs_array.reshape(len(keys), -1).mean(axis=1)\n",
    "\n",
    "plt.bar(xx, std_probs_mean, width=0.2, label=\"Standard CNN\")\n",
    "plt.bar(xx + 0.2, dbvae_probs_mean, width=0.2, label=\"DB-VAE\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(xx, keys)\n",
    "plt.title(\"Network predictions on test dataset\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e5592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ff079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
