{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b74808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F  \n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19e043af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CasualSelfAttention(nn.Module):   #Relationships between tokens (via self-attention)\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularlization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # not really a bias, more of a mask, but following the OpenAi/HF naming\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch, sequence length, embeding size\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # Every other student compares that question to their Key (what they know).\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # question a student is asking\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # Each student gives their Value (knowledge), weighted by how well their Key matched the Query.\n",
    "        # attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))  #(B, n_head, T, head_dim) @ (B, n_head, head_dim, T) ==> (B, n_head, T, T)\n",
    "        # Determines how relevant each other token is\n",
    "        # q: (2, 4, 5, 64)\n",
    "        # kᵀ: (2, 4, 64, 5)\n",
    "        # att = q @ kᵀ: (2, 4, 5, 5)\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # shape will become (1, 1, T, T)\n",
    "        # [[[1, 0, 0, 0],\n",
    "        #   [1, 1, 0, 0],\n",
    "        #   [1, 1, 1, 0],\n",
    "        #   [1, 1, 1, 1]]]\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        # Ensures focus is on the most relevant tokens\n",
    "        # softmax does not change the dimensionality; \n",
    "        # it just normalizes the values along the specified dimension (dim=-1).\n",
    "        y = att @ v  # (B, n_head, T, head_dim):\n",
    "        # Gathers contextualized information from those tokens\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # (B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "class MLP(nn.Module):  #A two-layer feedforward network : Learns token-level transformations\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Applies to each token independently\n",
    "        Self-attention | Multiple tokens (sequence-wise) | Mixes information across positions\n",
    "        MLP | One token (feature-wise) | Refines how info is represented per token\n",
    "        Self-attention: “How does this token relate to other tokens in the sentence?”\n",
    "        MLP: “How do I better express or transform the features of this one token?”\n",
    "        \"\"\"\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)  #Projects to higher dimension (richer representation)\n",
    "        self.gelu = nn.GELU(approximate='tanh')    #Adds non-linearity (helps model complex patterns)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)   #Projects back to original embedding size\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        x = self.c_fc()\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Layernorm: computes stats over C (embedding dim) for each token\n",
    "        \"\"\"\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):  # residual connections - Helps training deeper models\n",
    "        \"\"\"\n",
    "        Without residuals: every author rewrites the whole story from scratch.\n",
    "        With residuals: each author edits or improves what the previous one wrote.\n",
    "        Eases Gradient Flow:makes training deeper networks more stable and faster.\n",
    "        Helps with Identity Mapping: If a layer isn’t useful, the model can easily learn to do nothing:  This prevents deeper layers from hurting performance and helps optimization.\n",
    "        Encourages Incremental Learning: it can just refine or tweak it.\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.attn(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"\n",
    "    Head size = how deeply each attention head can understand the input.\n",
    "    Depth (larger head size = deeper understanding per head)\n",
    "\n",
    "    Number of heads = how broadly the model explores different relationships.\n",
    "    Have enough heads to capture diverse relationships (semantic, positional, etc.)\n",
    "    Breadth (more heads = more perspectives)    \n",
    "    \"\"\"\n",
    "    block_size: int = 1024  # max sqeunce length (T)\n",
    "    vocab_size: int = 50257  # 65 number of tokes: 50000 BPE merges + 256 bytes tokens + <|endoftext|>\n",
    "    n_layer: int = 12  # number of transformer block or stacked layers\n",
    "    n_head: int = 12  # number of attention heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        \"\"\" \n",
    "        input x -> (B, T)\n",
    "        wte -> (B, T, C(n_embd)) (x)\n",
    "        wpe -> (B, T, C(n_embd)) (position)\n",
    "        \"\"\"\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),  #word token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),  # position embeddings\n",
    "            # “The word ‘dog’ at position 1 is not the same as ‘dog’ at position 100.”\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),  # transformer layers\n",
    "            ln_f = nn.LayerNorm(config.n_embd),  # final layer\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)  # output layer\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of the length {T}, block size of {B}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # pos embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb  # adding these\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)  #(B, T, vocab_size)\n",
    "        return logits\n",
    "    \n",
    "    # load parameters\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257  # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()  #.state_dict() that returns a dictionary of all learnable parameters and buffers in the model.\n",
    "        sd_keys = sd.keys()  # key is .weight / .bias etc..\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  #This removes any keys that end with .attn.masked_bias  These are buffer entries (not trainable parameters) often used for attention masks \n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
    "        transposed = ['attn.c_attn.weight','attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2deea8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\develop\\mit\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\konno\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "didn't crash yay\n"
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "print(\"didn't crash yay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6660d58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device detection\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "print(f\"using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c596349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([5, 8])\n",
      "logit shape torch.Size([5, 8, 50257])\n",
      "after logit shape torch.Size([5, 50257])\n",
      "probs torch.Size([5, 50257])\n",
      "topk_probs torch.Size([5, 50]) torch.Size([5, 50])\n",
      "ix torch.Size([5, 1])\n",
      "xcol torch.Size([5, 1])\n",
      "x== torch.Size([5, 9])\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences = 5\n",
    "max_length = 10\n",
    "model = GPT(GPTConfig())\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# prefix tokens\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "x = tokens.to(device)\n",
    "# print(\"length of tokens\", len(tokens), tokens)\n",
    "print(\"x\", x.size())  #=>(5, 8)\n",
    "\n",
    "# generate right now.  (B, T)  B = 5, T = 8\n",
    "torch.manual_seed(66)\n",
    "torch.cuda.manual_seed(66)\n",
    "# while x.size(1) < max_length:\n",
    "for i in range(1):\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)  #[5, 8, 50257]  (B, T, vocab_size)\n",
    "        print(\"logit shape\", logits.shape)\n",
    "        logits = logits[:, -1, :]  #[5, 50257]  (B, vocab_size)\n",
    "        # only need the last token’s logits to predict the next word.\n",
    "        # Get logits for last token to sample next\n",
    "        print(\"after logit shape\", logits.shape)\n",
    "        probs = F.softmax(logits, dim=-1) #[5, 50257]  (B, vocab_size)\n",
    "        print(\"probs\", probs.shape)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)  #torch.Size([5, 50]) torch.Size([5, 50])\n",
    "        # topk_probs[i] gives the top 50 probabilities for the i-th example in the batch.\n",
    "        # topk_indices[i] gives the corresponding token indices (in your vocab of 50257 tokens).\n",
    "        print(\"topk_probs\", topk_probs.shape, topk_indices.shape)\n",
    "        ix = torch.multinomial(topk_probs, 1)  #[5, 1]\n",
    "        # Get the top-k most probable tokens.\n",
    "        # Sample one from them randomly, but with preference given to higher-probability tokens.\n",
    "        # This avoids always picking the most likely token (which is greedy).\n",
    "        # Adds creativity and diversity to the generated output.\n",
    "        print(\"ix\", ix.shape)\n",
    "        xcol = torch.gather(topk_indices, -1, ix)  #[5, 1]\n",
    "        print(\"xcol\", xcol.shape)\n",
    "        x = torch.cat((x, xcol), dim=1)  #[5, 9]\n",
    "        print(\"x==\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aaff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tril(torch.ones(8, 8)).view(1, 1, 8, 8)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "batch_size = 2\n",
    "seq_length = 4\n",
    "vocab_size = 100\n",
    "embedding_dim = 8\n",
    "block_size = 16  # max sequence length\n",
    "\n",
    "# Sample input\n",
    "token_ids = torch.tensor([\n",
    "    [5, 8, 2, 3],   # example 1\n",
    "    [7, 6, 1, 0]    # example 2\n",
    "])  # Shape: (2, 4)\n",
    "\n",
    "# Position indices (0 to T-1)\n",
    "position_ids = torch.arange(seq_length)  # Shape: (4,)\n",
    "\n",
    "# Embedding layers\n",
    "wte = nn.Embedding(vocab_size, embedding_dim)  # token embedding\n",
    "wpe = nn.Embedding(block_size, embedding_dim)  # position embedding\n",
    "\n",
    "# Apply embeddings\n",
    "tok_emb = wte(token_ids)           # (2, 4, 8)\n",
    "pos_emb = wpe(position_ids)        # (4, 8)\n",
    "x = tok_emb + pos_emb              # (2, 4, 8) — broadcasted addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c94154",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2594e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_emb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da34d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
