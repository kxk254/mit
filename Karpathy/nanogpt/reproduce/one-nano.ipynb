{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b74808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F  \n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e043af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CasualSelfAttention(nn.Module):   #Relationships between tokens (via self-attention)\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.h_head == 0\n",
    "        # key query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularlization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # not really a bias, more of a mask, but following the OpenAi/HF naming\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch, sequence length, embeding size\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # Every other student compares that question to their Key (what they know).\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # question a student is asking\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # Each student gives their Value (knowledge), weighted by how well their Key matched the Query.\n",
    "        # attention\n",
    "        att = (q @ k.transpose(-2, -1) * (1.0 / math.sqrt(k.size(-1))))  #(B, n_head, T, head_dim) @ (B, n_head, head_dim, T) ==> (B, n_head, T, T)\n",
    "        # Determines how relevant each other token is\n",
    "        # q: (2, 4, 5, 64)\n",
    "        # kᵀ: (2, 4, 64, 5)\n",
    "        # att = q @ kᵀ: (2, 4, 5, 5)\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # shape will become (1, 1, T, T)\n",
    "        # [[[1, 0, 0, 0],\n",
    "        #   [1, 1, 0, 0],\n",
    "        #   [1, 1, 1, 0],\n",
    "        #   [1, 1, 1, 1]]]\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        # Ensures focus is on the most relevant tokens\n",
    "        # softmax does not change the dimensionality; \n",
    "        # it just normalizes the values along the specified dimension (dim=-1).\n",
    "        y = att @ v  # (B, n_head, T, head_dim):\n",
    "        # Gathers contextualized information from those tokens\n",
    "        y = y.transpose(1, 2).contigious().view(B, T, C)\n",
    "        # (B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "class MLP(nn.Module):  #A two-layer feedforward network : Learns token-level transformations\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Applies to each token independently\n",
    "        Self-attention | Multiple tokens (sequence-wise) | Mixes information across positions\n",
    "        MLP | One token (feature-wise) | Refines how info is represented per token\n",
    "        Self-attention: “How does this token relate to other tokens in the sentence?”\n",
    "        MLP: “How do I better express or transform the features of this one token?”\n",
    "        \"\"\"\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)  #Projects to higher dimension (richer representation)\n",
    "        self.gelu = nn.GELU(approximate='tanh')    #Adds non-linearity (helps model complex patterns)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)   #Projects back to original embedding size\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        x = self.c_fc()\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Layernorm: computes stats over C (embedding dim) for each token\n",
    "        \"\"\"\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):  # residual connections - Helps training deeper models\n",
    "        \"\"\"\n",
    "        Without residuals: every author rewrites the whole story from scratch.\n",
    "        With residuals: each author edits or improves what the previous one wrote.\n",
    "        Eases Gradient Flow:makes training deeper networks more stable and faster.\n",
    "        Helps with Identity Mapping: If a layer isn’t useful, the model can easily learn to do nothing:  This prevents deeper layers from hurting performance and helps optimization.\n",
    "        Encourages Incremental Learning: it can just refine or tweak it.\n",
    "        \"\"\"\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.attn(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"\n",
    "    Head size = how deeply each attention head can understand the input.\n",
    "    Depth (larger head size = deeper understanding per head)\n",
    "\n",
    "    Number of heads = how broadly the model explores different relationships.\n",
    "    Have enough heads to capture diverse relationships (semantic, positional, etc.)\n",
    "    Breadth (more heads = more perspectives)    \n",
    "    \"\"\"\n",
    "    block_size: int = 1024  # max sqeunce length (T)\n",
    "    vocab_size: int = 50257  # 65 number of tokes: 50000 BPE merges + 256 bytes tokens + <|endoftext|>\n",
    "    n_layer: int = 12  # number of transformer block or stacked layers\n",
    "    n_head: int = 12  # number of attention heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        \"\"\" \n",
    "        input x -> (B, T)\n",
    "        wte -> (B, T, C(n_embd)) (x)\n",
    "        wpe -> (B, T, C(n_embd)) (position)\n",
    "        \"\"\"\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),  #word token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),  # position embeddings\n",
    "            # “The word ‘dog’ at position 1 is not the same as ‘dog’ at position 100.”\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),  # transformer layers\n",
    "            ln_f = nn.LayerNorm(config.n_embd),  # final layer\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)  # output layer\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of the length {T}, block size of {B}\"\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # pos embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb  # adding these\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)  #(B, T, vocab_size)\n",
    "        return logits\n",
    "    \n",
    "    # load parameters\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257  # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024  # always 1024 for GPT model checkpoints\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()  #.state_dict() that returns a dictionary of all learnable parameters and buffers in the model.\n",
    "        sd_keys = sd.keys()  # key is .weight / .bias etc..\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  #This removes any keys that end with .attn.masked_bias  These are buffer entries (not trainable parameters) often used for attention masks \n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
    "        transposed = ['attn.c_attn.weight','attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9aaff50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "          [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(8, 8)).view(1, 1, 8, 8)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a96130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "batch_size = 2\n",
    "seq_length = 4\n",
    "vocab_size = 100\n",
    "embedding_dim = 8\n",
    "block_size = 16  # max sequence length\n",
    "\n",
    "# Sample input\n",
    "token_ids = torch.tensor([\n",
    "    [5, 8, 2, 3],   # example 1\n",
    "    [7, 6, 1, 0]    # example 2\n",
    "])  # Shape: (2, 4)\n",
    "\n",
    "# Position indices (0 to T-1)\n",
    "position_ids = torch.arange(seq_length)  # Shape: (4,)\n",
    "\n",
    "# Embedding layers\n",
    "wte = nn.Embedding(vocab_size, embedding_dim)  # token embedding\n",
    "wpe = nn.Embedding(block_size, embedding_dim)  # position embedding\n",
    "\n",
    "# Apply embeddings\n",
    "tok_emb = wte(token_ids)           # (2, 4, 8)\n",
    "pos_emb = wpe(position_ids)        # (4, 8)\n",
    "x = tok_emb + pos_emb              # (2, 4, 8) — broadcasted addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60c94154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2644,  0.1056, -0.2670, -0.8304, -1.0564, -0.3676, -0.5851,  0.0837],\n",
       "        [-0.0536,  0.9074, -0.0990,  1.3859, -0.7152, -0.7872, -0.1566,  0.4855],\n",
       "        [-0.2358, -1.7843,  0.5786, -2.4957, -1.9267,  0.5426,  1.5097,  0.4468],\n",
       "        [ 0.0316,  1.0584, -1.0380,  0.7808,  0.8842, -0.0497, -0.8083, -0.6393]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2594e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9580, -0.5378, -1.1977, -0.9342,  0.7744, -0.9729, -0.4574,  0.2160],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_emb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3da34d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0106, -0.0364,  0.6077,  1.7487, -2.0082,  0.3499,  0.6859,  1.0258],\n",
       "        [ 1.9044,  0.3696, -1.2967,  0.4518,  0.0592, -1.7601, -0.6139,  0.7016],\n",
       "        [ 0.1103, -0.7721, -0.0748, -3.2344, -2.5200, -0.2137,  1.5868, -0.1149],\n",
       "        [ 0.6435,  0.4013, -1.1126,  1.4585,  2.4205,  0.9778, -1.8962, -1.0458]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
